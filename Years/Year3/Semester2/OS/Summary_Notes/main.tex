\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=0.5in]{geometry}
\renewcommand\vec{\mathbf}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{OS Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak

\nChapter{OS}

\nSection{Introduction \& Structure}

\nDefinition{OS}{
    An intermediary between the user of a computer and computer hardware. A program itself most intimately connected to the hardware. Everything you don't need to write in order to run your application.
    Library, all operations on I/O, syscalls. The OS can be an invisible intermediary
    
    \begin{center}
        \nImg{OS-1}
    \end{center}

    Main benefits of OS abstraction:

    \begin{itemize}
        \item Application benefits
            \begin{itemize}
                \item programming simplicity - see high level abstraction (files) instead of low level hardware details (device registers)
                \item abstractions are \textbf{reusable} across many platforms
                \item \textbf{portability} (across machine configurations or architectures) - device independence: 3com or intel card?
            \end{itemize} 
        \item User benefits
            \begin{itemize}
                \item safety: program sees its own virtual machine, can believe it owns the computer 
                \item OS \textbf{protects} programs from each other 
                \item OS \textbf{fairly multiplexes} resources across programs
                \item efficiency - \textbf{share} one computer across many users. \textbf{Concurrent} execution of multiple programs
            \end{itemize} 
    \end{itemize}
    }

\nDefinition{Basic OS Concepts}{
    \textbf{Multiprogramming} : ability to keep multiple programs running in parallel (almost). 
    This is done by keeping all possible jobs in the \textbf{job pool} on the disk, when a job is ready to take its turn to execute, it's brought to main memory and run for a bit untill the OS decides
    to switch it for another (because of an interrupt) or it completes.
    \\

    \textbf{Multitasking/timesharing} : while multiprogramming makes teh OS efficient, it does not faciliate for user interaction! 
    For the user to be able to run multiple tasks simultaneously and have a smooth experience, the CPU needs to actually 
    switch jobs much more frequently so as it appears that all of them are being processed simultaneously. A system which faciliates \textbf{timesharing} is \textbf{interactive} - it frequently awaits user input and has short response time. A time-shared system enables
    the system to be used by \textbf{multiple} users simultaneously.
    \\
    
    \textbf{Process} : a program which is loaded in main memory. This may be a full on program, or a printer job. Processes may spawn sub-processes if they wish to using syscalls.
    This is the \textbf{unit of work in a system}.
    \\

    \textbf{Job scheduling} : prioritising which jobs should be in main memory at any time.
    \\

    \textbf{CPU scheduling} : prioritising which jobs in main memory should be executed first.
    \\

    \textbf{Virtual memory} : a technique that allows each program to see the entire memory as theirs and not mess with other processes as well as running programs which require more memory than is \textbf{physically available}
    \\

    \textbf{Logical memory} : memory as seen by the programmer, abstracted away from the nitty-gritty mechanical details of the OS.
    \\

    \textbf{Interrupt driven} : if there's no demand for action from the OS, it will IDLE. Events are almost always signalled by the occurence of an interrupt or a trap 
    \\

    \textbf{Trap} : (exception) is a software generated interrupt caused by either an error or a syscall. 
    \\

    \textbf{Dual-mode/multimode operation} : in a multiprogramming system processes are run in parallel, hence 
    protections must be put into place so that processes cannot impede other processes. Most commonly 
    this is done via introduction of \textbf{user mode} and \textbf{kernel mode}. In user mode certain possibly harmful \textbf{priviliged instructions} are
    forbidden by the \textbf{hardware itself} which sends a trap signal (switch to kernel mode, timer management, I/O control)
    \\

    \textbf{Virtual machine manager} : virtual machine management software which can be set to run on a third mode (which requires more mode bits), giving it less power 
    than the kernel but more than the user. Virtualisation does not necessarily require its own privilege level 
    \\

    \textbf{Mode bits} : reserved bits in the hardware which signify which mode we're in. Typically 0 = user mode and 1 = kernel mode. Always set 
    \\

    \textbf{before} passing control to the user program.
    \\ 

    \textbf{File} : abstract memory concept which is mapped to physical storage space. Each file may contain 
    absolutely any data. The OS is responsible for creating/removing files, creating removing directories to organise files,
    suporting primitives for manipulating files and directories, mapping files onto secondary storage, backing up files on 
    stable (nonvolatile) storage media.
    
}

\nDefinition{OS Services}{
    \begin{center}
        \nImg[1]{OS-2}
    \end{center}
    \textbf{User Interface} : can appear in many different forms, CLI, GUI or batch - where commands and directives run directly from files.
    \\
    
    \textbf{Program execution} : the OS loads programs into memory and runs their instructions, then halts them.
    \\
    
    \textbf{IO Operations} : interactions with external devices such as the keyboard or mouse.
    \\

    \textbf{File-system manipulation} : search through files and directories, creation and deletion of files, permission management.
    \\

    \textbf{Communications} : facilities for exchanging information between different processes on the same computer or via network. I.e. \textbf{Shared memory} or perhaps \textbf{message passing}.
    \\ 

    \textbf{Error detection} : the OS needs to be aware of errors which occur and correct them as they appear. These can happen anywhere in the system, including hardware and software.
    \\

    \textbf{Resource allocation} : distribution of resources available to different jobs and users at the same time efficiently.
    \\

    \textbf{Accounting} : keeping track of who used what resources for either economic purposes or analytics.
    \\

    \textbf{Protection and security} : all data needs to be securely stored and only available to the users who have the correct perissions.
    Several processes cannot interfere with each other or harm the system.
    \\

    \textbf{Command interpreter} : allows the user to directly interface with the system via commands either in the form of a GUI or CLI or in other forms.
    When multiple are available these are shown as \textbf{shells}. The commands themselves may be stored by the shell, or the shell might simply direct the appropriate 
    loading of file-stored directives which run the appropriate commands (i.e. PATH resolution)
    \\

    \textbf{System calls} : calls to the OS to develop certain specific functions such as opening files or starting sub-processes. Many OS use API's 
    on top of system calls to make portability more achievable.
    \\

    \textbf{System-call interface} : the interface provided by programming languages to interface with the system calls in different OS' which the compiler knows the specifics of.
}

\nDefinition{Syscalls}{
    The user cannot perform IO operations by himself, he must ask the OS to do it for them.

    Syscalls define procedures which the OS performs for the user in privileged mode.

    Usually implemented as vectors, where each syscall is simply an offeset to a base address.

    Mechanically just a procedure call (but is not one! in a normal procedure call the caller knows the location of the procedure, in this case a syscall is just an ID), the caller puts arguments in a place the callee expects,
    then retrieves output from known place. Usually each system call will have wrapper functions provided by each 
    programming language, i.e. the \textbf{system call interface}, which the compiler understands.

    \nImgs[0.48]{OS-3}{OS-4}
}

\nDefinition{OS Structures}{
    \nImg[1]{OS-5}
    \nDefinition{Monolithic Kernel}{
        All major subsystems implemented in kernel. 

        \begin{itemize}
            \item[+] low system interaction cost (procedure call)
            \item[-] hard to understand and modify 
            \item[-] unrelible (no isolation between system modules)
            \item[-] hard to maintain   
        \end{itemize}
    }
    \nDefinition{Microkernel}{
        Minimize what goes into kernel, implement rest of OS 
        as user-level processes 

        \begin{itemize}
            \item[+] better reliability (isolation between components)
            \item[+] ease of extension and customization (easy to replace parts)
            \item[-] poor performance (a lot of user/kernel switches)  
        \end{itemize}
    }

    \nDefinition{Layered Kernel}{
        Implement OS as a set of layers, each layer interacts only with layer below.
        \begin{center}
            \nImg[0.5]{OS-6}    
        \end{center}    
    
        \begin{itemize}
            \item[+] more reliable (separation of components)
            \item[-] strict layering isn't flexible enough - in real life modules might need to communicate with not only nearby layers.
            \item[-] poor performance, each layer crossing has overhead associated with it (due to API generalization)
            \item[-] Disjunction between model and reality - system modelled as layers, but not really built that way   
        \end{itemize}
    }

}


\nDefinition{Dynamically loadable kernel modules}{
    Core services in the kernel, others dynamically loaded.
    \\

    Common in modern implementations:
    \begin{itemize}
        \item \textbf{Monolithic}: load the code in kernel space (Solaris, Linux, etc.)
        \item \textbf{Microkernel}: load the code in user space (any)
    \end{itemize}
    

    \begin{itemize}
        \item[+] Convenient: no need for rebooting for newly added modules
        \item[+] Efficient: no need for message passing unlike microkernel
        \item[+] Flexible: any module can call any other module unlike layered model  
        \item[-] Memory fragmentation: fragments OS memory which is normally unfragmented when loaded initially
    \end{itemize}
}

\nDefinition{Hybrid OS Design}{
    Many different approaches. Key idea: exploit the benefits of monolithic and microkernel designs.
    Extensibility via kernel modules.
    
}

\nSection{IO}

\nDefinition{I/O}{
    Variety of I/O Devices
    \\

    \textbf{Port} : Connection point for a device (e.g., USB, parallel, serial, ethernet)
    \\
    \textbf{Bus} : Peripheral buses (e.g. PCI/PCIe), Expansion bus connects relatively slow devices 
    \\
    \textbf{Device}
    \\
    \textbf{Controller(host adapter)} : electronics that operate port, bus, device (sometimes integrated). Contains processor, microcode, private memory, bus controller etc. 

    \begin{center}
        \nImgs[0.46]{OS-7}{OS-8} 
    \end{center}
    
    Controllers have \textbf{registers} for data and control as well as \textbf{buffers}, mostly for data.
    Communication Methods:
    \begin{itemize}
        \item \textbf{IO Ports}
        \item \textbf{Memory-mapped IO}
        \item Hybrid
    \end{itemize}
}

\nDefinition{I/O Ports}{
    \begin{itemize}
        \item Each control register has an I/O port number
        \item Special instructions exist to access the I/O port space
        \item CPU reads in from device I/O PORT to CPU register (IN REG, PORT)
        \item CPU writes to device I/O PORT from CPU register (OUT PORT, REG)
        \item Instructions are privileged (OS kernel only)
        \item Seperate I/O Port space and memory space:
            
        \nImg[0.8]{OS-9}
    \end{itemize}

    \begin{center}
        \nImg[0.6]{OS-10}
    \end{center}
}

\nDefinition{Memory Mapped I/O}{
    \begin{itemize}
        \item All control registers and buffers mapped into the memory space 
        \item Each control register is assigned a unique memory address
        \item There is no actual RAM memory for that address
        \item Such addresses may be at the top of the physical address space 
        
            \nImg[0.7]{OS-11} 
    \end{itemize}

    \begin{center}
        \nImg{OS-12}
    \end{center}
}

\nDefinition{Hybrid I/O}{
    Simply do both, use memory mapped I/O for the \textbf{data buffers}, and keep separate I/O ports for \textbf{control registers}.
}

\nDefinition{Offloaded Communication}{
    The CPU can request data from an I/O controller one byte at at time (Programmed IO).
    This wastes CPU time for large data transfers, small data transfers are ok.

    CPU can instead offload data transfers using DMA:
    \\

    \textbf{DMA (Direct Memory Access) controller} transfers data fro the CPU, either from/to IO or between IO devices.
    \\

    This requires a DMA controller either on the device's host controller, and the motherboard.
    This controller contains registers to be read/written by the software:
    \begin{itemize}
        \item Memory address register
        \item byte count register 
        \item Control registers: direction, unit, byte burst size etc..
    \end{itemize}
}

\nDefinition{OS Device drivers}{
    Great variety of devices, each one has very different specs.

    OS Deals with IO devices in a standard and uniform way.
    Each type of driver is an interface which the vendor can implement as a class. Each OS has its own standard.

    \begin{center}
        \nImg[0.8]{OS-13}
    \end{center}

}

\nDefinition{Life cycle of an IO request}{
    \begin{center}
        \nImg[1]{OS-14}
    \end{center}   
}

\nSection{Processes}

\nDefinition{Process}{
    Process is the OS's abstraction for execution.

    \begin{itemize}
        \item Program is the list of instructions, initialized data, etc 
        \item A process is a \textbf{program in execution}
        \item A single flow/sequence of instruction in execution 
        \item An address space (an abstraction of the CPU)
    \end{itemize}

    Only one process can be running on a processor core at any instant

    Contents:
    \begin{itemize}
        \item An address space, containing:
            \begin{itemize}
                \item Code (instructions) for running program
                \item Data for the running program (static data, heap data, stack)
            \end{itemize}
        \item A CPU state, consisting of 
            \begin{itemize}
                \item Program counter, indicating the next instruction 
                \item Stack pointer, current stack position 
                \item Other general-purpose register values
            \end{itemize}
        \item A set of OS resources 
            \begin{itemize}
                \item Open files
                \item network connections 
                \item sounc channels
            \end{itemize}
    \end{itemize}
    
    I.e. everything needed to run the program
    
    \begin{center}
        \nImg[0.7]{OS-15}
    \end{center}


    Each process is identified by a process ID (\textbf{PID}). PID's are unique and global. With certain exceptions (cgroups)
    \\

    Operations that create processes return a PID, and those which operate on processes accept PID's as arguments
    }

\nDefinition{Process representation}{
    Each process is represented internally by the OS with a \textbf{Process control block (PCB)} or process/task descriptor,
    identified by the PID.

    OS keeps all of a process's execution state in (or linked from) the PCB when the process isn't running:
    \begin{itemize}
        \item PC, SP, registers etc.
        \item when a process execution is stopped, its state is transferred out of the hardware into the PCB
    \end{itemize}
    When the process is running its state is spread between the PCB and the hardware (CPU regs)

    PCB's contain:
    \begin{itemize}
        \item Process ID (PID)
        \item Parent process ID 
        \item Execution state 
        \item PC, SP, registers 
        \item Address space info 
        \item UNIX user ID, group ID 
        \item Scheduling priority 
        \item Accounting info 
        \item Pointers for state queues 
    \end{itemize}
    and likely many more.

    Whenever the OS gets control becase of :
    \begin{itemize}
        \item Syscalls
        \item Exceptions
        \item Interrupts
    \end{itemize}
    The OS then saves the CPU state into the PCB.
    Whenever the process is resumed into execution again, its PCB is loaded onto the machine registers SP, PC etc.

    This is called a \textbf{Context switch}
}

\nDefinition{Context Switch}{
    \begin{center}
        \nImg[0.8]{OS-16}
    \end{center}
}

\nDefinition{Execution states}{
    Each process has an \textbf{Execution state}, which indicates what it's currently doing.
    \begin{itemize}
        \item \textbf{Ready}: waiting to be assigned to a CPU 
        \item \textbf{Running}: executing on a CPU
        \item \textbf{Waiting}: Waiting for an event, e.g. IO completion, or a message from another process.
    \end{itemize}

    \begin{center}
        \nImg[0.8]{OS-17}
    \end{center}
}

\nDefinition{State queues}{
    The OS maintains a collection of queues, that represent the state of all processes in the system.
    Typically one queue for each state (executing, waiting etc..)
    Each PCB is queued onto a state queue according to the current state of the process it represents.
    As a process changes state, its PCB is unlinked from one queue, and then linked onto another.
    \\

    There may be many wait queues, one for each type of wait (specific device, timer, message) etc.
}

\nDefinition{Process creation}{
    New processes are created by existing processes (parent-child)

    The first process is started by the OS, everything else stems from it (init in linux)
    \\

    Depending on OS, child processes inherit certain attributes of parent, (i.e. open file table: implies stdin/stdout/stderr)
    Some systems divide resources of parent between children.

    }

\nDefinition{UNIX - fork()}{
    On UNIX systems, process creation is done through the fork() system call:
    \begin{itemize}
        \item Creates and initializes a new PCB 
        \item Initializes kernel resources of new process with resources of parent (e.g. open files)
        \item Initializes PC,SP to be same as parent
        \item Creates a new address space, which is an identical copy of this of the parent's (by value)
    \end{itemize}

    The fork call returns "twice" once in the parent, and once in the child.
    In the child the PID returned is 0 and in the parent, the child's PID is returned.

    \begin{center}
        \nImg[0.8]{OS-18}
    \end{center}

}

\nDefinition{UNIX - exec()}{
    In order to start a new program instead of just copying the old one, we must use exec().
    Which is the call which stops the current process, loads a new program into the address space, 
    initializes the hardware context and args for the new program and finally places the PCB onto the ready queue

    \begin{center}
        \nImg[0.8]{OS-19}
    \end{center}

    Alternatively use vfork() which is faster but less safe
    }

\nDefinition{UNIX - vfork()}{
    Same as fork, but hte child's address space \textbf{IS} by address the same space as the parents.
    
    Usage relies on the child not modifying the address space before doing an execve() call, otherwise bad things can happen.

    \begin{center}
        \nImg[0.7]{OS-20}
    \end{center}

}

\nDefinition{Copy-on-write (COW) fork()}{
    Retains original semantics, but copies "only what is necessary" rather than the entire address space.

    On fork():
    \begin{itemize}
        \item create a new address space 
        \item initialize page tables with same mappings as parents (identical)
        \item Set both parent and child page tables to make all pages read-only
        \item if either parent or child writes to memory, an exception occurs
        \item On exception, OS copies the page, adjusts page tables, etc..
    \end{itemize}
}

\nSection{Interprocess communication}


\nDefinition{Shared memory}{
    Allow processes to communicate and synchronize:
    \begin{itemize}
        \item Sharing part of address space 
        \item OS doesn't mediate communication (no overhead)
        \item Usually OS prevents processes from accessing each other's memory
        \item Processes should agree to void this restriction
    \end{itemize}

    Data:
    \begin{itemize}
        \item Format decided by application
        \item Direct access (not mediated by the OS) - very fast 
        \item Application programmer fully manages the data transfer - not trivial
    \end{itemize}

    Possible use cases:
    \begin{itemize}
        \item Passing of large (single) objects (image)
        \item Notification variable 
    \end{itemize}

    \begin{center}
        \nImg[0.9]{OS-21}
    \end{center}
}

\nDefinition{Message Passing}{
    Allow processes to communicate and synchronize:
    \begin{itemize}
        \item Without sharing part of address space 
        \item OS mediates communication (overhead likely)
    \end{itemize}

    Works with processes on the same machine and also those on different inter-networked machines!
    This is not possible with shared memory.

    Message passing facility provides at least two operations:
    \begin{itemize}
        \item Send(message)
        \item Receive(message)
    \end{itemize}

    Communication link:
    \begin{itemize}
        \item Several implementation tradeoffs, .e.g. messages size
        \item Fixed 
        \item Variable
    \end{itemize}
    
    Communicating processes must refer to each other:
    \begin{itemize}
        \item Direct communication:
            \begin{itemize}
                \item Symmetric: explicit name of sender and receiver
                    \begin{itemize}
                        \item send(P, message) - send message to P
                        \item receive(P, message) - receive message from P
                    \end{itemize}
                \item Assymetric: Explicit at least on one end:
                    \begin{itemize}
                        \item send(P, message) - send to p 
                        \item receive(id, message) - receive from any process, sender saved in id
                    \end{itemize}
            \end{itemize}
        \item Indirect communication:
            \begin{itemize}
                \item No need to know/explicitly in advance sender and/or receiver 
                \item Mailboxes (e.g., POSIX mailbox)
                    \begin{itemize}
                        \item send(A, message) - send message to mailbox A
                        \item receive(A, message) - receive message from mailbox A 
                    \end{itemize}
                \item A mailbox can be accesed by more than two processes
                \item Multiple mailboxes might exist between processes 
            \end{itemize}
    \end{itemize}

        send() and receive() calls might be implemented as \textbf{blocking} or \textbf{synchronous} as well as \textbf{nonblocking} or \textbf{asynchronous}.
    different combinations of these might be offered:
    \begin{itemize}
        \item Blocking send 
        \item Nonblocking send 
        \item Blocking receive 
        \item Nonblocking receive
    \end{itemize}

    \textbf{Rendezvous} - When both send and receive are blocking
    }

\nDefinition{Buffering}{
    Messages exchanged reside in temporary buffers/queues with either: 
    \begin{itemize}
        \item Zero capacity (no buffering) - no message waiting, sender must block until recipient receives message
        \item Bounded capacity - n messages may reside. If the queue is not full, then nonblocking, otherwise blocking
        \item Unbounded - never blocks
    \end{itemize}
}

\nDefinition{Example implementation: Pipes}{
    A pipe acts as a conduit allowing two processes to communicate \textbf{one-way} only and either \textbf{Annonymously} or \textbf{Named}.
    
    \begin{center}
        \nImg[0.9]{OS-22}
    \end{center}
    
    Mechanically act like file descriptors (streams)
}


\nDefinition{Client-Server communication}{
    \begin{itemize}
        \item \textbf{Sockets} abstraction
            \begin{itemize}
                \item  endpoint for communication 
                \item identified by an IP address concatenated with a port number 
                \item servers implementing specific services (SSH, FTP, HTTP) listen to well-known ports
                \item an SSH server listens to port 22, an FTP server listens to port 21, a web or http server listens to port 80
            \end{itemize}
        \item \textbf{Remove procedure call} (RPC)
            \begin{itemize}
                \item Abstract the procedure-call mechanism 
                \item for use between systems with network connections
                \item similar in many respects ot the IPC 
                \item uses message-based communication to provide remote service
            \end{itemize}
    \end{itemize}
}

\nDefinition{Signals}{
    \begin{itemize}
        \item OS mechanism to notify a process (one way)
        \item From the OS POV can be thought as a software-generated interruption/exception (synchronous or asynchronous)
        \item From other processes POV, it is only a notification, no data is transferred. A communication method for: management, synchronization etc.
    \end{itemize}

    UNIX: 
    signal handlers must be registered, and provide code for handling the signal.
    \begin{center}
        \nImg[0.50]{OS-23}
    \end{center}
}

\nSection{Threads}

\nDefinition{Concurrency}{
    Concurrency is carrying out multiple tasks in parallel but only one at the same time instance:
    \begin{center}
        \nImg{OS-24}
    \end{center}
}
\nDefinition{Parallelism}{
    Parallelism is carrying out multiple tasks in parallel at the same time instance:
    \begin{center}
        \nImg{OS-25}
    \end{center}
}

\nDefinition{Communication problems}{
    Multiple processes are required for both concurrency and parallelism, this requires communication. But the 
    methods discussed thus far have limited usability.
    \\

    Message passing:
    \begin{itemize}
        \item[-] slow, OS mediates
    \end{itemize}
    Shared Memory:
    \begin{itemize}
        \item[-] limited shareability, not all pointers work (both processes have different virtual memory layouts)
        \item[-] OS resources not shared by default - cumbersome
    \end{itemize}


    Possible solution:
    \begin{enumerate}
        \item Fork several processes 
        \item cause each of them to map to the same shared memory (shmget())
        \item make them open the same OS resources
    \end{enumerate}
    
    Couple problems:
    \begin{itemize}
        \item[-] cumbersome 
        \item[-] has limited shareability again 
        \item[-] inefficient - takes a long time to create all this, requires a PCB per process etc..   
    \end{itemize}
}

\nDefinition{Threads}{
    Threads are a great solution to the communication problem:
    Instead of spawning a new process per task, use \textbf{threads}.
    \\

    Each thread is part of the same spawning process, shares \textbf{address space \& OS resources}.
    \\

    Threads only differ in their \textbf{execution state (instruction flow)} i.e. private stack, and CPU state

    \begin{center}
        \nImg[0.7]{OS-26}
    \end{center}

    A thread abstracts the execution state away from a process, and now a process represents the static parts of a task (address space, OS resources etc)

    \begin{center}
        \nImgs[0.46]{OS-27}{OS-28}
    \end{center}

    Threads become the \textbf{unit of scheduling} (depending on implementation of course).

    Think processes are boxes for threads in which they execute.
    }

\nDefinition{Thread Control Block - TCB}{
    On the OS side, the PCB need to be adjusted to accomodate threads, easiest way is to create sub blocks for each thread 
    representing execution state:

    TCB contains:
    \begin{itemize}
        \item Program counter 
        \item CPU registers 
        \item Scheduling information 
        \item Pending I/O information 
    \end{itemize}

    PCB stores:
    \begin{itemize}
        \item Memory management information
        \item Accounting information
    \end{itemize}

    \begin{center}
        \nImgs[0.46]{OS-29}{OS-30}
    \end{center}
}


\nDefinition{User vs Kernel level threading}{
    Threading can be either implemented as part of the OS, or as a library in the user space 

    \nImgs[0.45]{OS-31}{OS-32}

    \nDefinition{Kernel level threading 1:1}{
        \begin{itemize}    
            \item OS allocates and manages threads
            \item TID's are used to identify threads
            \item[+] if one thread blocks, the OS can run other threads within the same process 
            \item[+] possiblity to efficiently overlap IO and CPU time within a process
            \item[+] Threads are cheaper than processes - less state to manage
            \item[-] pretty expensive for fine-grained use
                \begin{itemize}
                    \item[-] Orders of magnitute more expensive than procedure calls 
                    \item[-] thread operations are syscalls (context switches + argument checks)
                    \item[-] Must maintain kernel state for each thread
                \end{itemize}

        \end{itemize}
    }

    \nDefinition{User Level threading 1:N}{
        \begin{itemize}
            \item Threads managed at the user level, within the process 
            \item A library in the program manages the threads 
            \item the thread manager doesnt need to manipulate address spaces (Only OS can)
            \item Threads differ only in hardware contexts (PC,SP,registers), which can be manipulated by user-level code
            \item The thread package multiplexes user-level threads in a process
            \item TID's are now unique per process not globally
            \item No context switching between thread operations, these are done via procedure calls now (10-100x times faster than kernel threads)
            \item[-] if one thread tries to do IO, the whole process is blocked!

        \end{itemize}
    }

    \nDefinition{N:M Threading}{
        Best of both worlds, can start OS level threads for threads which will use IO.
    }
    }

\nSection{Scheduling}

\nDefinition{Dispatcher}{
    Mechanism used to switch between tasks (save and restore state)
}

\nDefinition{Scheduler}{
    Decides on policy (implemented by scheduling algorithms) for ordering excution of tasks (threads/processes)
}

\nDefinition{CPU Bursts}{
    Bursts of CPU processing done by a task. \textbf{Application dependent}
}

\nDefinition{IO Bursts}{
    Similar to CPU Burst but for IO operations
}

\nDefinition{Performance goals}{
    \begin{itemize}
        \item CPU Utilization 
        \item Throughput (processes completed per unit time)
        \item Turnaround time (time from submission of task to completion)
        \item Waiting time (all periods spent waiting in the ready queue from submission)
        \item Response time (time from submission of request to when response produced)
        \item Energy (joules per instruction) subject to some constraint (fps)
    \end{itemize}

    In most cases we optimize the \textbf{average metric}. Goals may be conflicting
}

\nDefinition{Fairness}{
    No single compelling definition of fair for process resource allocation.
    \\
    
    Sometimes goal is to be unfair and prioritize some classes of requests higher.
    \\

    We want to avoid starvation - everyone needs at least some service
}

\nDefinition{Classes of schedulers}{
    \begin{itemize}
        \item Batch - throughput / utilization oriented
        \item Interactive - response time oriented 
        \item Real time - deadline driven
    \end{itemize}
}

\nDefinition{Preemptive scheduling}{
    \textbf{Non-preemptive} scheduling:
    \begin{itemize}
        \item Processes/threads execute until completion or until they want
        \item The scheduler gets involved only at exit or on request
    \end{itemize}

    \textbf{Preemptive} scheduling:
    \begin{itemize}
        \item While a process/thread executes, its execution may be paused, and another process/thread resumes its eecution
        \item Involountary process switch
    \end{itemize}
}

\nSection{Scheduling algorithms}

\nDefinition{First-come First-served (FCFS)}{
    Processes/tasks served in the order they arrive:
    \\

    \begin{tabular}{|c|c|c|}
        \hline
        Process & CPU time & Turnaround time \\ \hline
        P1 & 24 & 24 \\ \hline 
        P2 & 3 & 24 + 3 = 27 \\ \hline 
        P3 & 3 & 24 + 3 + 3 = 30 \\ \hline
    \end{tabular}
    \\ 

    Execution order: P1,P2,P3 

    Avg. Turnaround time: $\frac{24 + 27 + 30}{3} = 27$

    \begin{itemize}
        \item[-] Non pre-emptive 
        \item[-] Poor average response time 
        \item[-] poor utilisation of \textbf{other resources} - a CPU-intensive job prevents I/O- intensive job from tiny bit of computaion on the CPU before returning to IO and keeping disk busy  
    \end{itemize}
}

\nDefinition{Shortest Job First (SJF)}{
    Associate with each process the length of its CPU time 
    Sort jobs, shortest CPU time goes first 
    Can be preemptive (simply re-sort including the current process - Shortest Remaining Time Next, SRTN)
    \\

    \nDefinition{Non-Preemptive}{
        \begin{tabular}{|c|c|c|c|}
            \hline
            Process & Arrival time & CPU time & Turnaround time \\ \hline
            P1 & 0 & 7 & 7 \\ \hline 
            P2 & 2 & 4 & 12 - 2 = 10 \\ \hline 
            P3 & 4 & 1 & 8 - 4 = 4 \\ \hline
            P4 & 5 & 4 & 16 - 5 = 11 \\ \hline
    
        \end{tabular}
        \\ 
    
        Execution order: P1,P3,P2,P4
    
        Avg. Turnaround time: $\frac{7 + 10 + 4 + 11}{4} = 8$
    
    }

    \nDefinition{Preemptive}{
        \begin{tabular}{|c|c|c|c|}
            \hline
            Process & Arrival time & CPU time & Turnaround time \\ \hline
            P1 & 0 & 7 & 16 \\ \hline 
            P2 & 2 & 4 & 7 - 2 = 5 \\ \hline 
            P3 & 4 & 1 & 5 - 4 = 1 \\ \hline
            P4 & 5 & 4 & 11 - 5 = 6 \\ \hline
    
        \end{tabular}
        \\ 
    
        Execution order: P1 (2s),P2 (2s),P3(1s),P2(2s),P4(4s),P1(5s)
    
        Avg. Turnaround time: $\frac{16 + 5 + 1 + 6}{4} = 7$
    
    }

    
    \begin{itemize}
        \item[+] Preemptive is optimal 
        \item[-] Too complex, to be implemented in practice 
        \item[-] not always possible to determine the CPU/IO burst lenghts  
    \end{itemize}
}



\nDefinition{Round-robin (RR)}{
    Processes run in discrete time slots, after each time slot a new process/task is chosen to be run
    \\

    Time quantum = 20  
    \begin{tabular}{|c|c|c|}
        \hline
        Process & CPU time & Turnaround time \\ \hline
        P1 & 53 & 125 - 0 = 125 \\ \hline 
        P2 & 8 & 28 - 0 = 28 \\ \hline 
        P3 & 68 & 153 - 0 = 153 \\ \hline
        P4 & 24 & 112 - 0 = 112 \\ \hline

    \end{tabular}
    \\ 

    Execution order: P1(20s),P2(8s),P3(20s),P4(20s),P1(20s),P3(20s),P4(4s),P1(13s),P3(20s),P3(8s)
    \\

    Avg. Turnaround time: $\frac{125 + 28 + 153 + 112}{4} = 104.5$

    Long time quanta cause poor response times with a lot of processes, a too low time quanta, causes a lot of context switching loss.
    \begin{itemize}
        \item[+] Solves fairness and starvation 
        \item[+] Fair allocation of CPU across jobs
        \item[+] Low average waiting time when job lengths vary
        \item[+] Good for responsiveness (interactivity) if small number of jobs
        \item[-] Context switching time may add up for long jobs   
    \end{itemize}
}

\nDefinition{Priority (PRIO)}{
    Always execute highest-priority runnable jobs to completion
    \\

    \begin{tabular}{|c|c|c|c|}
        \hline
        Process & CPU time & Priority & Turnaround time \\ \hline
        P1 & 10 & 3 & 16 \\ \hline 
        P2 & 1 & 1 & 1 \\ \hline 
        P3 & 2 & 4 & 18 \\ \hline
        P4 & 1 & 5 & 19 \\ \hline
        P5 & 5 & 2 & 6 \\ \hline

    \end{tabular}

    Execution order: P2,P5,P1,P3,P4
    \\

    Avg. Turnaround time: $\frac{16 + 1 + 18 + 19 + 6}{5} = 12$

    How to assign priorities ? Based on process type, User, price paid etc. or 
    dynamically, based on how long the process ran etc..

    \begin{itemize}
        \item[-] Starvation - lower priority jobs dont get to run because higher priority always running 
        \item[-] deadlock - priority inversion - happens when a low priority task has lock needed by high priority task (busy waiting) 
    \end{itemize}
}

\nDefinition{Multiple Queues(MQ)}{
    Multiple round-robin scheduled queues, with queues of higher priority always scheduled first
    \\

    \begin{center}
        \nImg[0.6]{OS-33}
    \end{center}

    Time quantum = 2
    \begin{tabular}{|c|c|c|c|}
        \hline
        Process & CPU time & Priority & Turnaround time \\ \hline
        P1 & 10 & 3 & 19 \\ \hline 
        P2 & 1 & 1 & 1 \\ \hline 
        P3 & 2 & 3 & 10 \\ \hline
        P4 & 1 & 3 & 11 \\ \hline
        P5 & 5 & 2 & 6 \\ \hline
    \end{tabular}

    Execution order: P2(1s),P5(5s),P1(2s),P3(2s),P4(1s),P1(8s)
    \\

    Avg. Turnaround time: $\frac{19 + 1 + 10  + 11 + 6}{5} = 9.4$

    How to assign priorities ? Based on process type, User, price paid etc. or 
    dynamically, based on how long the process ran etc..

    \begin{itemize}
        \item[-] Starvation - lower priority jobs dont get to run because higher priority always running 
        \item[-] deadlock - priority inversion - happens when a low priority task has lock needed by high priority task (busy waiting) 
    \end{itemize}
}

\nDefinition{Multilevel Feedback Queue(MLFQ)}{
    Same as MQ but each queue has a different time quanta. Time quanta are increasing inversely with priority of queue (higher priority lower time quanta).
    Each process starts in queue 1 - but when it exceeds its quanta it's pushed lower in the queues. When a process becomes inactive it is moved to a higher priority.
    This can be gamed by making a process interactive.
    \\

    \begin{center}
        \nImg[0.6]{OS-34}
    \end{center}

    Time quanta = (1,2,4,8,16,32,64)
    \begin{tabular}{|c|c|c|}
        \hline
        Process & CPU time & Turnaround time \\ \hline
        P1 & 100 & 102 \\ \hline 
        P2 & 2 & 3 \\ \hline 
    \end{tabular}

    Execution order: P1(1s),P2(1s),P1(2s),P2(1s),P1(4s),P1(16s),P1(37s)
    \\

    Avg. Turnaround time: $\frac{102 + 3}{2} = 52.5$

    8 context switches vs 101 with fixed quanta
    \begin{itemize}
        \item[-] Starvation - lower priority jobs dont get to run because higher priority always running 
        \item[-] deadlock - priority inversion - happens when a low priority task has lock needed by high priority task (busy waiting) 
    \end{itemize}
}
\end{document}
