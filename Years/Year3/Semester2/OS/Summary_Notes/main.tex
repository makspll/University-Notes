\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{cancel}

\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=0.5in]{geometry}
\renewcommand\vec{\mathbf}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{OS Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak

\nChapter{OS}

\nSection{Introduction \& Structure}

\nDefinition{OS}{
    An intermediary between the user of a computer and computer hardware. A program itself most intimately connected to the hardware. Everything you don't need to write in order to run your application.
    Library, all operations on I/O, syscalls. The OS can be an invisible intermediary
    
    \begin{center}
        \nImg{OS-1}
    \end{center}

    Main benefits of OS abstraction:

    \begin{itemize}
        \item Application benefits
            \begin{itemize}
                \item programming simplicity - see high level abstraction (files) instead of low level hardware details (device registers)
                \item abstractions are \textbf{reusable} across many platforms
                \item \textbf{portability} (across machine configurations or architectures) - device independence: 3com or intel card?
            \end{itemize} 
        \item User benefits
            \begin{itemize}
                \item safety: program sees its own virtual machine, can believe it owns the computer 
                \item OS \textbf{protects} programs from each other 
                \item OS \textbf{fairly multiplexes} resources across programs
                \item efficiency - \textbf{share} one computer across many users. \textbf{Concurrent} execution of multiple programs
            \end{itemize} 
    \end{itemize}
    }

\nDefinition{Basic OS Concepts}{
    \textbf{Multiprogramming} : ability to keep multiple programs running in parallel (almost). 
    This is done by keeping all possible jobs in the \textbf{job pool} on the disk, when a job is ready to take its turn to execute, it's brought to main memory and run for a bit untill the OS decides
    to switch it for another (because of an interrupt) or it completes.
    \\

    \textbf{Multitasking/timesharing} : while multiprogramming makes teh OS efficient, it does not faciliate for user interaction! 
    For the user to be able to run multiple tasks simultaneously and have a smooth experience, the CPU needs to actually 
    switch jobs much more frequently so as it appears that all of them are being processed simultaneously. A system which faciliates \textbf{timesharing} is \textbf{interactive} - it frequently awaits user input and has short response time. A time-shared system enables
    the system to be used by \textbf{multiple} users simultaneously.
    \\
    
    \textbf{Process} : a program which is loaded in main memory. This may be a full on program, or a printer job. Processes may spawn sub-processes if they wish to using syscalls.
    This is the \textbf{unit of work in a system}.
    \\

    \textbf{Job scheduling} : prioritising which jobs should be in main memory at any time.
    \\

    \textbf{CPU scheduling} : prioritising which jobs in main memory should be executed first.
    \\

    \textbf{Virtual memory} : a technique that allows each program to see the entire memory as theirs and not mess with other processes as well as running programs which require more memory than is \textbf{physically available}
    \\

    \textbf{Logical memory} : memory as seen by the programmer, abstracted away from the nitty-gritty mechanical details of the OS.
    \\

    \textbf{Interrupt driven} : if there's no demand for action from the OS, it will IDLE. Events are almost always signalled by the occurence of an interrupt or a trap 
    \\

    \textbf{Trap} : (exception) is a software generated interrupt caused by either an error or a syscall. 
    \\

    \textbf{Dual-mode/multimode operation} : in a multiprogramming system processes are run in parallel, hence 
    protections must be put into place so that processes cannot impede other processes. Most commonly 
    this is done via introduction of \textbf{user mode} and \textbf{kernel mode}. In user mode certain possibly harmful \textbf{priviliged instructions} are
    forbidden by the \textbf{hardware itself} which sends a trap signal (switch to kernel mode, timer management, I/O control)
    \\

    \textbf{Virtual machine manager} : virtual machine management software which can be set to run on a third mode (which requires more mode bits), giving it less power 
    than the kernel but more than the user. Virtualisation does not necessarily require its own privilege level 
    \\

    \textbf{Mode bits} : reserved bits in the hardware which signify which mode we're in. Typically 0 = user mode and 1 = kernel mode. Always set 
    \\

    \textbf{before} passing control to the user program.
    \\ 

    \textbf{File} : abstract memory concept which is mapped to physical storage space. Each file may contain 
    absolutely any data. The OS is responsible for creating/removing files, creating removing directories to organise files,
    suporting primitives for manipulating files and directories, mapping files onto secondary storage, backing up files on 
    stable (nonvolatile) storage media.
    
}

\nDefinition{OS Services}{
    \begin{center}
        \nImg[1]{OS-2}
    \end{center}
    \textbf{User Interface} : can appear in many different forms, CLI, GUI or batch - where commands and directives run directly from files.
    \\
    
    \textbf{Program execution} : the OS loads programs into memory and runs their instructions, then halts them.
    \\
    
    \textbf{IO Operations} : interactions with external devices such as the keyboard or mouse.
    \\

    \textbf{File-system manipulation} : search through files and directories, creation and deletion of files, permission management.
    \\

    \textbf{Communications} : facilities for exchanging information between different processes on the same computer or via network. I.e. \textbf{Shared memory} or perhaps \textbf{message passing}.
    \\ 

    \textbf{Error detection} : the OS needs to be aware of errors which occur and correct them as they appear. These can happen anywhere in the system, including hardware and software.
    \\

    \textbf{Resource allocation} : distribution of resources available to different jobs and users at the same time efficiently.
    \\

    \textbf{Accounting} : keeping track of who used what resources for either economic purposes or analytics.
    \\

    \textbf{Protection and security} : all data needs to be securely stored and only available to the users who have the correct perissions.
    Several processes cannot interfere with each other or harm the system.
    \\

    \textbf{Command interpreter} : allows the user to directly interface with the system via commands either in the form of a GUI or CLI or in other forms.
    When multiple are available these are shown as \textbf{shells}. The commands themselves may be stored by the shell, or the shell might simply direct the appropriate 
    loading of file-stored directives which run the appropriate commands (i.e. PATH resolution)
    \\

    \textbf{System calls} : calls to the OS to develop certain specific functions such as opening files or starting sub-processes. Many OS use API's 
    on top of system calls to make portability more achievable.
    \\

    \textbf{System-call interface} : the interface provided by programming languages to interface with the system calls in different OS' which the compiler knows the specifics of.
}

\nDefinition{Syscalls}{
    The user cannot perform IO operations by himself, he must ask the OS to do it for them.

    Syscalls define procedures which the OS performs for the user in privileged mode.

    Usually implemented as vectors, where each syscall is simply an offeset to a base address.

    Mechanically just a procedure call (but is not one! in a normal procedure call the caller knows the location of the procedure, in this case a syscall is just an ID), the caller puts arguments in a place the callee expects,
    then retrieves output from known place. Usually each system call will have wrapper functions provided by each 
    programming language, i.e. the \textbf{system call interface}, which the compiler understands.

    \nImgs[0.48]{OS-3}{OS-4}
}

\nDefinition{OS Structures}{
    \nImg[1]{OS-5}
    \nDefinition{Monolithic Kernel}{
        All major subsystems implemented in kernel. 

        \begin{itemize}
            \item[+] low system interaction cost (procedure call)
            \item[-] hard to understand and modify 
            \item[-] unrelible (no isolation between system modules)
            \item[-] hard to maintain   
        \end{itemize}
    }
    \nDefinition{Microkernel}{
        Minimize what goes into kernel, implement rest of OS 
        as user-level processes 

        \begin{itemize}
            \item[+] better reliability (isolation between components)
            \item[+] ease of extension and customization (easy to replace parts)
            \item[-] poor performance (a lot of user/kernel switches)  
        \end{itemize}
    }

    \nDefinition{Layered Kernel}{
        Implement OS as a set of layers, each layer interacts only with layer below.
        \begin{center}
            \nImg[0.5]{OS-6}    
        \end{center}    
    
        \begin{itemize}
            \item[+] more reliable (separation of components)
            \item[-] strict layering isn't flexible enough - in real life modules might need to communicate with not only nearby layers.
            \item[-] poor performance, each layer crossing has overhead associated with it (due to API generalization)
            \item[-] Disjunction between model and reality - system modelled as layers, but not really built that way   
        \end{itemize}
    }

}


\nDefinition{Dynamically loadable kernel modules}{
    Core services in the kernel, others dynamically loaded.
    \\

    Common in modern implementations:
    \begin{itemize}
        \item \textbf{Monolithic}: load the code in kernel space (Solaris, Linux, etc.)
        \item \textbf{Microkernel}: load the code in user space (any)
    \end{itemize}
    

    \begin{itemize}
        \item[+] Convenient: no need for rebooting for newly added modules
        \item[+] Efficient: no need for message passing unlike microkernel
        \item[+] Flexible: any module can call any other module unlike layered model  
        \item[-] Memory fragmentation: fragments OS memory which is normally unfragmented when loaded initially
    \end{itemize}
}

\nDefinition{Hybrid OS Design}{
    Many different approaches. Key idea: exploit the benefits of monolithic and microkernel designs.
    Extensibility via kernel modules.
    
}

\nSection{IO}

\nDefinition{I/O}{
    Variety of I/O Devices
    \\

    \textbf{Port} : Connection point for a device (e.g., USB, parallel, serial, ethernet)
    \\
    \textbf{Bus} : Peripheral buses (e.g. PCI/PCIe), Expansion bus connects relatively slow devices 
    \\
    \textbf{Device}
    \\
    \textbf{Controller(host adapter)} : electronics that operate port, bus, device (sometimes integrated). Contains processor, microcode, private memory, bus controller etc. 

    \begin{center}
        \nImgs[0.46]{OS-7}{OS-8} 
    \end{center}
    
    Controllers have \textbf{registers} for data and control as well as \textbf{buffers}, mostly for data.
    Communication Methods:
    \begin{itemize}
        \item \textbf{IO Ports}
        \item \textbf{Memory-mapped IO}
        \item Hybrid
    \end{itemize}
}

\nDefinition{I/O Ports}{
    \begin{itemize}
        \item Each control register has an I/O port number
        \item Special instructions exist to access the I/O port space
        \item CPU reads in from device I/O PORT to CPU register (IN REG, PORT)
        \item CPU writes to device I/O PORT from CPU register (OUT PORT, REG)
        \item Instructions are privileged (OS kernel only)
        \item Seperate I/O Port space and memory space:
            
        \nImg[0.8]{OS-9}
    \end{itemize}

    \begin{center}
        \nImg[0.6]{OS-10}
    \end{center}
}

\nDefinition{Memory Mapped I/O}{
    \begin{itemize}
        \item All control registers and buffers mapped into the memory space 
        \item Each control register is assigned a unique memory address
        \item There is no actual RAM memory for that address
        \item Such addresses may be at the top of the physical address space 
        
            \nImg[0.7]{OS-11} 
    \end{itemize}

    \begin{center}
        \nImg{OS-12}
    \end{center}
}

\nDefinition{Hybrid I/O}{
    Simply do both, use memory mapped I/O for the \textbf{data buffers}, and keep separate I/O ports for \textbf{control registers}.
}

\nDefinition{Offloaded Communication}{
    The CPU can request data from an I/O controller one byte at at time (Programmed IO).
    This wastes CPU time for large data transfers, small data transfers are ok.

    CPU can instead offload data transfers using DMA:
    \\

    \textbf{DMA (Direct Memory Access) controller} transfers data fro the CPU, either from/to IO or between IO devices.
    \\

    This requires a DMA controller either on the device's host controller, and the motherboard.
    This controller contains registers to be read/written by the software:
    \begin{itemize}
        \item Memory address register
        \item byte count register 
        \item Control registers: direction, unit, byte burst size etc..
    \end{itemize}
}

\nDefinition{OS Device drivers}{
    Great variety of devices, each one has very different specs.

    OS Deals with IO devices in a standard and uniform way.
    Each type of driver is an interface which the vendor can implement as a class. Each OS has its own standard.

    \begin{center}
        \nImg[0.8]{OS-13}
    \end{center}

}

\nDefinition{Life cycle of an IO request}{
    \begin{center}
        \nImg[1]{OS-14}
    \end{center}   
}

\nSection{Processes}

\nDefinition{Process}{
    Process is the OS's abstraction for execution.

    \begin{itemize}
        \item Program is the list of instructions, initialized data, etc 
        \item A process is a \textbf{program in execution}
        \item A single flow/sequence of instruction in execution 
        \item An address space (an abstraction of the CPU)
    \end{itemize}

    Only one process can be running on a processor core at any instant

    Contents:
    \begin{itemize}
        \item An address space, containing:
            \begin{itemize}
                \item Code (instructions) for running program
                \item Data for the running program (static data, heap data, stack)
            \end{itemize}
        \item A CPU state, consisting of 
            \begin{itemize}
                \item Program counter, indicating the next instruction 
                \item Stack pointer, current stack position 
                \item Other general-purpose register values
            \end{itemize}
        \item A set of OS resources 
            \begin{itemize}
                \item Open files
                \item network connections 
                \item sounc channels
            \end{itemize}
    \end{itemize}
    
    I.e. everything needed to run the program
    
    \begin{center}
        \nImg[0.7]{OS-15}
    \end{center}


    Each process is identified by a process ID (\textbf{PID}). PID's are unique and global. With certain exceptions (cgroups)
    \\

    Operations that create processes return a PID, and those which operate on processes accept PID's as arguments
    }

\nDefinition{Process representation}{
    Each process is represented internally by the OS with a \textbf{Process control block (PCB)} or process/task descriptor,
    identified by the PID.

    OS keeps all of a process's execution state in (or linked from) the PCB when the process isn't running:
    \begin{itemize}
        \item PC, SP, registers etc.
        \item when a process execution is stopped, its state is transferred out of the hardware into the PCB
    \end{itemize}
    When the process is running its state is spread between the PCB and the hardware (CPU regs)

    PCB's contain:
    \begin{itemize}
        \item Process ID (PID)
        \item Parent process ID 
        \item Execution state 
        \item PC, SP, registers 
        \item Address space info 
        \item UNIX user ID, group ID 
        \item Scheduling priority 
        \item Accounting info 
        \item Pointers for state queues 
    \end{itemize}
    and likely many more.

    Whenever the OS gets control becase of :
    \begin{itemize}
        \item Syscalls
        \item Exceptions
        \item Interrupts
    \end{itemize}
    The OS then saves the CPU state into the PCB.
    Whenever the process is resumed into execution again, its PCB is loaded onto the machine registers SP, PC etc.

    This is called a \textbf{Context switch}
}

\nDefinition{Context Switch}{
    \begin{center}
        \nImg[0.8]{OS-16}
    \end{center}
}

\nDefinition{Execution states}{
    Each process has an \textbf{Execution state}, which indicates what it's currently doing.
    \begin{itemize}
        \item \textbf{Ready}: waiting to be assigned to a CPU 
        \item \textbf{Running}: executing on a CPU
        \item \textbf{Waiting}: Waiting for an event, e.g. IO completion, or a message from another process.
    \end{itemize}

    \begin{center}
        \nImg[0.8]{OS-17}
    \end{center}
}

\nDefinition{State queues}{
    The OS maintains a collection of queues, that represent the state of all processes in the system.
    Typically one queue for each state (executing, waiting etc..)
    Each PCB is queued onto a state queue according to the current state of the process it represents.
    As a process changes state, its PCB is unlinked from one queue, and then linked onto another.
    \\

    There may be many wait queues, one for each type of wait (specific device, timer, message) etc.
}

\nDefinition{Process creation}{
    New processes are created by existing processes (parent-child)

    The first process is started by the OS, everything else stems from it (init in linux)
    \\

    Depending on OS, child processes inherit certain attributes of parent, (i.e. open file table: implies stdin/stdout/stderr)
    Some systems divide resources of parent between children.

    }

\nDefinition{UNIX - fork()}{
    On UNIX systems, process creation is done through the fork() system call:
    \begin{itemize}
        \item Creates and initializes a new PCB 
        \item Initializes kernel resources of new process with resources of parent (e.g. open files)
        \item Initializes PC,SP to be same as parent
        \item Creates a new address space, which is an identical copy of this of the parent's (by value)
    \end{itemize}

    The fork call returns "twice" once in the parent, and once in the child.
    In the child the PID returned is 0 and in the parent, the child's PID is returned.

    \begin{center}
        \nImg[0.8]{OS-18}
    \end{center}

}

\nDefinition{UNIX - exec()}{
    In order to start a new program instead of just copying the old one, we must use exec().
    Which is the call which stops the current process, loads a new program into the address space, 
    initializes the hardware context and args for the new program and finally places the PCB onto the ready queue

    \begin{center}
        \nImg[0.8]{OS-19}
    \end{center}

    Alternatively use vfork() which is faster but less safe
    }

\nDefinition{UNIX - vfork()}{
    Same as fork, but hte child's address space \textbf{IS} by address the same space as the parents.
    
    Usage relies on the child not modifying the address space before doing an execve() call, otherwise bad things can happen.

    \begin{center}
        \nImg[0.7]{OS-20}
    \end{center}

}

\nDefinition{Copy-on-write (COW) fork()}{
    Retains original semantics, but copies "only what is necessary" rather than the entire address space.

    On fork():
    \begin{itemize}
        \item create a new address space 
        \item initialize page tables with same mappings as parents (identical)
        \item Set both parent and child page tables to make all pages read-only
        \item if either parent or child writes to memory, an exception occurs
        \item On exception, OS copies the page, adjusts page tables, etc..
    \end{itemize}
}

\nSection{Interprocess communication}


\nDefinition{Shared memory}{
    Allow processes to communicate and synchronize:
    \begin{itemize}
        \item Sharing part of address space 
        \item OS doesn't mediate communication (no overhead)
        \item Usually OS prevents processes from accessing each other's memory
        \item Processes should agree to void this restriction
    \end{itemize}

    Data:
    \begin{itemize}
        \item Format decided by application
        \item Direct access (not mediated by the OS) - very fast 
        \item Application programmer fully manages the data transfer - not trivial
    \end{itemize}

    Possible use cases:
    \begin{itemize}
        \item Passing of large (single) objects (image)
        \item Notification variable 
    \end{itemize}

    \begin{center}
        \nImg[0.9]{OS-21}
    \end{center}
}

\nDefinition{Message Passing}{
    Allow processes to communicate and synchronize:
    \begin{itemize}
        \item Without sharing part of address space 
        \item OS mediates communication (overhead likely)
    \end{itemize}

    Works with processes on the same machine and also those on different inter-networked machines!
    This is not possible with shared memory.

    Message passing facility provides at least two operations:
    \begin{itemize}
        \item Send(message)
        \item Receive(message)
    \end{itemize}

    Communication link:
    \begin{itemize}
        \item Several implementation tradeoffs, .e.g. messages size
        \item Fixed 
        \item Variable
    \end{itemize}
    
    Communicating processes must refer to each other:
    \begin{itemize}
        \item Direct communication:
            \begin{itemize}
                \item Symmetric: explicit name of sender and receiver
                    \begin{itemize}
                        \item send(P, message) - send message to P
                        \item receive(P, message) - receive message from P
                    \end{itemize}
                \item Assymetric: Explicit at least on one end:
                    \begin{itemize}
                        \item send(P, message) - send to p 
                        \item receive(id, message) - receive from any process, sender saved in id
                    \end{itemize}
            \end{itemize}
        \item Indirect communication:
            \begin{itemize}
                \item No need to know/explicitly in advance sender and/or receiver 
                \item Mailboxes (e.g., POSIX mailbox)
                    \begin{itemize}
                        \item send(A, message) - send message to mailbox A
                        \item receive(A, message) - receive message from mailbox A 
                    \end{itemize}
                \item A mailbox can be accesed by more than two processes
                \item Multiple mailboxes might exist between processes 
            \end{itemize}
    \end{itemize}

        send() and receive() calls might be implemented as \textbf{blocking} or \textbf{synchronous} as well as \textbf{nonblocking} or \textbf{asynchronous}.
    different combinations of these might be offered:
    \begin{itemize}
        \item Blocking send 
        \item Nonblocking send 
        \item Blocking receive 
        \item Nonblocking receive
    \end{itemize}

    \textbf{Rendezvous} - When both send and receive are blocking
    }

\nDefinition{Buffering}{
    Messages exchanged reside in temporary buffers/queues with either: 
    \begin{itemize}
        \item Zero capacity (no buffering) - no message waiting, sender must block until recipient receives message
        \item Bounded capacity - n messages may reside. If the queue is not full, then nonblocking, otherwise blocking
        \item Unbounded - never blocks
    \end{itemize}
}

\nDefinition{Example implementation: Pipes}{
    A pipe acts as a conduit allowing two processes to communicate \textbf{one-way} only and either \textbf{Annonymously} or \textbf{Named}.
    
    \begin{center}
        \nImg[0.9]{OS-22}
    \end{center}
    
    Mechanically act like file descriptors (streams)
}


\nDefinition{Client-Server communication}{
    \begin{itemize}
        \item \textbf{Sockets} abstraction
            \begin{itemize}
                \item  endpoint for communication 
                \item identified by an IP address concatenated with a port number 
                \item servers implementing specific services (SSH, FTP, HTTP) listen to well-known ports
                \item an SSH server listens to port 22, an FTP server listens to port 21, a web or http server listens to port 80
            \end{itemize}
        \item \textbf{Remove procedure call} (RPC)
            \begin{itemize}
                \item Abstract the procedure-call mechanism 
                \item for use between systems with network connections
                \item similar in many respects ot the IPC 
                \item uses message-based communication to provide remote service
            \end{itemize}
    \end{itemize}
}

\nDefinition{Signals}{
    \begin{itemize}
        \item OS mechanism to notify a process (one way)
        \item From the OS POV can be thought as a software-generated interruption/exception (synchronous or asynchronous)
        \item From other processes POV, it is only a notification, no data is transferred. A communication method for: management, synchronization etc.
    \end{itemize}

    UNIX: 
    signal handlers must be registered, and provide code for handling the signal.
    \begin{center}
        \nImg[0.50]{OS-23}
    \end{center}
}

\nSection{Threads}

\nDefinition{Concurrency}{
    Concurrency is carrying out multiple tasks in parallel but only one at the same time instance:
    \begin{center}
        \nImg{OS-24}
    \end{center}
}
\nDefinition{Parallelism}{
    Parallelism is carrying out multiple tasks in parallel at the same time instance:
    \begin{center}
        \nImg{OS-25}
    \end{center}
}

\nDefinition{Communication problems}{
    Multiple processes are required for both concurrency and parallelism, this requires communication. But the 
    methods discussed thus far have limited usability.
    \\

    Message passing:
    \begin{itemize}
        \item[-] slow, OS mediates
    \end{itemize}
    Shared Memory:
    \begin{itemize}
        \item[-] limited shareability, not all pointers work (both processes have different virtual memory layouts)
        \item[-] OS resources not shared by default - cumbersome
    \end{itemize}


    Possible solution:
    \begin{enumerate}
        \item Fork several processes 
        \item cause each of them to map to the same shared memory (shmget())
        \item make them open the same OS resources
    \end{enumerate}
    
    Couple problems:
    \begin{itemize}
        \item[-] cumbersome 
        \item[-] has limited shareability again 
        \item[-] inefficient - takes a long time to create all this, requires a PCB per process etc..   
    \end{itemize}
}

\nDefinition{Threads}{
    Threads are a great solution to the communication problem:
    Instead of spawning a new process per task, use \textbf{threads}.
    \\

    Each thread is part of the same spawning process, shares \textbf{address space \& OS resources}.
    \\

    Threads only differ in their \textbf{execution state (instruction flow)} i.e. private stack, and CPU state

    \begin{center}
        \nImg[0.7]{OS-26}
    \end{center}

    A thread abstracts the execution state away from a process, and now a process represents the static parts of a task (address space, OS resources etc)

    \begin{center}
        \nImgs[0.46]{OS-27}{OS-28}
    \end{center}

    Threads become the \textbf{unit of scheduling} (depending on implementation of course).

    Think processes are boxes for threads in which they execute.
    }

\nDefinition{Thread Control Block - TCB}{
    On the OS side, the PCB need to be adjusted to accomodate threads, easiest way is to create sub blocks for each thread 
    representing execution state:

    TCB contains:
    \begin{itemize}
        \item Program counter 
        \item CPU registers 
        \item Scheduling information 
        \item Pending I/O information 
    \end{itemize}

    PCB stores:
    \begin{itemize}
        \item Memory management information
        \item Accounting information
    \end{itemize}

    \begin{center}
        \nImgs[0.46]{OS-29}{OS-30}
    \end{center}
}


\nDefinition{User vs Kernel level threading}{
    Threading can be either implemented as part of the OS, or as a library in the user space 

    \nImgs[0.45]{OS-31}{OS-32}

    \nDefinition{Kernel level threading 1:1}{
        \begin{itemize}    
            \item OS allocates and manages threads
            \item TID's are used to identify threads
            \item[+] if one thread blocks, the OS can run other threads within the same process 
            \item[+] possiblity to efficiently overlap IO and CPU time within a process
            \item[+] Threads are cheaper than processes - less state to manage
            \item[-] pretty expensive for fine-grained use
                \begin{itemize}
                    \item[-] Orders of magnitute more expensive than procedure calls 
                    \item[-] thread operations are syscalls (context switches + argument checks)
                    \item[-] Must maintain kernel state for each thread
                \end{itemize}

        \end{itemize}
    }

    \nDefinition{User Level threading 1:N}{
        \begin{itemize}
            \item Threads managed at the user level, within the process 
            \item A library in the program manages the threads 
            \item the thread manager doesnt need to manipulate address spaces (Only OS can)
            \item Threads differ only in hardware contexts (PC,SP,registers), which can be manipulated by user-level code
            \item The thread package multiplexes user-level threads in a process
            \item TID's are now unique per process not globally
            \item No context switching between thread operations, these are done via procedure calls now (10-100x times faster than kernel threads)
            \item[-] if one thread tries to do IO, the whole process is blocked!

        \end{itemize}
    }

    \nDefinition{N:M Threading}{
        Best of both worlds, can start OS level threads for threads which will use IO.
    }
    }

\nSection{Scheduling}

\nDefinition{Dispatcher}{
    Mechanism used to switch between tasks (save and restore state)
}

\nDefinition{Scheduler}{
    Decides on policy (implemented by scheduling algorithms) for ordering excution of tasks (threads/processes)
}

\nDefinition{CPU Bursts}{
    Bursts of CPU processing done by a task. \textbf{Application dependent}
}

\nDefinition{IO Bursts}{
    Similar to CPU Burst but for IO operations
}

\nDefinition{Performance goals}{
    \begin{itemize}
        \item CPU Utilization 
        \item Throughput (processes completed per unit time)
        \item Turnaround time (time from submission of task to completion)
        \item Waiting time (all periods spent waiting in the ready queue from submission)
        \item Response time (time from submission of request to when response produced)
        \item Energy (joules per instruction) subject to some constraint (fps)
    \end{itemize}

    In most cases we optimize the \textbf{average metric}. Goals may be conflicting
}

\nDefinition{Fairness}{
    No single compelling definition of fair for process resource allocation.
    \\
    
    Sometimes goal is to be unfair and prioritize some classes of requests higher.
    \\

    We want to avoid starvation - everyone needs at least some service
}

\nDefinition{Classes of schedulers}{
    \begin{itemize}
        \item Batch - throughput / utilization oriented
        \item Interactive - response time oriented 
        \item Real time - deadline driven
    \end{itemize}
}

\nDefinition{Preemptive scheduling}{
    \textbf{Non-preemptive} scheduling:
    \begin{itemize}
        \item Processes/threads execute until completion or until they want
        \item The scheduler gets involved only at exit or on request
    \end{itemize}

    \textbf{Preemptive} scheduling:
    \begin{itemize}
        \item While a process/thread executes, its execution may be paused, and another process/thread resumes its eecution
        \item Involountary process switch
    \end{itemize}
}

\nSection{Scheduling algorithms}

\nDefinition{First-come First-served (FCFS)}{
    Processes/tasks served in the order they arrive:
    \\

    \begin{tabular}{|c|c|c|}
        \hline
        Process & CPU time & Turnaround time \\ \hline
        P1 & 24 & 24 \\ \hline 
        P2 & 3 & 24 + 3 = 27 \\ \hline 
        P3 & 3 & 24 + 3 + 3 = 30 \\ \hline
    \end{tabular}
    \\ 

    Execution order: P1,P2,P3 

    Avg. Turnaround time: $\frac{24 + 27 + 30}{3} = 27$

    \begin{itemize}
        \item[-] Non pre-emptive 
        \item[-] Poor average response time 
        \item[-] poor utilisation of \textbf{other resources} - a CPU-intensive job prevents I/O- intensive job from tiny bit of computaion on the CPU before returning to IO and keeping disk busy  
    \end{itemize}
}

\nDefinition{Shortest Job First (SJF)}{
    Associate with each process the length of its CPU time 
    Sort jobs, shortest CPU time goes first 
    Can be preemptive (simply re-sort including the current process - Shortest Remaining Time Next, SRTN)
    \\

    \nDefinition{Non-Preemptive}{
        \begin{tabular}{|c|c|c|c|}
            \hline
            Process & Arrival time & CPU time & Turnaround time \\ \hline
            P1 & 0 & 7 & 7 \\ \hline 
            P2 & 2 & 4 & 12 - 2 = 10 \\ \hline 
            P3 & 4 & 1 & 8 - 4 = 4 \\ \hline
            P4 & 5 & 4 & 16 - 5 = 11 \\ \hline
    
        \end{tabular}
        \\ 
    
        Execution order: P1,P3,P2,P4
    
        Avg. Turnaround time: $\frac{7 + 10 + 4 + 11}{4} = 8$
    
    }

    \nDefinition{Preemptive}{
        \begin{tabular}{|c|c|c|c|}
            \hline
            Process & Arrival time & CPU time & Turnaround time \\ \hline
            P1 & 0 & 7 & 16 \\ \hline 
            P2 & 2 & 4 & 7 - 2 = 5 \\ \hline 
            P3 & 4 & 1 & 5 - 4 = 1 \\ \hline
            P4 & 5 & 4 & 11 - 5 = 6 \\ \hline
    
        \end{tabular}
        \\ 
    
        Execution order: P1 (2s),P2 (2s),P3(1s),P2(2s),P4(4s),P1(5s)
    
        Avg. Turnaround time: $\frac{16 + 5 + 1 + 6}{4} = 7$
    
    }

    
    \begin{itemize}
        \item[+] Preemptive is optimal 
        \item[-] Too complex, to be implemented in practice 
        \item[-] not always possible to determine the CPU/IO burst lenghts  
    \end{itemize}
}



\nDefinition{Round-robin (RR)}{
    Processes run in discrete time slots, after each time slot a new process/task is chosen to be run
    \\

    Time quantum = 20  
    \begin{tabular}{|c|c|c|}
        \hline
        Process & CPU time & Turnaround time \\ \hline
        P1 & 53 & 125 - 0 = 125 \\ \hline 
        P2 & 8 & 28 - 0 = 28 \\ \hline 
        P3 & 68 & 153 - 0 = 153 \\ \hline
        P4 & 24 & 112 - 0 = 112 \\ \hline

    \end{tabular}
    \\ 

    Execution order: P1(20s),P2(8s),P3(20s),P4(20s),P1(20s),P3(20s),P4(4s),P1(13s),P3(20s),P3(8s)
    \\

    Avg. Turnaround time: $\frac{125 + 28 + 153 + 112}{4} = 104.5$

    Long time quanta cause poor response times with a lot of processes, a too low time quanta, causes a lot of context switching loss.
    \begin{itemize}
        \item[+] Solves fairness and starvation 
        \item[+] Fair allocation of CPU across jobs
        \item[+] Low average waiting time when job lengths vary
        \item[+] Good for responsiveness (interactivity) if small number of jobs
        \item[-] Context switching time may add up for long jobs   
    \end{itemize}
}

\nDefinition{Priority (PRIO)}{
    Always execute highest-priority runnable jobs to completion
    \\

    \begin{tabular}{|c|c|c|c|}
        \hline
        Process & CPU time & Priority & Turnaround time \\ \hline
        P1 & 10 & 3 & 16 \\ \hline 
        P2 & 1 & 1 & 1 \\ \hline 
        P3 & 2 & 4 & 18 \\ \hline
        P4 & 1 & 5 & 19 \\ \hline
        P5 & 5 & 2 & 6 \\ \hline

    \end{tabular}

    Execution order: P2,P5,P1,P3,P4
    \\

    Avg. Turnaround time: $\frac{16 + 1 + 18 + 19 + 6}{5} = 12$

    How to assign priorities ? Based on process type, User, price paid etc. or 
    dynamically, based on how long the process ran etc..

    \begin{itemize}
        \item[-] Starvation - lower priority jobs dont get to run because higher priority always running 
        \item[-] deadlock - priority inversion - happens when a low priority task has lock needed by high priority task (busy waiting) 
    \end{itemize}
}

\nDefinition{Multiple Queues(MQ)}{
    Multiple round-robin scheduled queues, with queues of higher priority always scheduled first
    \\

    \begin{center}
        \nImg[0.6]{OS-33}
    \end{center}

    Time quantum = 2
    \begin{tabular}{|c|c|c|c|}
        \hline
        Process & CPU time & Priority & Turnaround time \\ \hline
        P1 & 10 & 3 & 19 \\ \hline 
        P2 & 1 & 1 & 1 \\ \hline 
        P3 & 2 & 3 & 10 \\ \hline
        P4 & 1 & 3 & 11 \\ \hline
        P5 & 5 & 2 & 6 \\ \hline
    \end{tabular}

    Execution order: P2(1s),P5(5s),P1(2s),P3(2s),P4(1s),P1(8s)
    \\

    Avg. Turnaround time: $\frac{19 + 1 + 10  + 11 + 6}{5} = 9.4$

    How to assign priorities ? Based on process type, User, price paid etc. or 
    dynamically, based on how long the process ran etc..

    \begin{itemize}
        \item[-] Starvation - lower priority jobs dont get to run because higher priority always running 
        \item[-] deadlock - priority inversion - happens when a low priority task has lock needed by high priority task (busy waiting) 
    \end{itemize}
}

\nDefinition{Multilevel Feedback Queue(MLFQ)}{
    Same as MQ but each queue has a different time quanta. Time quanta are increasing inversely with priority of queue (higher priority lower time quanta).
    Each process starts in queue 1 - but when it exceeds its quanta it's pushed lower in the queues. When a process becomes inactive it is moved to a higher priority.
    This can be gamed by making a process interactive.
    \\

    \begin{center}
        \nImg[0.6]{OS-34}
    \end{center}

    Time quanta = (1,2,4,8,16,32,64)
    \begin{tabular}{|c|c|c|}
        \hline
        Process & CPU time & Turnaround time \\ \hline
        P1 & 100 & 102 \\ \hline 
        P2 & 2 & 3 \\ \hline 
    \end{tabular}

    Execution order: P1(1s),P2(1s),P1(2s),P2(1s),P1(4s),P1(16s),P1(37s)
    \\

    Avg. Turnaround time: $\frac{102 + 3}{2} = 52.5$

    8 context switches vs 101 with fixed quanta
    \begin{itemize}
        \item[-] Starvation - lower priority jobs dont get to run because higher priority always running 
        \item[-] deadlock - priority inversion - happens when a low priority task has lock needed by high priority task (busy waiting) 
    \end{itemize}
}


\nSection{Memory}


\nDefinition{Why use abstraction}{
    In the case that a program sees the physical memory directly,
    the program can mess with the OS and BIOS code, whether intentionally or unintentionally.
    There is a need for a protection system.
    \\

    The situation gets worse when multiple programs are running with access 
    to direct physical memory. In which case each program can alter all the other running programs.
    The total number of programs that can be run is then limited by the memory size!

    \begin{center}
        \nImg{OS-35}
    \end{center}
}


\nDefinition{Protection - Base and Limit registers}{
    A simple solution to prevent bad memory accesses, is to use base and limit registers.

    This requires hardware support, but is enough to allow multiple \textbf{relocatable} programs to be run on the same physical address space.

    \begin{center}
        \nImgs[0.45]{OS-36}{OS-37}
    \end{center}

}

\nDefinition{Address Space}{
    Abstraction from physical memory space. Defines the set of memory addresses that a process can use independently from other processes.
    \\ 

    To make it easier to manage memory of multiple processes, we can make 
    processes use \textbf{logical addresses} instead of physical ones! 

    These logical addresses are independent of physical addresses, yet the data lives in physical memory, so the OS manages the data location in the physical memory.
    Logical addresses are \textbf{translated by hardware} with the OS' aid.
    \\ 

    \begin{center}
        \nImg[0.8]{OS-38}
    \end{center}
    }

\nDefinition{Memory-Management Unit (MMU)}{
    A hardware component which translates CPU generated addresses to physical addresses.
    \\ 

    Programs deal with logical addresses and never see the physical ones.
    \\ 

    Can be implemented in many ways (paging, relocation+limit registers, segmentation etc.)


    \begin{center}
        \nImg{OS-39}
    \end{center}

    \nDefinition{Relocation register}{
        \centering
        \nImg{OS-40}

        \raggedright
        \begin{itemize}
            \item[-] no protection, nothing checks if address is outside of range  
        \end{itemize}
    }
    \nDefinition{Relocation + limit registers}{
        \centering
        \nImg{OS-41}

        \begin{itemize}
            \item[+] program doesn't have to be relocatable (it operates on "fake" physical memory) = simpler loader and faster load time
            \item[+] protection - each program has its own private address space 
            \item[+] each program can have a different partition size (amount of physical memory allocated)
        \end{itemize}
    }
}



\nDefinition{Memory allocation}{
    The process of allocating physical memory to programs requiring it. 

    We want efficient and fast allocation with smallest memory waste. Allocation is done with hardware support (MMU)
}

\nDefinition{Contigious Allocation}{
    All techniques above allocate memory in 
    contigious chunks.

    \nDefinition{Multiple-partition allocation}{
        Allocation of contigious block of physical memory of various sizes (to minimize loss)
        \\ 

        Hole: a block of available memory; holes have various sizes.
        When a process arrives, OS allocates memory from a hole large enough to accomodate it.
        On process exiting, the partition is returned to OS, and adjacent holes are merged. 
        OS keeps information about allocated partitions and free partitions.

        \begin{itemize}
            \item[-] degree of multiprogramming limited by number of partitions used
        \end{itemize}

        \centering 
        \nImg{OS-42}
        \raggedright

        Many possible policies:
        \nDefinition{First-fit}{
            Allocate the \textbf{first} hole that is big enough
        }

        \nDefinition{Best-fit}{
            Allocate the \textbf{smallest} hole that is big enough; must search entire list, unless ordered by size
            - produces the smallest leftover hole 
        }

        \nDefinition{Worst-fit}{
            Allocate the \textbf{largest} hole; must also search entire list 
            - produces the largest leftover hole
        }

        First and best fit better than worst fit in terms of speed and storage utilization.
    }
}

\nDefinition{Swapping}{
    What to do when we run out of memory? 
    We have to keep some programs in the backing storage.
    To do this we need to be able to swap in and out programs from memory.

    \centering 
    \nImg{OS-43}
}

\nDefinition{Growing programs problem}{

    Programs are not actually static! They can allocate memory dynamically, and hence grow in memory.

    Two solutions possible:
    \begin{itemize}
        \item[a)] Allocate more space than needed, if more needed, relocate
        \item[b)] Allocate space for growing program in its address space
    \end{itemize}

    \centering 
    \nImg[0.6]{OS-44}

    Both of these aren't ideal
}

\nDefinition{Memory fragmentation}{
    Processes create memory holes, which might not be convieniently sized for other programs to occupy:

    \centering 
    \nImg{OS-45}

    \raggedright

    \textbf{External fragmentation} - you allocate the exact amount of memory requested, total memory space exists to satisfy a request, but it is not contiguous.
    
    \textbf{Internal fragmentation} - you allocate more than what required, allocated memory may be slightly larger than requested memory

    \textit{First fit analysis - N blocks allocated = 0.5 N blocks lost to fragmentation, 1/3 may be unusable}
}

\nSection{Paging}

\nDefinition{Non-Contigious Allocation}{
    Issues with Contigious allocation:
    \begin{itemize}
        \item loads the entire program at once into memory when needed
        \item[-] external fragmentation 
        \item[-] long swap times 
        \item[-] long \textbf{compaction} times (tight reallocation)
    \end{itemize}

    Presenting Non-contigious allocation:
    \begin{itemize}
        \item request physical space for each program's chunk when required
        \item split logical address space in chunks 
        \item contigious in logical memory, non-contigious on physical memory
        \item segmentation + paging 
    \end{itemize}
}

\nDefinition{Segmentation}{
    Partition an address space into \textbf{variable size} chunks/units.
    \textbf{Logical units} = stack,heap,data,code,subroutines .. 
    \\

    A logical address is split into (segment\#, offset)
    \\
    The logical address space is divided in \textbf{variable size} chunks

    \begin{center}
        \nImg[0.7]{OS-46}
    \end{center}


    \nDefinition{Hardware support}{
        Segment table:
        \begin{center}
            \nImg[0.7]{OS-47}
        \end{center}

        Multiple base/limit pairs, one for each part of the program (segment)
        the segment number is used as the index into the segment table
        
        The segments can roughly be associated with addresses based on conventions.
    
    
    }

    \begin{itemize}
        \item[+] allows non-contiguous physical addresses 
        \item[+] reduces fragmentation by exploting varying sized holes
        \item[+] allocated chunks are smaller than the entire program address space 
        \item[+] enables sharing (same segment can be shared across processes, think threads)
        \item[-] process view and physical memory are extremely different 
        \item[+] protection, process can only access its own memory        
        \item[-] external fragmentation still occurs
    \end{itemize}
}


\nDefinition{Paging}{
    Similar to segmentation, but logical address space divided in \textbf{fixed size} chunks
    Here we \textbf{combine} base address with offset instead of adding to entire address by single offset, this establishes
    a mapping. Each process might have its own page table, or a single one can be used.
    \\ 

    Pages mapped to \textbf{frames}, page size = frame size 
    \\

    \begin{center}
        \nImg[0.7]{OS-48}
    \end{center}

    Logical address is (page \#, page offset). page \# is used as index into \textbf{page table} which again contains base addresses,
    page offset is \textbf{combined} with base address to create physical memory address.

    \begin{center}
        \nImg{OS-49}
    \end{center}

    \textit{logical address space size = $2^m$ bytes. Page and frame size = $2^n$ bytes}

    \nDefinition{Hardware Support}{
        \begin{center}
            \nImg[0.8]{OS-50}
        \end{center}
    }

    \begin{itemize}
        \item[+] no external fragmentation!
        \item[-] internal fragmentation depends on page size, if program "overflows" when dividing by page size. 
        \item[-] translations may require memory accesses (costly)
        \item[-] different page sizes available on every system 
        \item \textit{average fragmentation = .5 frame size, worst case = frame size - 1 byte}
        \item[-] page tables get huge, require more complex arrangements to cut size down
    \end{itemize}
    }

\nDefinition{Translation look-aside buffers (TLB)}{
    In order to avoid wasteful memory lookups, a TLB cache is used when paging.

    If translation exists in TLB, use it (free), otherwise fetch translation from memory (expensive)

        \begin{center}
            \nImg[0.7]{OS-51}
        \end{center}



    \textbf{Memory access time} - time the CPU waits to access main memory directly 
    
    \textbf{Hit ratio ($\alpha$)} - percentage of times taht a page number is found in TLB 
    
    \textbf{Miss ratio} - 1 - Hit ratio 

    \textbf{Effective access time (EAT)} - the average experienced access time:
    \\ 

    with $\alpha = 80\%$, 100ns access times = $0.8 * 100 + 0.2 * 200 = 120ns$

    with $\alpha = 99\%$, 100ns access times = $0.00 * 100 + 0.01 * 200 = 101ns$
    }

\nDefinition{Memory protection}{
    Page table entries (PTEs) can contain more information:

    \begin{itemize}
        \item protection bits - read-only, read-write,execute-only
        \item valid bits - valid = translation exists and is in the logical address space of process otherwise not in process' space (when more frames than logical pages)
        \item violations result in traps
    \end{itemize}

    \begin{center}
        \nImg[0.6]{OS-52}
    \end{center}
}

\nDefinition{Multi level TLBs}{
    Consider 32-bit logical address space, with page size of 4kB ($2^{12}$).
    The entire address space is divided into 1 million frames ($2^{32}/2^{12}$)
    Each entry is 32 bits and so we need 4MB in total for the page table


    We can create a table for the page table itself, to reduce the amount of "page translations" held in memory, we would only need to store the outer table entries:

    So the first half of the address gets split according to the hierarchy,
    i.e. 22 bit page number can be divided into 12 bits for the outer and 10 for the inner tables.
    this means we only have to store the $2^{12}$ entries of the page table
    \begin{center}
        \nImg{OS-54}
    \end{center}
    }

\nDefinition{Hashed page tables}{
    The logical page number can be hashed to generate the page table index,
    then the elements at that index need to be searched against (each entry would have the logical page number, the page frame, and the pointer to the next element)

    \begin{center}
        \nImg{OS-53}
    \end{center}

    Advantages: speed, fast access times
}

\nDefinition{Inverted Page table}{
    Instead of each process having a page table and keeping track of all possible logical pages,
    track all physical pages 

    each physical page = one entry 

    each entry has the virtual address owning that memory location along with the pid of the process whose virtual memory that is

    this requires less memory for translations, but much more time is needed to search through the table,
    we can use hash tables to limit the search to one/few page-table entries

    Shared memory requires OS intervention
    \begin{center}
        \nImg{OS-55}
    \end{center}
}

\nDefinition{Advantages of paging}{
    \begin{itemize}
        \item inter-process memory protection
        \item prevents code being rewritten (protection bits)
        \item detects null pointer dereferencing at runtime 
        \item reduce memory usage (shared libraries) - one copy of a library only necessary, not one per process
        \item generalizes use of "shared memory"
        \item can implement \textbf{copy-on-write(CoW)} mechanics easilly with read only pages and page faults 
        \item memory-mapped files - can load a file on demand into logical addresses
    \end{itemize}
}
\nSection{Virtual Memory}

\nDefinition{Overlays}{
    What if the logical address space is greater than the available physical memory ?
    Can a program correctly execute even if it is not all in memory at all times?

    One solution is to simply divide the program into \textbf{overlays}, which are smaller than the physical memory,
    then load and unload them as needed via some root overlay manager (user program) which determines when a new overlay needs to be loaded.

    This is something you'd need to use when really constrained on physical memory. However there are better solutions
}

\nDefinition{Virtual memory}{
    fully decouples address space from physical memory, allows larger logical address space than physical memory.

    We've already seen an example: \textbf{Paged virtual memory}!

    We can simply keep some of the pages out of the main memory and on the disk, and swap pages into memory when needed.
    This requires the \textbf{eviction} of older pages.

    Main memory becomes a \textbf{cache} for pages!

    \begin{center}
        \nImg{OS-56}
    \end{center}
}

\nDefinition{Page Fault}{
    \begin{enumerate}
        \item Software accesses a page that is not in memory 
        \item Hardware triggers a \textbf{page fault}
        \item Operating system checks internal data structures:
            \begin{itemize}
                \item If invalid reference, abort the original program
                \item If not in main memory currently, continue
            \end{itemize}
        \item Operating system finds a free frame - swaps page into frame via disk operation 
        \item Operating system sets internal data structures to indicate page now in memory + set valid bit in page table 
        \item Operating system restarts the instruction that caused the page fault 
    \end{enumerate}

    \begin{center}
        \nImg[0.7]{OS-57}
    \end{center}
}

\nDefinition{Demand paging}{
    Pages brought into memory \textbf{when accessed first}

    Few systems try to anticipate future needs, 
    pages may be clustered - OS can anticipate which pages usually get used together and bring them all in when one is accessed

    \begin{itemize}
        \item[-] expensive heavilly depends on storage latency 
    \end{itemize}
}

\nDefinition{Page allocation and replacement}{
    When there are no more free frames, we must \textbf{evict} unfortunate 
    pages in memory.
    
    \begin{center}
        \nImg[0.7]{OS-58}
    \end{center}
}

\nDefinition{Page replacement policies}{
    What page to evict ?

    Reduce page-fault rate by selecting \textbf{best victim page} - one that we will never touch again or in the near future.
    \\

    \textbf{Belady's Theorem} - evicting the page that won't be used for the longest period of time minimizes page fault rate
    \\

    evict \textbf{unmodified} pages first - since don't have to write them back to disk 

    \nDefinition{First-in-First-Out (FIFO)}{
        replace page that has been inserted the longest time ago

        \begin{itemize}
            \item[+] simple, just a linked list or queue
            \item Belady's anomaly, more page frames do not guarantee less page faults!
        \end{itemize}

        \begin{center}
            \nImg{OS-59}
        \end{center}
    }


    \nDefinition{Least Recently Used (LRU)}{
        Replace page that has not been used in the most amount of time

        \begin{itemize}
            \item[+] uses past knowledge rather than predictions
            \item[+] does not suffer from Belady's anomaly (stack algorithm) 
            \item[-] requires substantial \textbf{hardware assistance} - keeping track of last usage of each page
        \end{itemize}

        Need the following page table entry bits (\textbf{hardware}):
        \begin{itemize}
            \item Page referenced (if accessed or not)
            \item Page modified (if access was in write)
        \end{itemize}

        and need to keep track of a history for each page in the kernel.
    }

    \nDefinition{Second chance}{
        Variant of FIFO, makes use of page usage data.
        use circular queue and \textbf{reference bits}, keep "victim" pointer,
        if the victim was referenced, set reference bit to 0, and move pointer along by one, if not
        replace that page with new one, and move pointer along.
        \begin{center}
            \nImg[0.8]{OS-60}
        \end{center}
        }
}

\nDefinition{Frames as resources}{
    We can decide to limit the number of physical frames available 
    for certain processes.
    Frame Allocation can be:
    \begin{itemize}
        \item Equal: an equal share for all processes
        \item Proportional: a share based on the program size or other factors
    \end{itemize}

    Frame Replacement can be:
    \begin{itemize}
        \item Local: each process is given a limit of pages it can use (pages against itself, i.e. evicts its own pages):
            \begin{itemize}
                \item[+] doesn't affect other processes 
                \item[-] poor overall utilization of frames, and long access times   
            \end{itemize}
        \item Global: the "victim" is chosen from all available frames regardless of owner
            \begin{itemize}
                \item[-] risk of global \textbf{thrashing}
                \item[+] processes' page frame allocation can vary dynamically  
            \end{itemize}
    \end{itemize}
}

\nDefinition{Working set}{

    let t be the time, k the working set \textbf{window} (measured in page refs) then
    
    WS(k,t) = \textbf{Working set} in time interval t,t-k.
    \\ 

    Then a page is in the WS, iff it was referenced in the last k references by the process.

    \begin{center}
        \nImg[0.6]{OS-61}
    \end{center}

    The size of the working set changes with program locality - when program has poor locality, more pages are referenced and the working set grows

    The working set must be wholly in memory, otherwise heavy faulting - \textbf{thrashing} 
    }

\nDefinition{Page-Fault Frequency Allocation}{
    We can make use of the working set to come up with a great allocation algorithm.

    Notice: there is a relationship between the size of the working set and the page-fault rate of a process.

    Use a local replacement algorithm (i.e. process pages against itself, with limited number of pages)

    The higher the page fault frequency (PFF), the more the process is struggling to keep its working set in memory.
    
    set an acceptable PFF range (lower and upper bound), if actual frequency higher than upper bound, increase the number of frames allocated to the process,
    if frequency lower than lower bound, remove one of the process' frames.

    \begin{center}
        \nImg[0.6]{OS-62}
    \end{center}
}

\nDefinition{Thrashing}{
    When system spends most of its time servicing page faults, little time spent doing useful work.

    Reasons:
    \begin{itemize}
        \item Could be that there is enough memory but replacement algorithm is not working well - it's incompatibile with the program behaviour
        \item Could be that memory is over-committed - OS sees CPU poorly utilized and adds more processes
        
        \nImg[0.4]{OS-63}
    \end{itemize}
}

\nDefinition{Buddy system}{
    A contiguous power of 2 memory allocator for kernel structures. Satisfies request in units sized in powers of 2.
    all requests are rounded up to nearest power of 2.

    \begin{center}
        \nImg{OS-64}
    \end{center}
}

\nDefinition{Slab allocation}{
    Contiguous memory allocator for kernel structures.
    Slabs are made up of one or more physically contiguous pages.
    A cache consists of one or more slabs. There is a cache for each \textbf{unique kernel data structure}.
    Each cache populated with objects If there are any \textbf{free slabs}, the allocation is immediate (no search for memory space)

    \begin{center}
        \nImg{OS-65}
    \end{center}
    }

\nSection{Secondary Storage}


\nDefinition{Magnetic Disk}{
    Traditional cheap HDD storage. 
    Addressed either geometrically (Cylinder, head, sector)
    or more recently using \textbf{Logical block addresses (LBA)}

    \begin{center}
        \nImgs[0.45]{OS-66}{OS-67}
    \end{center}

    Performance depends on:
    \begin{itemize}
        \item \textbf{Seek time} - moving the disk arm to the correct cylinder. Depends on arm speed, does not diminish quickly due to physics
        \item \textbf{Rotation (latency)} - waiting for the sector to rotate under head. Depends on RPM of disk, rates are slowly increasing 
        \item \textbf{Transfer time} - transferring data from surface to disk controller. Depends on density of bytes on disk, increasing relatively quickly
    \end{itemize}

    OS tries to minimise disk usage, mostly the seeking and rotation time.

    OS can inflate file block size , or co-locate "related items" to reduce seeking (blocks of the same file, data and metadata for a file)
    
    OS may keep data or metadata in memory to reduce physical disk access.
    
    OS may fetch blocks into memory before requested (to hide slow disk accesses)
}

\nDefinition{Block scheduling}{
    Applications request data accesses to the OS, OS maintains \textbf{request queues} and generates 
    \textbf{transfer commands} to/from the disk(s) (seeks,waits for rotations, data transfers, all the low level commands)

    App waiting time can be reduced by modifying the order of blocks requested:
    This of course brings up a number of factors that need to be considered as with any scheduling:
    \begin{itemize}
        \item fairness
        \item efficiency
        \item speed
    \end{itemize}

    Many block scheduling algorithms exist.
    }

\nDefinition{First come first served (FCFS)}{
    Simply keep a queue.

    \begin{center}
        \nImg[0.7]{OS-68}
    \end{center}

    \begin{itemize}
        \item[+] reasonable when load is low 
        \item[-] long waiting time for long request queues   
    \end{itemize}
}

\nDefinition{Shortest seek time first (SSTF)}{
    Order requests according to seek time, and serve them in that order

    \begin{center}
        \nImg[0.7]{OS-69}
    \end{center}

    \begin{itemize}
        \item[+] minimizes seek time, and therefore maximizes request rate
        \item[-] unfairly favors middle (clustered) blocks  
    \end{itemize}
    
}

\nDefinition{SCAN}{
    Disk arm starts at one end of the disk, then moves toward the other end, at which the direction is reversed and this repeated

    \begin{center}
        \nImg[0.7]{OS-70}
    \end{center}

    \begin{itemize}
        \item[+] simple, easy to understand 
        \item[+] no starvation
        \item[+] better than FCFS
        \item[+] good under heavy load
        \item[-] not fair - long waiting time for cylinders just visited by the head   
        \item[-] bad when all requests are uniformly spaced out ()
        \item[-] skews wait times non-uniformly
    \end{itemize}
}

\nDefinition{C-SCAN}{
    Same as SCAN, but once other side is reached, it returns to start position and repeats.

    \begin{center}
        \nImg[0.7]{OS-71}
    \end{center}

    \begin{itemize}
        \item[+] more uniform wait times than SCAN
        \item[+] good under heavy load  
    \end{itemize}
}

\nDefinition{Selecting an algorithm}{
    All algorithms behave like FCFS with one request.

    Performance usually depends on number and type of requests

    Requests for disk service can be influenced by file-allocation methods and the metadata layout.  
}

\nDefinition{Solid state drives (SSD)}{
    A version of storage with no moving parts. More expensive, more robust.
    \begin{center}
        \nImg{OS-72}
    \end{center}

    Read access time is mostly independent of the device geometry - block scheduler is not needed.

    Unit of read/write is a page (typically 4kB).

    Usually lower write vs read speed.
    Higher write lag than read lag.
    write usually lower throughput than read

    Flash media must be \textbf{erased before you can write to them}

    Unit of erase is typically 64-256 pages.
    Limited number of times before unusable (10,000 - 1,000,000 times)


    \begin{itemize}
        \item[-] higher cost per GB 
        \item[+] more energy efficient than HDD 
        \item[+] much more physicaly robust    
    \end{itemize}
}

\nDefinition{Storage device management}{
    Storing data is not enough - need metadata 

    Before storing data, device needs to be initialized 

    \begin{center}
        \nImg[1]{OS-73}
    \end{center}
}
\nSection{File System}

\nDefinition{File System}{
    \textbf{Abstracts} away secondary storage. \textbf{Files} are unit of abstraction, organized into \textbf{directories}.

    Enables \textbf{sharing} of data between:

    \begin{itemize}
        \item Processes
        \item People
        \item Machines
        \item etc.
    \end{itemize}

    Provides additional:
    \begin{itemize}
        \item Access control 
        \item Consistency
        \item Reliability
        \item etc.
    \end{itemize}
}


\nDefinition{File}{
    A \textbf{named collection of related information} with some properties:
    
    \begin{itemize}
        \item Content 
        \item Size
        \item Owner
        \item Protection
        \item Last read/write time
    \end{itemize}

    File \textbf{types} are understood by \textbf{file system}:
    \begin{itemize}
        \item Directories, symbolic links, \textbf{devices}
        \item Programs, Data
    \end{itemize}

    Types need to be encoded into file name on windows, not on linux (.com,.exe etc..)
    
}

\nDefinition{File access methods}{
    File systems provide different \textbf{access methods}

    \begin{itemize}
        \item \textbf{Sequential} - read/write bytes one at a time, in order
        \item \textbf{Direct} - random access given by a byte \# 
        \item \textbf{Record} - file is an array of fixed- or variable-sized records
        \item \textbf{Indexed} - One file contains an index to a record in another file
    \end{itemize}
}

\nDefinition{Directory structures}{
    \nDefinition{Single-level}{
        Files must have unique names

        \centering 
        \nImg[0.6]{OS-74}
    }

    \nDefinition{Two-level}{
        Sharing requires path abstraction 

        \centering 
        \nImg[0.6]{OS-75}
    }

    \nDefinition{Tree structured}{
        Eventual replication of files 

        \centering 
        \nImg[0.6]{OS-76}
    }

    \nDefinition{Acyclic graph}{
        Links as a solution to avoid duplication 

        \centering 
        \nImg[0.6]{OS-77}
    }
}

\nDefinition{Directories}{
    Directories provide way for users to \textbf{organize their files} under conenient \textbf{file name spaces} for user and FS 

    Most file systems support \textbf{multi-level directories} (/,/usr,/usr/local,/usr/local/bin)

    Most file systems also support notion of \textbf{current directory}
    \\ 

    \textbf{Absolute names} - fully-qualified starting from root of FS 
    
    \textbf{Relative names} - specified with respect to current directory


    \nDefinition{Internals}{
        Usually just a \textbf{file} with special metadata.

        Organized as a \textbf{symbol table} - list/hash table of name \& file references

        Contains \textbf{attributes} such as:
        \begin{itemize}
            \item Size
            \item Protection
            \item Location on disk
            \item Creation time
            \item Access time
        \end{itemize}

        Usually unordered ('ls' sorts the results)
    }
}

\nDefinition{File protection}{
    File system implements protection system:
    \begin{itemize}
        \item Control \textbf{who}(user/group) can access \textbf{what} (file/directory)
        \item Control \textbf{how} the file can be accessed (read/write/exec)
    \end{itemize}

    \textbf{Objects} - the \textbf{what}

    \textbf{Principals} - the \textbf{who}
    
    \textbf{Actions} - the \textbf{how}
    
    Protection system dictates whether a given action performed by a given principal on a given object should be allowed.

}


\nDefinition{Protection models}{
    Two different models:
    \begin{itemize}
        \item Access Control Lists (ACLs) - for each \textbf{object} keep list of \textbf{principal's} allowed actions
        \item Capabalities - for each \textbf{principal}, keep list of \textbf{objects} and \textbf{principal's} allowed actions
    \end{itemize}

    \begin{center}
        \nImg[0.7]{OS-78}
    \end{center}       
    
    \textbf{Principals} usually summarised as groups:
    \begin{itemize}
        \item Owner
        \item Group (customized lists of users)
        \item Other
    \end{itemize}

}


\nDefinition{Data Structures}{
    Some metadata is needed for a file system:

    \nDefinition{In backing storage}{
        \begin{itemize}
            \item Boot control block - information needed to boot an OS
            \item A per file File Control Block (FCB) - details about each file, unique ID 
            \item Volume control block - volume details, number of blocks, size of blocks, free-block count and pointers, free-FCB count and pointers
            \item Directory structure - used to organise files
        \end{itemize}
    }

    \nDefinition{In memory}{
        \begin{itemize}
            \item Mount table - information about each currently mounted volume
            \item Directory-structure cache - information about most recently accessed directories (for speeding up queries)
            \item The system-wide open-file table - contains copies of FCB's for each open file with other information
            \item The per-process open-file table - contains pointers to the appropriate entries in the system-wide open-file table for all files that the process has opened
            \item Buffers - hold file-system blocks when they are being read/written to a FS 
        \end{itemize}
    }

}

\nDefinition{Disk allocation strategies}{
    \begin{center}
        \nImgs[0.20]{OS-79}{OS-80}
        \nImgs[0.20]{OS-81}{OS-82}
    \end{center}

    \nDefinition{Contiguous Allocation}{
        Each file is required to occupy a contiguous part of the disk. A file entry contains 
        \textbf{Address of starting block} and \textbf{length of file in blocks}

        \begin{itemize}
            \item[-] External fragmentation, holes that cannot be occupied due to size
            \item[+] reading file is quick 
            \item[+] most resilient to file damage (know when there is memory errors) 
        \end{itemize}
    }

    \nDefinition{Linked Allocation}{
        Each file entry contains pointer to the next block, each directory entry contains the first and last block of a file.

        \begin{itemize}
            \item[+] no external fragmentation 
            \item[+] files can grow as long as there is space
            \item[+] only really good for sequential access  
            \item[-] no random access, need to traverse all of previous blocks - O(N)
        \end{itemize}
    }
        
    \nDefinition{Indexed Allocation}{

        Each file has a corresponding index block, which contains the indices of all the blocks which make up the file in order.
        Directory entries tie files to their index blocks.

        \begin{itemize}
            \item[-] wastes a lot of space if files are made up of few blocks (internal fragmentation) 
            \item[+] no external fragmentation, files can grow as long as there is space
            \item[+] fast random access unlike linked allocation 
        \end{itemize}
    }

    \nDefinition{File allocation table (FAT)}{
        FAT table directly correlates entries to sectors, with each entry pointing to next sector of owning file.
        directories contain sector numbers of start of each file.
        
        \begin{itemize}
            \item[+] more resistant to damage than linked allocation, can store multiple FAT tables easilly 
            \item[+] allows random access, but not too fast still need to traverse FAT table (but this is more localized and smaller) 
            \item[+] only FAT needs to be traversed for each file operation (not various parts of disk)  
            \item[-] FAT might be spacious and depends on number of entries
            \item[-] internal fragmentation depending on block size 
        \end{itemize}
    }
                        
}


\nDefinition{INodes - Indexed allocation}{
    Every file and directory is represented by an inode with a unique id (\textbf{index node})
    
    every inode contains:
    \begin{itemize}
        \item Metadata - file owner, access rights etc. 
        \item Location - of file blocks on disk
    \end{itemize}

    \begin{center}
        \nImg{OS-83}
    \end{center}

    \nDefinition{Directories}{
        A directory is a flat file of fixed-size entries.
        Each entry links filename to inode ID

        \begin{center}
            \nImg{OS-84}
        \end{center}
    
        To resolve a pathname, e.g. "/etc/passwd", start at root and walk down chain of nodes:

        \begin{center}
            \nImg[0.8]{OS-85}
        \end{center}
        
        Multiple directories can contain the same file! (hard link in unix)
        }
}

\nDefinition{Inode file systems - data layout}{
    The data on the disk contains different areas:

    \begin{itemize}
        \item Superblock - specifies boundaries of other areas, contains head of freelists of inodes and file blocks 
        \item Inode area - contains inodes for each file on the disk (each the same size)
        \item File contents area - fixed-size blocks referenced by inodes
    \end{itemize}

    \begin{center}
        \nImg[0.8]{OS-86}
    \end{center}

    To find any inode, simply use their ID as index to the inode area! Can do this since they are fixed size.

    \begin{itemize}
        \item[-] filesystem has fixed number of potential inodes - set when FS is created 
    \end{itemize}
}


\nDefinition{Inode file system - block list}{
    Each inode contains metadata and the \textbf{block list}.
    
    To be able to store very small and very large files, the block pointers contain a number of direct pointer slots
    (i.e. ones that point directly to data blocks) and a number of single, double and triple indirect pointers which point to blocks containing more pointers!

    \begin{center}
        \nImg{OS-87}
    \end{center}

    \nDefinition{Example}{ters and if each inode contains 10 direct pointers, one single, double and triple indirect pointers (13 in total), then:
        With 512B data block sizes,32b(4B) poin

        \begin{itemize}
            \item Can access files as large as $10 \cdot 512B = 5120B$ with direct pointers 
            \item The single indirect pointer, can link to a block containing $512B/4B = 128$ direct pointers, each pointing to a block: this lets us access $128 \cdot 512B = 64kB$
            \item The double pointer points to 128 single pointers which point to 128 blocks of data each, this lets us access $128 * 128 * 512B = 8MB$
            \item Similarly, we can access $128^{3} * 512B = 1GB$ with a triple indirect pointer
            \item In total: $5120B + 64kB + 8MB + 1GB \approx 1GB$ 
        \end{itemize}
    }

}

\nDefinition{Inode file system - overview}{
    The file system is simply just a huge data structure:

    \begin{center}
        \nImg[0.9]{OS-88}
    \end{center}

    An important part of a file system is to lay all this data out on a disk efficiently, also keeping in mind the physical constraints of the disk,
    i.e. trying to keep locality within a directory, and allowing sequential access to many files for when seek times are expensive.

}
\nSection{Synchronization}

\nDefinition{Synchronisation}{
    Cooperating tasks access the same data:
    \begin{itemize}
        \item Two or more threads of the same process share \textbf{data} and code
        \item Two or more processes can share data with \textbf{shared memory}
    \end{itemize}

    \textbf{Concurrent or parallel} access to shared data is either virtually done at the same time or actually at the same time for parallel access. This may result in \textbf{data inconsistency} (two processes may overwrite some data in a way which makes it invalid).

    How to ensure \textbf{ordered} execution of cooperating tasks?

    }

\nDefinition{Ordered execution}{
    Instructions executed by a single thread/process are \textbf{totally ordered}: $A < B < C$. i.e. A happens before B and C etc.. 
    \\ 

    Without synchronization, instructions executed by a distinct thread/process must be considered \textbf{unordered/simultaneous}: cannot guarantee any of: $X < X'$ or $X' < X$ or C == A' or C == B'
    \\
    Creation relations always hold: $A < B < A' < B'$
    
    \begin{center}
        \nImg[0.8]{OS-89}
    \end{center}
}

\nDefinition{Race condition}{
    When the result of concurrent or parallel execution is non-deterministic and depends on which thread runs precisely when.
}

\nDefinition{Example Race condition}{
    Race conditions happen when two \textbf{interleaved} sections of the same code, execute in a harmful way.

    The below code shows a case where the bank account does not register one of the transactions!
    \begin{center}
        \nImg[0.8]{OS-90}
    \end{center}
}

\nDefinition{Example Race condition \# 2}{
    Race conditions can also happen between \textbf{seemingly} atomic instructions, in reality $count++$ would be executed as multiple 
    assembly instructions: 
    \begin{center}
        \nImg[0.6]{OS-91}
        \nImg[0.6]{OS-92}
    \end{center}
}



\nDefinition{Critical region}{
    Critical regions are the areas of code which write/read to shared state, and can cause non-determinism:

    \begin{center}
        \nImg[0.8]{OS-93}
        \nImg[0.8]{OS-94}
    \end{center}


}
\nDefinition{Avoiding race conditions}{
    Many factors need to ideally be taken under consideration:
    \begin{itemize}
        \item \textbf{Mutual exclusion} - at most one thread/process is in one critical region 
        \item \textbf{Progress} - No process running outside its critical region may block other processes 
        \item \textbf{Bounded waiting} - No process should have to wait forever to enter its critical region 
        \item \textbf{Performance} - Overhead of entering and exiting critical section should be small wrt to work done inside it
    \end{itemize}
}
\nDefinition{Locks}{
    Locks are ways of ensuring mutual exclusion in critical parts of the code.

    When one thread holds a lock, other threads wait untill this lock is released.

    \nDefinition{Example}{
        \begin{center}
            \nImg[0.9]{OS-95}
        \end{center}
    }

    \nDefinition{Bad implementation}{
        Still has critical region itself!
        \begin{center}
            \nImg[0.9]{OS-96}
        \end{center}
    }
    \nDefinition{Good implemetation}{
        need \textbf{atomic} instruction ideally:

        \begin{center}
            \nImg[0.7]{OS-97}
        \end{center}
    }

    \begin{itemize}
        \item[+] simple easy to use and do the job effectively 
        \item[-] burn a lot of CPU cycles for no reason 
    \end{itemize}
}

\nSection{Semaphores and Monitors}

\nDefinition{sleep() and wakeup()}{
    sleep() - caller guves up CPU for some time, untill a thread/process wakes it up 
    \\ 

    wakeup() - caller wakes up some sleeping thread/process 

    in practice sleep() might just be calling yield() (giving up CPU scheduling priority), signals might be used for waking up etc..
}

\nDefinition{Semaphore}{
    A sleep/wakeup version of a lock, which prevents the burning of CPU cycles!

    \nDefinition{Implementation}{
        \begin{center}
            \nImg[0.55]{OS-98}
        \end{center}

        \textit{note: the tread objects might be TCB's as either user or kernel level constructs. \color{red}{Each function must be executed atomically! (spinlocks)}}
    }


    \begin{itemize}
        \item threads/processes are blocked at the level of program logic
        \item[+] no busy-waiting (burning cpu)
        \item[+] scheduler is aware about thread/process waiting, other tasks can execute  
        \item[-] still use spinlocks so some cpu cycles are still burned
        \item[-] easy to make mistakes, more complex problems can be solved but require more complex control
        \item[-] no guarantee of proper usage, signal() call is not enforced
    \end{itemize}

    \nDefinition{Binary semaphore}{
        Value initialzied to 1. Protects single resource 
    }

    \nDefinition{Counting semaphore}{
        Value initialized to \# of protected resources. Positive means resource available, negative or zero = number of waiting for resources or no resources available
    }

    \nDefinition{Synchronisation semaphore}{
        Value initialized to 0, 0 means no events pending, \textbf{positive} value means there are events pending (waiting in sleep).
        
        \textit{If you have a routine S1 which needs to happen before S2, you call \textbf{signal} after S1, and \textbf{wait} before S2.
            This means that any process wanting to execute S2, will have to wait (value = -1) untill S1 is executed by some process which reduces value to 0. 
        }
    }
}

\nDefinition{Semaphore example}{
    \begin{center}
        \nImg[0.7]{OS-99}
    \end{center}
    
    \textit{note: semaphores here are used to limit burning of CPU when buffer is full or empty, but we still need another semaphore on the actual buffer operations}
}

\nDefinition{Semaphore example - reading}{
    Sometimes we want to let multiple reading threads in, but only one writer:
    \begin{center}
        \nImg[0.9]{OS-100}
    \end{center}

    \textit{note: how the first reader blocks the write semaphore, and the last releases it, while every writer does the same. We still need to lock the manipulation of the shared variables}
}

\nDefinition{Semaphore example - forever blocked}{
    \begin{center}
        \nImg[0.9]{OS-101}
    \end{center}

    \textit{note: order of locks is important! bug prone}
}

\nDefinition{Monitor}{
    Address key usability issues with semaphores, a \textbf{programming language construct}.
    An \textbf{abstract data type/class} in which every method automatically acquires a lock on entry and releases it on exit. 

    includes: 
    \begin{itemize}
        \item \textbf{shared data} structures 
        \item \textbf{procedures} that operate on the shared data 
        \item \textbf{synchronisation} between concurrent execution flows that invoke those procedures
    \end{itemize}

    data is encapsulated from within:
    \begin{itemize}
        \item protects the data from unstructured access
        \item prevents ambiguity about synchronisation
    \end{itemize}

    \textbf{Automatic} mutual exclusion
    \begin{itemize}
        \item Only \textbf{one thread} can be executing inside at any time 
        \item Synchronization is implicitly associated with the monitor (free)
        \item If a second thread tries to execute a monitor procedure it \textbf{blocks} untill the first has left the monitor 
        \item more restrictive than semaphores but easier to use 
    \end{itemize}

    \begin{center}
        \nImg[0.8]{OS-102}
    \end{center}
}

\nDefinition{Deadlock problems}{
    Not as flexible, what to do in deadlocks ?
    \begin{center}
        \nImg{OS-103}
    \end{center}

    \nDefinition{Condition variables}{
        \begin{itemize}
            \item \textbf{cond\_wait(c)}
                \begin{itemize}
                    \item Release monitor lock, so somebody else can get in 
                    \item Wait for someone else to call \textbf{cond\_signal(c)}
                    \item Condition variables have associated wait queues
                \end{itemize}
            \item \textbf{cond\_signal(c)}
                \begin{itemize}
                    \item Wake up at most one waiting thread 
                    \item Wake up immediately, signaller steps outside 
                    \item If no waiting threads, signal is lost (difference from semaphores, no history)
                \end{itemize}
            \item  \textbf{cond\_broadcast(c)}
                \begin{itemize}
                    \item Wake up all waiting threads (for c)
                \end{itemize}
        \end{itemize}
    }

}

\nDefinition{Monitor example}{
    The compiler can be made to insert enter and exit monitor calls wheerever the programer uses wait and signal calls:
    \begin{center}
        \nImg[0.8]{OS-104}
    \end{center}
}

\nSection{Deadlocks}

\nDefinition{Deadlock}{   
    A situation in which every process/thread in a set of processes/threads is \textbf{waiting for an event} that can be \textbf{caused only by another process/thread in the same set}
    \begin{center}
        \nImg[0.7]{OS-105}
    \end{center}
    }

\nDefinition{Livelock}{   
    Similar to deadlock but occurs when a thread/process \textbf{continuously attempts} an action that fails
    
    \begin{center}
        \nImg[0.7]{OS-106}
    \end{center}
    }

    \nDefinition{Necessary conditions for deadlock}{
        
        {\color{red}{without any of the below conditions, a deadlock is not possible}}

        \begin{itemize}
            \item[A)] \textbf{Mutual exclusion} - each resource is either currently assigned to exactly \textbf{one} process/thread or is available 
            \item[B)] \textbf{Hold and wait} - process/threads currently holding resources that were granted earlier can request new resources 
            \item[C)] \textbf{No preemption} - Resources previously granted cannot be forcibly taken away from a process/thread   
            \item[D)] \textbf{Circular wait} - there must be a circular list of two or more processes/threads each waiting for a resource held by the next member of the chain
        \end{itemize}

        
    }

\nDefinition{System model}{
    A system consists of a \textbf{finite number of resources}, paritioned into several types/classes, to be 
    \textbf{distributed among a number of competing threads/processes}.
    
    \begin{itemize}
        \item Resource types/classes (\textbf{m}) - \textbf{$R_{1},R_{2},R_{m}$}
        \item Each resource type has \textbf{$E_{i}$} instances
        \item assume serially reusable resources (request -> use -> release)
        \item Processes/threads (\textbf{n}) -  \textbf{$P_{1},P_{2},P_{m}$}
    \end{itemize}

    Let \textbf{(V,E)} be the directed graph representing the \textbf{current state} (snapshot) of resource usage.
    
    Then $$V = \{P | \textit{P is a process}\} \cup \{R | R \textit{is a resource type}\}$$
    And $$E = \{E | \textit{E is a directed request edge } (P_{i},R_{i})\} \cup \{E | \textit{E is an assigment edge } (R_{i},P_{i})\}$$
    
    \begin{center}
        \nImg[0.7]{OS-107}
    \end{center}
}


\nDefinition{Allocation graph - no deadlock}{
    \begin{center}
        \nImg[0.55]{OS-108}
    \end{center}

    \textit{note: \color{red}  No cycles $\longrightarrow$ thread/process is not deadlocked}
}
\nDefinition{Allocation graph - deadlock}{
    \begin{center}
        \nImg[0.55]{OS-109}
    \end{center}

    \textit{note: \color{red}  cycles $\longrightarrow$ thread/process \textbf{may} be deadlocked}
}
\nDefinition{Allocation graph - no deadlock cycle}{
    \begin{center}
        \nImg[0.55]{OS-110}
    \end{center}
    
    
    Only if each resource involved in a cycle has \textbf{exactly one instance}, then the processes are deadlocked iff they are a part of such a cycle.


    \textit{note: cycle $\neq$ deadlock}
}
\nDefinition{Handling deadlocks}{
    \begin{itemize}
        \item \textbf{Prevention} - Negate one of the four necessary conditions
        \item \textbf{Avoidance} - Each resource request is analyzed and denied if it may deadlock
        \item \textbf{Detection and recovery} - detect deadlock after it happens, recover from it 
        \item \textbf{Do nothing}(ye olde ostrich algorithm) - ignore the problem and pretend deadlocks never occur since they're so rare (actually done in some OS)
    \end{itemize}
}

\nDefinition{Attacking - Mutual exclusion}{
    \begin{itemize}
        \item Avoid assigning a resource unless absolutely unnecessary 
        \item Try to make sure that as few processes as possible may actually claim the resource
        \item Practically - don't assign resources to a single processes for reads, and for writes use a mediator which serializes the write (spooler deamon in case of spooler)
        \item \textbf{Problem} - process may deadlock filling up the mediator
    \end{itemize}
}

\nDefinition{Attacking - Hold and wait}{
    \begin{itemize}
        \item Prevent processes that hold resources from waiting for more resources 
        \item Require all processes to request all their resources before "executing"
        \item If everything is available, the process will be allocated whatever it needs and can run to completion
        \item If one or more resources are busy, nothing will be allocated and the process will just wait - then reallocate everything again
        \item \textbf{Problem} - processes don't always know all the resources they require
    \end{itemize}
}

\nDefinition{Attacking - No preemption}{
    \begin{itemize}
        \item If a process is holding some resources and requests another resource which is not available, then all resources the process is currently holding are preempted (released)
        \item Practically, can only be applied to resources whose state can be easilly saved and restored (CPU registers, memory space, etc..)
        \item \textbf{Problem} - Not all resources can be virtualized
    \end{itemize}
}

\nDefinition{Attacking - Circular wait}{
    \begin{itemize}
        \item All requests of resources must be made in numerical order by process
        \item \textbf{May be impossible to find an ordering to satisfy everyone}
    \end{itemize}

    \begin{center}
        \nImg[1]{OS-111}
    \end{center}
}

\nDefinition{Deadlock avoidance}{
    The OS/runtime needs to be given \textbf{additional information} in advance:
    \begin{itemize}
        \item What resources a thread/process will \textbf{request and use} during lifetime
        \item Complete \textbf{sequence of requests and releases} of each thread/process
        \item \textbf{Decide for each request} if a thread/process waits, considering the \textbf{resource-allocation state}
        \item Algorithm dynamically examines resource-allocation state for \textbf{circular-wait condition} existence
    \end{itemize}
}

\nDefinition{Safe state}{
    When the system \textbf{can allocate all leftover resources} to each thread/process (up to maximum) \textbf{in some order} and still avoid \textbf{deadlocks}

    A system is in a safe state iff there exists a \textbf{safe sequence}
    \begin{itemize}
        \item A sequence ($p_{1},p_{2},p_{3}..p_{n}$) of \textbf{ all the processes in the systems}
        \item such that for each p, the resources that p can still request, can be satisfied by:
            \begin{itemize}
                \item Currently \textbf{available resources} +
                \item \textbf{Resources held} by all the p's before it in the sequence
            \end{itemize}
        \item In this situation if $p_{i}$ needs some resource which is not immediately available:
            \begin{itemize}
                \item $p_{i}$ can wait untill all $p_{j}$ s.t. $j < i$ are done
                \item Then $p_{i}$ can obtain the needed resources, execute and return allocated resources then terminate 
                \item when $p_{i}$ terminates $p_{i+1}$ can obtain its needed resources and so on
            \end{itemize}
    \end{itemize}

    Because of all the other neccessary conditions:
    
    \textbf{safe state = non deadlocked state}
    
    \textbf{deadlocked state $\longrightarrow$ unsafe state}
    
    \textbf{unsafe state $\cancel{\longrightarrow}$ deadlock}

    \begin{center}
        \nImg[0.34]{OS-112}
    \end{center}

    \begin{itemize}
        \item In a \textbf{safe state} - OS can avoid  \textbf{unsafe states}
        \item In an \textbf{unsafe satet} - OS cannot prevent resource allocations resulting in deadlocks
    \end{itemize}
    }

\nDefinition{Banker's algorithm}{
    Algorithm used to avoid deadlocks.

    Requires:
    \begin{itemize}
        \item Set of \textbf{controlled resources}
        \item \textbf{Number of units} of each resource is known 
        \item Each application declares \textbf{maximum possible requirement} of each resource type
    \end{itemize}

    Before approving any resource allocation request, check if it leads to a safe state, if not reject it.

}

\nDefinition{Deadlock detection}{
    Maintain a different directed graph - the \textbf{wait-for} graph, where processes are nodes, and directed edges (P1,P2) imply P1 is waiting for P2.

    Periodically search for cycles in this graph, because \textbf{cycle $\longrightarrow$ deadlock!}

    Search is $O(n^{2})$.

    
    \begin{center}
        \nImg[0.4]{OS-113}
    \end{center}

    \textit{note:\color{red} does not work with multiple instances of each resource type}

    }

\nDefinition{Recovery from deadlock}{
    \begin{itemize}
        \item Abort \textbf{all} deadlocked processes or just one at a time untill deadlock is eliminated
        \item this may leave resources in an incorrect state
        \item Alternatively, preempt resources:
            \begin{itemize}
                \item Select victim - resource and process to preempt 
                \item Rollback - return resource to safe state, restart process for that state 
                \item Starvation - avoid same process/thread as victim
            \end{itemize} 
    \end{itemize}
}
\nSection{Virtualization}

\nDefinition{Virtualisation}{
    Process of creating a \textbf{virtual} version of a \textbf{physical} object

    \textbf{Hardware virtualisation} - process of creating \textbf{virtual} version of real hardware
    Virtual hardware can be used to run a complete OS 
}

\nDefinition{Virtual machine}{
    A virtual representation of a physical machine
}

\nDefinition{Virtual machine monitor/Hypervisor}{
    A software application that monitors and manages running virtual machines
}

\nDefinition{Host machine}{
    The physical machine that a virtual machine is running on
}

\nDefinition{Guest machine}{
    The virtual machine running on the host machine
}

\nDefinition{Virtualisation diagram}{
    \begin{center}
        \nImg[1]{OS-114}
    \end{center}
}

\nDefinition{VMM/Hypervisor types}{
    \begin{itemize}
        \item \textbf{Native hypervisors (bare metal)} - run directly on the host machine, and share out resources (memory and devices) between guest machines (Oracle VM server)
        \item \textbf{Hosted hypervisors} - run as an application inside an operating system, and support virtual machines running as individual processes (QUEMU, VBox)
    \end{itemize}

    \begin{center}
        \nImg[1]{OS-115}
    \end{center}
}

\nDefinition{Uses of virtualisation}{
    \begin{itemize}
        \item \textbf{Personal} - running multiple OS on one host, without rebooting. Running windows inside OX X 
        \item \textbf{Technical} - OS/hardware design, kernel debugging/testing
        \item \textbf{Commercial} - Data centre server consolidation, high availability/migration
    \end{itemize}
}

\nDefinition{Types of virtualisation}{
    \begin{itemize}
        \item \textbf{Software emulation} - maximum flexibility, but very slow
        \item \textbf{Containers/namespaces} - isolate processes/ groups of processes within a single OS (Docker)
        \item \textbf{Full system/Hardware virtualisation} - isolate multple OS from each other, within a single physical machine
        \item \textbf{Same-architecture virtualisation} - guest machine is the same architecture as host machine (ARM or x86)
        \item \textbf{Cross-architecture virtualisation} - Guest machine has different architecture (requires software emulation)
    \end{itemize}
}

\nDefinition{Popek and Goldberg requirements for virtualisation}{
    \begin{itemize}
        \item \textbf{Efficiency} - the majority of guest instructions are executed directly on the host machine
            \begin{itemize}
                \item Normal guest machine instructions should be execute directly on the processor.
                \item System (kernel) instructions need to be emulated by the VMM
            \end{itemize}
        \item \textbf{Resource control} - VMM must remain in control of all machine resources
            \begin{itemize}
                \item VM cannot be able to arbitrary take control of any resources (security)
            \end{itemize}
        \item \textbf{Equivalence} - the VM must behave in a way that is indistinguishable from if it was running as a physical machine
            \begin{itemize}
                \item OS running inside VM, needs to believe it's running on physical machine (it shouldn't be able to detect it's on a VM)
                \item exceptions include: \textbf{temporal latency} (some instruction sequences will take longer to run), and \textbf{resource availability} (less resources available due to sharing between VM's)
            \end{itemize}
    \end{itemize}
}

\nDefinition{Methods of virtualisation}{
    \begin{itemize}
        \item \textbf{Full software emulation} 
                \begin{itemize}
                    \item violates efifciency property, but this no longer holds due to advent of \textbf{efficient binary translation}
                    \item Required for cross-architecture virtualisation, as guest instructions cannot execute natively on the host
                \end{itemize}
        \item \textbf{Trap-and-Emulate}
                \begin{itemize}
                    \item The guest OS system runs "de-priviliged", all non-privileged instructions execute natively on the host.
                    \item All privileged instructions trap to the VMM
                    \item VMM emulates these privileged operations
                    \item Guest resumes execution after emulation
                \end{itemize}
    \end{itemize}
}

\nDefinition{Virtualising x86}{
    \begin{itemize}
        \item Originally x86 was not "classically" virtualisable
        \item Some privileged instructions did not "trap", and so could not be emulated correctly 
        \item \textbf{Interpretation} is too slow (violates efficiency)
        \item \textbf{Code patching} (editing code after to re-write privileged instructions) leaves traces of virtualisation (violates equivalency)
        \item \textbf{Binary translation} better but still incurs overhead
        \item since 2005, x86 support virtualisation in hardware (intel-VT,AMD-V)
        \item this enables trap-and-emulate style virtualisation at the hardware level
        \item Unmodified OS can run natively on host machines
    \end{itemize}

    \begin{center}
        \nImg[1]{OS-116}
    \end{center}

}

\nDefinition{Hardware acceleration for Virtualisation}{
    \begin{itemize}
        \item Modern processes include hardware support for running VM's
        \item Hardware extensions allow all guest instructions (including system instructions) to run natively on the processor
        \item This works by providing an isolated view of the processor to virtual machines.
        \item OS can then run directly on the processor, believing they are running on real hardware
        \item Certain privileged instructions "trap" back to the hypervisor
    \end{itemize}
}

\nDefinition{Resource access}{
    \begin{itemize}
        \item VM's need:
            \begin{itemize}
                \item Memory
                \item Storage
                \item Networking
                \item Graphics
            \end{itemize}
        \item VMM shares out these resources 
        \item Access to physical memory is managed by the VMM 
        \item For an unmodified OS, expecting a "real" storage device (such as HDD), the VMM must emulate its interface!
        \item Some devices may be passed straight through to the VM, e.g. dedicated network cards.
    \end{itemize}
}

\nDefinition{Paravirtualisation}{
    \begin{itemize}
        \item Guest operating systems are \textbf{aware} they are being virtualised
        \item They co-operate with the hypervisor to enable increased memory and device performance 
        \item They no longer "trap-and-emulate", but instead request privileged operations from the hypervisor (API)
        \item They can co-operate with the hypervisor so that host memory can be more efficiently distributed
        \item Instead of providing an emulated storage device, the hypervisor can provide a paravirtualised implementation 
        \item Typically used in \textbf{data centres} for large-scale virtualisation 
    \end{itemize}
}

\nSection{Data-center technologies}
fuq clooud



\end{document}
