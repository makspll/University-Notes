\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{cancel}

\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=0.5in]{geometry}
\renewcommand\vec{\mathbf}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{ST Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak

\nChapter{ST}

\nSection{Introduction}

\nDefinition{Validation}{
    Checking that the software specification adheres to the requirements of the customer. Are we building the right product?
}
\nDefinition{Verification}{
    Checking that the software adheres to the software specification. Are we building the product right ?
}

\nDefinition{Software Specification}{
    Defines the product being created, includes:
    \begin{itemize}
        \item \textbf{Functional requirements} - describe features of a product
        \item \textbf{Non-Functional requirements} - constraints on the product (e.g. security, reliability etc.)
    \end{itemize}
}

\nDefinition{Software bug}{
    A software bug occurs when one of the following occurs:
    \begin{itemize}
        \item The software does not do something that the true specification says it should do.
        \item The software does something that the specification says it should not do.
        \item The software does something that the specification does not mention.
        \item The software does not do something that the product specification does not mention but should.
        \item The software is difficult to understand, hard to use, slow, etc..
    \end{itemize}
}

\nDefinition{Phases of Software Testing}{
    \begin{itemize}
        \item Test generation - Writing tests, Automatic test generation etc.
        \item Test Execution - Actually executing tests, build tools etc.
        \item Test Oracle - determining what the output for each test \textbf{should} be
        \item Test Adequacy - determining the effectiveness of the test suite.
    \end{itemize}
}

\nDefinition{Software Fault}{
    A static defect in software 
}

\nDefinition{Software Error}{
    An incorrect internal state that is the manifestation of some \textbf{fault}
}

\nDefinition{Software Failure}{
    External, incorrect behaviour with respect to the requirements, or other description of expected behaviour. (The actual moment the software fails, for some input space)
}

\nSection{Unit Testing}

\nDefinition{Unit Testing}{
    Looking for errors in a subsystem in isolation:

    \begin{itemize}
        \item Generally a class or object
        \item Junit in Java
    \end{itemize}
}

\nDefinition{Junit 4}{
    \begin{center}
        \nImg[1]{ST-1}
    \end{center}
}

\nDefinition{Unit Testing Tips}{
    \begin{itemize}
        \item Tests need to be \textbf{atomic} - the failure of a test should pinpoint the location of the fault 
        \item Each test name should be clear, long and descriptive 
        \item Assertions should always have clear messages to know what failed 
        \item Write many small tests, not one big one ($\approx 1$ assertion per test)
        \item Test for expected errors/exceptions too
        \item Choose descriptive assert method (not always assertTrue, i.e. prefer more specific methods)
        \item Choose representative test cases for equivalent input classes 
        \item Avoid complex logic in test methods if possible 
        \item Use helpers, @Before to reduce redundancy between tests
        \item Never rely on test execution order, or call other test fixtures (apart from helpers)
        \item Dont share state between tests 
    \end{itemize}
}


\nSection{Functional Testing}

\nDefinition{Functional Testing (Black-box testing)}{
    Deriving tets cases from program specifications. Functional refers to the source of information used in test case design, not to what is tested 
    Also known as \textbf{specification-based} testing.

    Functioal specification = description of intended program behaviour. We act as if we didn't know anything about the code (hence black-box)
    
    Example:
    \begin{center}
        \nImg[0.95]{ST-2}
    \end{center}

    \begin{center}
        \nImg[0.8]{ST-3}
    \end{center}


    \begin{itemize}
        \item[+] Often reveals ambiguities and inconsistency in specifications 
        \item[+] Useful for assesing testability
        \item[+] Useful explanation of specification (test cases outline the specification)
        \item[+] does not require any code to write tests (TDD)
        \item[+] Ideal for missing logic (if we only test what's there - white box testing, we won't identify what's missing!)
    \end{itemize}
    }


\nDefinition{Random Testing}{
    Pick possible inputs uniformly. 
    \begin{itemize}
        \item[+] avoids designer bias (test designer can make logical mistakes and bad assumptions)
        \item[-] treats all inputs as equally valuable
        \item[-] Real faults are distributed non-uniformly (think finding roots, 3 important cases, rest is same class)
        \item[-] input space is often extremely large    
    \end{itemize}
}
\nDefinition{Systematic Functional Testing}{
    Try to select inputs that are especially valuable (non-uniform selection)

    Usually by choosing representatives of classes that are apt to fail often or not at all.

    Functional Testing \textbf{IS} systematic testing
    \begin{center}
        \nImg[0.9]{ST-4}
    \end{center}
}

\nDefinition{Partition Testing}{
    Exploit some knowledge to choose input samples more likely to include "special" or trouble-prone regions of input space.
    Failures are sparce in the whole input space but we may find regions in which they are dense

    Ideally want \textbf{Equivalance classes} of inputs - for example, the positive numbers might be equivalent with respect to the program, i.e. 1 and 2 will cause the same general behaviour.

    Then we only really need one value from each partition, since if one condition/value in a partition passes, all others will also pass and vice-versa!

    \begin{center}
        \nImg[0.9]{ST-5}
    \end{center}
    }

\nDefinition{Quasi-Partition testing}{
    Separate the input space into classes, whose union is the entire space, where the classes can overlap
}

\nDefinition{Boundary Value Analysis}{
    Testing of the inputs on the boundaries between equivalence partitions (both \textbf{valid and invalid})
    \begin{center}
        \nImg[0.8]{ST-6}
    \end{center}
    }

\nDefinition{Functional vs Structural (White-box testing)}{
    Functional testing applies at all granuality levels: Unit, Integration, System and Regression testing.

    Structural testing applies to a relatively small part of the system: Unit and Integration testing.
}

\nDefinition{Functional Testing Process}{
    \begin{enumerate}
        \item Decompose the specification into \textbf{Independently testable features} (think functions: Airport connection check etc. )
        \item Select representative values of each input, or behaviours of a model (which part of the input space are interesting ?)
        \item Form test specifications (input/output values, model behaviours etc)
        \item Produce and execute actual tests
    \end{enumerate}
}

\nSection{Combinatorial Testing}

\nDefinition{Combinatorial Testing}{
    Identify distinct attributes that can be varied:
    \begin{itemize}
        \item Environment - characterized by combination of hardware and software (IE, Firefox, XP, OSX, 10GB Ram etc.)
        \item Input Parameters 
        \item State Variables
    \end{itemize}
    \textbf{Equivalence partitioning} and \textbf{boundary value analysis} can be used to identify values of each attribute.

    Systematically generate combinations of values for different attributes to be tested (combinations of attributes)

    It's often impractical to test all of the combinations of distinct attributes, so we can pick certain subsets according to combination strategies.
    }

\nDefinition{Key ideas}{
    \begin{itemize}
        \item Category-partition testing  - identification of values that characterise the input space, done manually, the actual combinations of attributes are generated automatically.
        \item Pairwise testing - systematically test interactions among attributes of the program input space with a relatively small number of test cases 
        \item Catalog-based testing - aggregate and synthesize the experience of test designers in a particular organization or applciation domain, to aid in identifying attribute values (automates the manual parts too) 
    \end{itemize}
}


\nDefinition{Category Partitioning (manual)}{
    \begin{enumerate}
        \item Decompose the specification into ITF's (Independently testable features)
        \item For each feature identify:
            \begin{itemize}
                \item Parameters (i.e. Arriving flight, Departing flight)
                \item Environmental elements (State of airport database) 
            \end{itemize}
        \item For each parameter and environment element identify \textbf{elementary characteristics} or \textbf{categories} (arrival destination matches departure origin, origin and destination airports exist in the base, database is available)
        \item Identify relevant values, For each characteristic (\textbf{category}) identify \textbf{classes of} values, e.g.:
            \begin{itemize}
                \item normal values (1,2,3)
                \item boundary values  (0,-1)
                \item special values (INF)
                \item error values (' ',"","CHEESE")
            \end{itemize}
        
        \item Introduce constraints ("[error]" cases are tested once, [property if-property] run only in combination with if-property, [single] run once)
    \end{enumerate}
}


\nDefinition{Constraints}{

    \begin{itemize}
        \item[-] [error], value class which corresponds to erroneous values, only needs to be tested once, with all the other parameters and environments set to any valid values
        \item[-] [property] [if-property] mark a value class as a property to be referenced by others
        \item[-] [if-property] only include this property in combinations combined with value classes marked with the given property     
        \item[-] [single], same as error, but difference in semantics - rationale
        
    \end{itemize}

    \begin{center}
        \nImg[0.8]{ST-7}
    \end{center}
}

\nDefinition{Pairwise Combinatorial Testing}{
    Generate combinations that efficently cover all pairs of classes. Most failures are triggered by single 
    values or combinations of a few values. Covering pairs, reduces the number of test cases, but reveals most faults.

    \begin{center}
        \nImg[0.9]{ST-9}
    \end{center}

    When designing test cases, we set all irrelevant values (not included in the pair) with valid values.

    In this case we want test cases which cover, all combinations of values between Display Mode and Language, Display Mode and Fonts, Fonts and Color and so on and so on.
    
    This would yield 136 pairs in total for this example ($3 \cdot 4 + 3 \cdot 3 ...$)
    }
 

\nDefinition{Catalog Based Testing}{
    Deriving value classes requires human judgement. Gathering experience in a systematic collection can:
    \begin{itemize}
        \item Speed up the test design process
        \item Routinize many decisions, better focusing human effort 
        \item Accelerate training and reduce human error 
        \item Catalaogs \textbf{capture the experience of test designers} by listing important cases for each possible type of variable
    \end{itemize}

    Process:
    \begin{enumerate}
        \item Analyze the initial specification to identify simple elements:
            \begin{itemize}
                \item Pre-conditions
                \item Post-conditions
                \item Definitions
                \item Variables
                \item Operations
            \end{itemize}
        \item Derive a first set of test case specifications from pre-conditions, post-conditions and definitions 
        \item Complete the set of test case specifications using test catalogs
    \end{enumerate}

    \begin{center}
        \nImg{ST-8}
    \end{center}
    }

\nSection{Finite Models}

    \nDefinition{Properties of models}{
        \begin{itemize}
            \item Compact: representable and manipulable in a reasonably compact form 
            \item Predictive: must represent some salient characteristics of the modelled artifact well enough to distinguish between good and bad outcomes of analysis
            \item Semantically meaningful: it is usually necessary to interpret analysis results in a way that permits diagnosis of the causes of failure
            \item Sufficiently general: models intended for analysis of some important characteristic must be general enough for practical use in the intended domain of application
        \end{itemize}
    }

    \nDefinition{Intraprocedural Control Flow Graph}{
        Nodes represent regions of source code (not necessarily lines), each block of code in a node has a single entry and exit point.
        Edges represent posibility that control flow transitions from end of one block to beginning of another
        
        \begin{center}
            \nImg[0.9]{ST-10}
        \end{center}
        }

    \nDefinition{Call Graph}{
        Similiar to control flow graph, but at higher level, nodes represent functions, and edges the possibility of a call from one call to another.

        \begin{center}
            \nImg[0.9]{ST-11}
        \end{center}

        \textit{Note: the 'calls' relation represented by the edges, is overestimated via call graphs, some calls are simply impossible in real execution due to state}
    }

    \nDefinition{Context Insensitive Call Graphs}{
        Each edge represents any call to a procedure with any context
        \begin{center}
            \nImg[0.9]{ST-12}
        \end{center}
    }

    \nDefinition{Context Sensitive Call Graphs}{
        Each edge represents a unique \textbf{call stack} as well as function call (different calls have different edges)
        \begin{center}
            \nImg[0.9]{ST-13}
        \end{center}
    }


    \nDefinition{Context exponential growth}{
        The call stacks grow exponentially making context sensitive call graphs very large:

        \begin{center}
            \nImg[0.9]{ST-14}
        \end{center}
    }

    \nDefinition{Finite State Machines}{
        Each node is a state, and each edge is a possible transition, can be used to model high-level program behaviour

        \begin{center}
            \nImg[0.6]{ST-15}
        \end{center}
    }

    \nDefinition{Models and System properties}{
        Models might be easier to validate against program specifications, and if the program satisfies some model which satisfies 
        the program specification, then those specifications are met by the program.

        \begin{center}
            \nImg[0.7]{ST-16}
        \end{center}
    }

\nSection{Structural Testing}

    \nDefinition{Structural Testing (White-box testing)}{
        Type of testing based on the structure of the program itself, still testing product functionality against the specification but the throughness measures are changed.

        Ansers the question "what is missing in our test suite?" - If a part of a program is not executed by any test case in the suite, faults in that part cannot be exposed - different types of structural testing define \textbf{part} differently. 
        
        \begin{itemize}
            \item[-] Executing all control flow elements does not guarantee fidning all faults - might depend on state
            \item[+] Increases confidence in thoroughness of testing, removes obvious inadequacies  
            \item[+] Coverage criteria can be automated fully
        \end{itemize}
        }

    \nDefinition{Statement Testing}{
        Adequacy criterion: each statement (or node in CFG) must be executed at least once
        $$\frac{\textit{executed statements}}{\textit{statements}}$$
    

        \begin{center}
            \nImg[1]{ST-17}
        \end{center}
       
        \begin{itemize}
            \item[-] can miss some cases (if branches without an else case for example)
        \end{itemize}
        \textit{Note: no essential differences between statement granuality, 100\% CFG node coverage iff 100\% statement coverage}    
    

    }

    \nDefinition{Branch Testing}{
        Adequacy criterion: each branch (edge in the CFG) must be executed at least once 
        $$\frac{\textit{executed branches}}{\textit{branches}}$$
    
        In above example:
        \begin{center}
            \nImg[0.7]{ST-18}
        \end{center}

        \begin{itemize}
            \item[+] subsumes statement testing
            \item[-] can still miss some states (compound logical expressions such as $A | B$, can lead to both branches with only A being varied)  
        \end{itemize}
        }
    
    \nDefinition{Basic condition testing}{
        Adequacy criterion: each atomic condition must be executed once when it's true and once when it's false. i.e. $A | B$ requires that A,B be False and True in some test (not all combinations though).
    
        $$\frac{\textit{truth values taken by all basic conditions}}{2\cdot \textit{basic conditions}}$$
        
        \begin{itemize}
            \item does not imply branch coverage (not realy comparable)
        \end{itemize}
        }
    
    \nDefinition{Compound condition testing}{
        Adequacy criterion: each compound condition must have all possible combinations of its atomic conditions executed.
        $$\frac{\textit{combinations of compound conditions executed}}{\textit{compound conditions cases}}$$

        \begin{itemize}
            \item[+] subsumes branch testing
            \item[-] $2^{n}$ cases arising from each condition with n atomic conditions  
        \end{itemize}
    }

    \nDefinition{Modified condition/decision (MC/DC)}{
        Adequacy criterion: all \textbf{important} combinations of conditions are tested, i.e. each basic condition shown to independently affect the outcome of each compound condition.

        \begin{itemize}
            \item For each basic condition C, two test cases are required 
            \item In each case, the values of all the other basic conditions must stay the same, and only C must vary
            \item The whole condition must evaluate to true in one case, and false in another 
            \item[+] n+1 test cases for n basic conditions (for minimal test suite)
            \item[+] subsumes statement, basic condition and branch testing
            \item[+] good balance of thoroughness and test size (widely used)
            
        \end{itemize}

        \begin{center}
            \nImg[0.9]{ST-19}
        \end{center}
    }


    \nDefinition{Path adequacy}{
        Adequacy criterion: each path must be executed at least once. I.e. a path is just a walk over the CFG (along the directed edges)
        $$\frac{\textit{executed paths}}{\textit{paths}}$$
        \begin{itemize}
            \item[-] number of paths is infinite if any loops are present in the CFG
            \item[-] usually impossible to satisfy 
            \item[+] very thorough   
        \end{itemize}
        }

    \nDefinition{Boundary interior path testing}{
        Adequacy criterion: Requires the set of paths from the root of the tree to each leaf is the required set of subpaths for boundary/interior coverage.
        $$\frac{\textit{paths}}{\textit{unique paths reaching a leaf}}$$
    
        \begin{center}
            \nImg[0.9]{ST-20}
        \end{center}

        \begin{itemize}
            \item[-] Still number of leaf paths can grow exponentially (parallel if statements N statements = $2^N$ paths that must be traversed)
            \item[-] some paths might not be reachable since some conditions are not independent 
            \item[+] more managable 
        \end{itemize}
        }

    \nDefinition{Loop boundary adequacy}{
        Adequacy criterion: for every loop:
            \begin{itemize}
                \item In at least one test case, the loop body is iterated zero times
                \item In at least one test case, the loop body is iterated once 
                \item In at least one test case, the loop body is iterated more than once
            \end{itemize}
    }

    \nDefinition{LCSAJ Adequacy}{
        Adequacy criterion: A test suite satisfies LCSAJ adequacy, if all the LCSAJ segments are executed in some test.

        $$\frac{\textit{LCSAJ's executed}}{all LCSAJ's}$$
        LCSAJ stands for Linear Code Sequence and Jump, i.e. a node in the CFG followed by a branch.

        Various "thoroughness" levels:
        \begin{itemize}
            \item $TER_{1}$ = statement coverage
            \item $TER_{2}$ = branch coverage
            \item $TER_{n+2}$ = coverage of n consecutive LCSAJs 
        \end{itemize}
    }

    \nDefinition{Cyclomatic adequacy}{
        Adequacy criterion: number of "linearly independent" paths (if you treat each node as an entry in a vector signifying the presence of an edge in the given path) is equal to cyclomatic number of the CFG.

        The cyclomatic number can be calculates as: $e - n + 2$
    }

    \nDefinition{Subsumption relation}{
        \begin{center}
            \nImg[0.9]{ST-21}
        \end{center}
    }
\nSection{Data Flow Models}

    \nDefinition{Def-Use Pairs}{
        Associates a point in a program where a value is produced with a point where it is used. 

        Definition - where a variable gets a value assigned:
        \begin{itemize}
            \item Variable declaration
            \item Variable initialization
            \item Assignment
            \item Values received by a parameter
        \end{itemize}

        Use - extraction of a value from a variable:
        \begin{itemize}
            \item Expression
            \item Conditional statement
            \item Parameter passing 
            \item Return
        \end{itemize}

        \begin{center}
            \nImg[1]{ST-22}
        \end{center}
    }

    \nDefinition{Definition-Clear path}{
        A definition-clear path is a path along the CFG from a definition to a use of the same variable without another definition of the variable between. (If such a definition occurs, it is said to \textbf{kill} the former definition)

        Def-use pairs are formed iff there exist definition-clear paths between the definition and the use of the variable.
    }

    \nDefinition{Data dependence graph}{
        Nodes just like in a CFG, but edges represent def-use relationships:

        \begin{center}
            \nImg[1]{ST-23}
        \end{center}
    }

    \nDefinition{Calculating def-use pairs}{
        There is an association (d,u) between a definition of variable v at d and its use at u iff:
        \begin{itemize}
            \item There is at least one control flow path from d to u
            \item With no intervening definition of v
            \item $v_{d}$ reaches u ($v_{d}$ is a reaching definition at u)
            \item If a control flow path passes through another definition e of the same variable v, $v_{e}$ kills $v_{d}$ at that point
        \end{itemize}
        Even if we consider only loop-free paths, the number of paths in a graph can be exponentially larger than the number of nodes and edges. In practice, don't search all paths, summarize the reaching definitions at a node over all the paths reaching it.
    
        \nDefinition{DF Algorithm}{
            The data flow algorithm is a flexible tool which can be applied over any CFG's to find out some properties of the given graph.
            
            The algorithm is based on the following set of equations, where b is some block of code (usually enough to carry out analysis on granuality of CFG node's or the edges of those nodes):
            \begin{align}
                out_{b} = trans_{b}(in_{b}) \\
                in_{b} = join_{p\in pred_{b}}(out_{p}) 
            \end{align}

            Here $trans_{b}$ is the \textbf{transfer} function, of the block b. It works on the entry state $in_{b}$ yielding the exit state $out_{b}$.
            The $join$ operation combines the exit statuses of the predecessors p, of b, yielding the \textbf{entry} state of b.

            I.e. join works out the state of some "data-based property" at the start of our block, by looking at the predecessor nodes, whereas the transfer function, works out how this property gets altered by the current block.

            Each particular type of data-flow analysis has its own specific transfer function and join operation. Some require backward analysis which is very similar except the transfer function is applied to the exit state, and yields the entry state, while the join operation works 
            on the entry states of the successors to yield the exit state, i.e.:

            \begin{align}
                out_{b} = join_{s\in succ_{b}}(in_{s})\\
                in_{b} = trans_{b}(out_{b}) 
            \end{align}

            The \textbf{entry point} (in forward flow, backward flow this would be the \textbf{exit point}) plays an important role, since it has no predecessors, its entry state is well defined at the start, if there are no cycles (no loops), solving the equations is straight forward, since we can simply work out the out states of nodes in \textbf{topological order} of the CFG.

            If \textbf{cycles are present}, an iterative approach is required
            \begin{center}
                \nImg[0.75]{ST-27}
            \end{center}
            }
    }



    \nDefinition{Reaching definition analysis}{
        Reaching definition analysis is a type of forward analysis which calculates for each program block, a set of definitions which may potentially reach this program point

        The DF equations for this analysis are:
        \begin{align}
            out_{b} = gen(b) \cup (in_{b} - kill(b)) \\
            in_{b} = \bigcup_{p \in pred_{b}}(out_{p}) 
        \end{align}

        in other words:

        \begin{align}
            trans_{b} = gen(b) \cup (in_{b} - kill(b)) \\ 
            join_{b} = \bigcup
        \end{align}

        where:
        $$
            gen(b) = \textit{set of definitions/modifiations locally available in block}         $$

        $$
            kill(b) = \textit{set of definitions, killed by definitions in the block (not locally available, but in the rest of the program)}
        $$
            }
    \nDefinition{Available expression analysis}{
        A forward analysis, which determines the set of available expressions, i.e. those which need to be recomputed (in compiler design) at a program point. An expression is available if the operands of the expression are not modified on any path from the occurence of that expression to the program point.
        This analysis uses the following DF equations:

        \begin{align}
            out_{b} = gen(b) \cup (in_{b} - kill(b)) \\
            in_{b} = \bigcap{p \in pred_{b}}(out_{p}) 
        \end{align}

        in other words:

        \begin{align}
            trans_{b} = gen(b) \cup (in_{b} - kill(b)) \\ 
            join_{b} = \bigcap
        \end{align}

        where: 
        $$
        gen(b) = \textit{set of expressions computed at b} $$
        $$
        kill(b) = \textit{set of expressions which whose variables are modified at b}
        $$
    }

    \nDefinition{Liveness analysis}{
        An example of \textbf{backward} analysis, which finds the set of live variables, i.e. those which hold a value which may be needed in later parts of a program at point p.

        in the example below:

        b = 3;
        c = 5;
        a = f(b * c);

        the set of live variables after line 2 is \{b,c\}, since they are used in the next line, and after line 1, this set is only \{b\}

        the DF equation used in this analysis are as follows:
        \begin{align}
            out(b) = 
                \begin{cases}
                    \bigcap{s \in succ_{b}}(in_{s}) \; \textit{ if b $\neq$ exit}  \\
                    \emptyset \; \textit{ otherwise }
                \end{cases}    \\
            in(b) = gen(b) \cup (out(b) - kill(b))
        \end{align}

        where:
        $$
        gen(b) = \textit{set of variables that are used in b, before any assignment} $$
        $$
        kill(b) = \textit{set of variables that are modified in s}
        $$

    }

    \nDefinition{Iterative solution of dataflow equations}{
        Cycles in a CFG complicate dataflow analyses, and necessitate a more complex approach. 
        We can deal with cycles with the following iterative algorithm:

        \begin{enumerate}
            \item Approximate the $in$ state of \textbf{each} block (for $\bigcap$ transfer functions, first guess is the union of all "gen" sets, for analyses using $\bigcup$ first guess is the empty set, i.e. the most \textbf{conservative} option).
            \item Compute the $out$ states bu applying the transfer function on the $in$ states.
            \item Now update the $in$-states are updated with the join operations
            \item Repeat steps 2-3 untill reaching a \textbf{fixpoint}, i.e. the point of convergence in which the in-states do not change (and so the out-states)
        \end{enumerate}

        The order in which we update the nodes matters, usually apply the most \textbf{natural} order, i.e. in forward analysis, you'd use \textbf{reverse postorder} - visit a node before any of its successors, unless when the successor is reached by a back edge (think loop start nodes).
        Or in the case of backward analysis, you'd want to use \textbf{postorder} i.e. visit a node after all its successor nodes have been visited (DFS) 
    }
    \nDefinition{Classification of analyses}{
        DF algorithm analyses can be classified based on:
        \begin{itemize}
            \item if a node's set depends on that of its predecessors/successors (\textbf{Forward or backward})
            \item if a node's set contains a value iff it is coming from any/all of its inputs (\textbf{any/all path})
        \end{itemize}

        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline 
                -& Any-path ($\bigcup$)&  All-paths ($\bigcap$)\\ \hline
                Forward (pred) & Reach & Avail \\ \hline 
                Backward (succ) & Live & "inevitable" \\ \hline 
            \end{tabular}   
        \end{center}

    }

    \nDefinition{Scope of DF analysis}{
        \begin{itemize}
            \item Intra-procedural - within a single method or procedure 
            \item Inter-procedural - across severall methods or classes
        \end{itemize}

        Cost/precision trade-offs for inter-procedural analysis are critical, and difficult. 
        Two main trade-offs:
        \begin{itemize}
            \item Context-sensitivity
            \item Flow-sensitivity
        \end{itemize}
    }

    \nDefinition{Context sensitivity}{
        \begin{center}
            \nImg[1]{ST-28}
        \end{center}
    }

    \nDefinition{Flow sensitivity}{
        Reach, Avail etc. were all \textbf{flow-sensitive}:
        \begin{itemize}
            \item They considered ordering and control flow decisions
            \item Within a single procedure or method, this is fairly cheap: $O(n^3)$ for n CFG nodes 
            \item Many inter-procedural flow analyses are \textbf{flow-insensitive} 
            \item $O(n^{3})$ would not be acceptable for all the statements in a program
            \item Often flow-insensitive is good enough (i.e. type checking)
        \end{itemize}
    }

    \nDefinition{Summary}{
        DF algorithms:
        \begin{itemize}
            \item[+] Can be implemented by efficient iterative algorithms 
            \item[+] Widely applicable (not just for "classic data flow" properties)
            \item[-] Unable to distinguish feasible from infeasible paths
            \item[-] Analysies spanning whole programs (e.g. alias analysis) must trade off precision against computational cost    
        \end{itemize}
    }

\nSection{Data Flow Testing}

\nDefinition{Def-Use pair testing}{
    Adequacy criteria: Each DU \textbf{pair} is exercised by at least one test case 
}

\nDefinition{Def-Use path testing}{
    Adequacy criteria: Each DU \textbf{clear path} (simple and non looping) is exercised by at least one test case 
}

\nDefinition{Def testing}{
    Adequacy criteria: Each Definition has at least one test case which exercises a DU pair containing that definition 
}

\nDefinition{DF with complex structures}{
    Problem of aliases - which references are (always or sometimes) the same?
    
    Arrays and pointers are critical for data flow analysis:
    \begin{itemize}
        \item Under-estimation of aliases may fail to include some DU paris
        \item Over-estimation, on the other hand, may introduce unfeasible test obligations
    \end{itemize} 

    For testing, it may be preferable to accept under-estimation
}

\nDefinition{Infeasibility}{
    \begin{center}
        \nImg[0.9]{ST-29}
    \end{center}

    \begin{itemize}
        \item In practice, reasonable coverage, is often but not always achievable
        \item All DU paths is more often impractical
    \end{itemize}
}

\nSection{Test Selection and Adequacy}

\nDefinition{Test suite adequacy}{
    A real way of measuring effective testing is impossible, provably undecidable.

    What we can do is define \textbf{inadequacy} criteria:
    \begin{itemize}
        \item If the specification describes different treatment in two cases, but the test suite does not check that the two cases are in fact treated differently,
         we may conclude that the test suite is inadequate to guard against faults in the program logic 
        \item If no test in the test suite executes a particular program statement, the test suite is inadequate to guard against faults in that statement.
        \item If a test suite fails to satisfy some criterion, the obligation that has not been satisfied may provide some useful information about improving the test suite 
        \item If a test suite satisfies all the obligation by all the criteria, we do not know definitively that it is an effective test suite, but we have some evidecnce of its thorougness
    \end{itemize}
}

\nDefinition{Unsatisfiability}{
    Sometimes no test suite can satisfy a criterion for a given program (defensive programming - can't happen checks for example)
    
    We can either exclude unsatisfiable obligations from the criterion or measure the extent to which a test suite approaches an adequacy criterion (percentage).

    An adequacy criterion is satisfied or not, acoverage measure is the fraction of satisfied obligations
}


\nDefinition{Comparing criteria}{
    We can try to distinguish stronger from weaker adequacy criteria. 

    Best way to do that is by describing conditions under which one adequacy criterion is provably stronger than another - 
    i.e. gives stronger guarantees
}

\nDefinition{Subsumption}{
    Test adequacy criterion A subsumes B iff, for every program P, every test suite satisfying A with respect to P also satisfies B with respect to P.

    Example: branch coverage subsumes statement coverage
}

\nDefinition{Uses of adequacy criteria}{
    We shouldn't blindly rely on coverage criteria as infallible measure of success of a test suite. Ideally use it to guide improvement to a test suite,
    not as the primary motivation behind test design!
}

\nSection{Mutation Testing}

\nDefinition{Mutation testing}{
    Adequacy criteria: A test suite satisfies the criteria, iff every generated mutant is killed by the suite.

    I.e. a mutant is a program with a small random modification. 
    A mutant is killed if the test suite fails on incorrect behaviour introduced by the mutant.
    Mutants might produce equivalent behaviour, which is not useful from a testing perspective. Want mutants which should mess up the program in weird ways

    \begin{itemize}
        \item[+] measures quality of test cases nicely 
        \item[+] provides tester with a clear target (mutants to kill)
        \item[-] computationally intensive, a lot of mutants
        \item[-] equivalent mutants are a practical problem - determining which mutants are equivalent is un-decidable!
        \item[-] really only useful at unit testing level     
    \end{itemize}
    }

\nDefinition{Mutation operators}{
    predefined program modification rules corresponding to a fault model.

    Ideally we would like mutation operators which are representative of all realistic types of faults that could occur in practice (be made by a real programmer).

    In general operator set is large, and can capture all syntactic variations in a program.
}

\nDefinition{Mutation coverage}{
    
    \begin{itemize}
        \item Complete coverage equals the killing of all non-equivalent mutants (or random sample, not necessarily all possible modifications)
        \item The amount of coverage is called the \textbf{mutation score}
        \item The number of mutants depends on definition of mutation operators and structure of software
        \item Random sampling used since this set of mutants can be large.
    \end{itemize}

}

\nDefinition{Assumptions}{
    \begin{itemize}
        \item Competent programmer assumption - they write programs which are nearly correct
        \item Coupling effect assumption - test cases that distinguish all programs differing from a correct one by only simple errors are so sensitive that they also implicitly distinguish more complex errors 
    \end{itemize}

    Some empirical evidence of the above
}

\nSection{Test Driven Developement}

\nDefinition{TDD}{
    Never write a single line of code unless you have a failing automated test.

    \begin{center}
        \nImg[0.5]{ST-30}
    \end{center}
}

\nDefinition{Summary}{
    \begin{itemize}
        \item[+] Confidence in change - can change code without shitting your pants 
        \item[+] Documents requirements via tests 
        \item[+] Discovers usability issues or problems with requirements early 
        \item[+] Regression testing = stable software = quality software
        \item[+] Major quality improvement for minor time investment
        \item[-] Programmers like to code, not test 
        \item[-] Test writing is consuming 
        \item[-] Test completeness is difficult to judge 
        \item[-] May not always be suitable
        \item Only really relevant to Functional testing          
    \end{itemize}
}

\nSection{Regression Testing}

\nDefinition{Regression testing}{
    Testing which tries to identify \textbf{regressions} in code, i.e. breaking changes introduced by modifications made to code.

    \begin{center}
        \nImg{ST-31}
    \end{center}

    Tests must be re-run after any change:
    \begin{itemize}
        \item Adding new features
        \item Changing or adapting software to new conditions
        \item Fixing bugs
    \end{itemize}

    Otherwise code may break.

    \begin{itemize}
        \item[-] takes a long time, after each change new tests pile up
        \item[-] difficult to tell which test cases should be removed or replaced/added
        \item[-] it often takes a while to run all the tests   
    \end{itemize}
}

\nDefinition{Test case maintenance}{
    Some maintenance is inevitable,
    Some maintenance should be avoided.
    Test suites should be modular.

    \textbf{Obsolete} - test cases which are obsolete are no longer valid and should be removed 
    \textbf{Redundant} - test cases which do not contribute significantly to the test suite (perhaps with coverage) may be removed since they're inlikely to find faults missed by similar test cases
}

\nDefinition{Test optimisation}{
    \begin{itemize}
        \item Re-test all - always running all tests in the test suite. Good when you want 100\% certainty the new version works on all tests developed for previous
        \item Test selection - Only select subset of test cases whose execution is relevant to changes - a test case cannot find a fault in code it doesn't execute - only execute test cases that execute changed or new code
        \item Test set minimisation - Identifying and removing redundant test cases to reduce test suite's size
        \item Test set prioritisation - Sort test cases in order of increasing cost per additional coverage - run n tests in that order untill max cost is reached
    \end{itemize}
}

\nDefinition{Control-flow and Data-flow Test Selection}{
    \begin{itemize}
        \item Re-run test cases only if they include changed elements
        \item Elements may be modified control flow nodes and edges, or DU pairs in data flow 
        \item Record elements touched by each test case in database to automate this process
    \end{itemize}
}
\nDefinition{Specification based test selection}{
    \begin{itemize}
        \item Pick test cases that test new and changed functionality
        \item No guarantee of independence - a test case that isn't for changed or added feature X might find a bug in feature X anyway 
        \item Typical approach: execute all tests, but start with those that related to changed and added features by specification 
    \end{itemize}
}

\nDefinition{Test set minimization}{
    \begin{itemize}
        \item Maximize coverage with minimum number of test cases
        \item The minimization algorithm can be exponential in time 
        \item Does not occur in our experience 
        \item Stop after a pre-defined number of iterations
        \item Obtan an approximate solution by using a greedy heuristic
    \end{itemize}

    \begin{center}
        \nImg[0.7]{ST-32}
    \end{center}
}

\nDefinition{Test set prioritisation}{
    \begin{center}
        \nImg{ST-33}
    \end{center}

    \begin{itemize}
        \item Execute all test cases, eventually
        \item Execute some sooner than others 
        \item Possibly prioritize using RR scheduling, Run tests which detected more faults in the past first, or based on the structure of the tests or code
    \end{itemize}
}

\nSection{Security Testing}

\nDefinition{Security testing}{
    \begin{itemize}
        \item Normal testing aims to ensure program meets customer requirements in terms of features and functionality
        \item Security testing aims to ensure the program fulfills security requirements - more interested in misuse cases, weird "corner" cases 
    \end{itemize}
}

\nDefinition{Common security testing approaches}{
    \begin{itemize}
        \item Test for known vulnerability types
        \item Attempt directed or random search of program state space to uncover the corner cases 
    \end{itemize}
}

\nDefinition{Penetration testing}{
    \begin{itemize}
        \item Manually try to "break" software
        \item Relies on human intuition and experience 
        \item Typically involves looking for known common problems 
        \item Can uncover problems that are impossible or difficult to find using automated methods - results completely dependent on skill of tester
    \end{itemize}
}

\nDefinition{Fuzz testing}{
    idea: send semi-valid input to a program and observe behaviour.

    \begin{itemize}
        \item Black-box testing - System treated as a "black box"
        \item The only feedback is the output and/or externally observable behaviour
        \item First proposed in a 1990 paper where completely random data was sent to 85 common Unix utilities in 6 different systems. 24-33\% crashed.
        \item Remember: crash implies memory protection errors, often signs of exploitable flaws in the program
    \end{itemize}

    \begin{center}
        \nImg[0.6]{ST-34}
    \end{center}

    \begin{itemize}
        \item Simplest method: completely random imput - wont work well in practice - input deviates too much from expected format, rejected early in processing 
        \item Mutation based fuzzing
        \item Generation based fuzzing 
    \end{itemize}
}

\nDefinition{Mutation based fuzzing}{
    Start with valid seed input, and mutate it:
    \begin{itemize}
        \item Flip some bits, change value of some bytes
        \item Programs that have highly structured input, e.g. XML, may require "smarter" mutations
    \end{itemize}

    How do we select appropriate input?:
    \begin{itemize}
        \item If official test suites are available, these can be used 
        \item Generally mostly used for programs that take files as input
        \item Trickier to do when interpretation of inputs depends on program state, e.g. network protocol parsers 
    \end{itemize}

    \begin{itemize}
        \item[+] easy to get started - little knowledge required of specific input format needed 
        \item[-] typically yields low code coverage, inputs tend to deviate too much from expected format - rejected early by sanity checks  
        \item[-] hard to reach "deeper" parts of program by random guessing (i.e. unlikely to guess magic constants correctly in switch cases etc.)
    \end{itemize}

    \begin{center}
        \nImg[0.8]{ST-35}
    \end{center}
}

\nDefinition{Generation based fuzzing}{
    Idea: use a specification of the input format (a grammar or similar) to automatically generate semi-valid inputs.

    Usually combined with various fuzzing heuristics that are known to trigger certain vulnerability types:
    \begin{itemize}
        \item Very long strings, empty strings
        \item Strings with format specifiers, "extreme" format strings
        \item Very large or small values
        \item Negative values where positives are expected
    \end{itemize}

    \begin{itemize}
        \item[+] Input is much closer to the expected, much better coverage
        \item[+] can includ emodels of protocol state machines to send messages in the sequence expected by SUT
        \item[-] Requires input format to be known
        \item[-] May take considerable time to write the input format grammar/specification.   
    \end{itemize}
}

\nDefinition{Fuzzing: Dispatcher}{
    Repsponsible for running the SUT, on each input generated by fuzzer
    \begin{itemize}
        \item Must provide suitable environment for SUT 
        \item SUT may modify environment (might require a VM)
    \end{itemize}
}

\nDefinition{Fuzzing: The Assessor}{
    Must automatically assess observed SUT behaviour to determine if a fault was triggered:
    \begin{itemize}
        \item For C/C++ programs: monitor for memory access violations, e.g. out of bounds reads or writes
        \item Simplest method: check if crashed 
        \item Problem: SUT may catch signals/exception to gracefully handle e.g. seg faults which makes it difficult to tell if a fault has occured 
        \item solution is to attach a programmable debugger to SUT 
        \item This can catch signals/exceptions prior to being delivered to application 
        \item Can help manual diagnosis of detected faults by recording stack traces, value of registers etc. 
        \item Not all result in failures though
        \item An out-of-bound read/write or use-after-free may e.g. not result in a memory access violation. 
        \item Solution: Use a dynamic-analysis tool that can monitor what goes on "under-the-hood" (this is very slow)
    \end{itemize}
}

\nDefinition{Limitations of Fuzz testing}{
    \begin{itemize}
        \item Many programs have an infinite input space, and state space - combinatorial explosion!
        \item Conceptually simple idea, but many practical challenges 
        \item Difficult to create a trully generic fuzzing framework that can cater for all possible input formats - might often require a custom fuzzer for each SUT 
        \item Semi-randomly generated inputs are very unlikely to trigger certain faults
        \item mutation based fuzzing can typically onl find "low hanging fruit" - shallow bugs which are easy to find 
        \item Generation-based fuzzers are much better but also require much more effort
    \end{itemize}

    \begin{center}
        \nImg[0.7]{ST-36}
    \end{center}
}


\nDefinition{Concolic testing}{
    idea: combine concrete and symbolic execution 

    \begin{itemize}
        \item Execute program for real on some input, record path taken 
        \item Encode path as query to SMT solver and negate one branch condition on the path 
        \item ask the solver to find new satisfying input that will give a different path 
    \end{itemize}

    Reported bugs are always accompanied by an input that triggers the bug - reported bugs are always real bugs
}

\nDefinition{Challenges of concolic testing}{
    Number of paths increases exponentially with number of branches:
    \begin{itemize}
        \item Most real-world programs have an infinite state space 
        \item Number of loop iterations may depend on size of input 
        \item Not possible to explore all paths - need strategy to explore interesting parts of the program
        \item DFS will easilly get stuck in one part of the program (a loop)
        \item BFS will take a very long time to reach deep states 
        \item Need to try smarter ways of exploring program state space  
    \end{itemize}
}

\nDefinition{Generational search ("whitebox fuzzing")}{
    \begin{itemize}
        \item Run program on concrete seed input and record path constraint 
        \item For each branch condition:
            \begin{itemize}
                \item Negate condition and keep the earlier conditions unchanged.
                \item Have SMT solver generate new satisfying input and run program with that input 
                \item Assign input a score based on how much it improves coverage
                \item Store input in a global worklist sorted on score 
            \end{itemize}
        \item Pick input at head of worklist and repeat
    \end{itemize}

    Coverage based heuristics avoid getting "stuck" like DFS. 
    If one test case executes the exact same code as in a previous test case, its score will be 0, so it will probably never be used again.
    Gives sparse but more diverse search of the paths of the program.

    \begin{itemize}
        \item Implements a form of greedy search heuristic 
        \item For example, loops may only be executed only once
        \item Only generates input "close" to the seed input 
        \item Will try to explore weird corner cases in code exercised by seed input 
        \item Choice of seed input important for good coverage
    \end{itemize}

    \begin{itemize}
        \item[+] Proven to work well in practice
        \item[+] Source code not needed, since can be done at machine code level,
        \item[-] Sacrifices some coverage due to additional approximations needed when working on machine code
        \item[-] Solving SAT/SMT problems is NP-complete 
        \item[-] may take unacceptably long time to solve certain constraints 
        \item[-] Solving SMT queries with non-linear arithmetic (multiplication, division, modulo etc) is undecidable in general - but decidable in fixed-precision data types which are typically used   
        \item[-] any usage of cryptographical functions usually ends in failure of symbolic execution
    \end{itemize}

    \begin{center}
        \nImg[0.7]{ST-37}
    \end{center}
    }

\nDefinition{Greybox fuzzing}{
    Probability of hitting a "deep" level of the code decreases exponentially with depth of code for mutation based fuzzing 

    Similarly, the time required for solving an SMT query is high, and increases exponentially with depth of path constraint 

    Black-box fuzzing is too "dumb" and whitebox fuzzing may be "too smart"

    idea: find spot in-between.

    Instead of recording full path constraint, record light-weight coverage information to guide fuzzing.
    Use evolutionary algorithms to learn input format 
    \begin{itemize}
        \item AFL is considered the current state-of-the-art in fuzzing 
        \item Performs regular mutation based-fuzzing (with several strategies) and measures code coverage 
        \item Every generated input that resulted in any new coverage is saved and later re-fuzzed
        \item This extremely simple algorithm allows AFL to learn how to reach deeper parts of the program 
        \item Also highly optimized for speed - can reach several thousand test cases per second 
        \item Often beats smarter and slower methods like whitebox fuzzing 
        \item Found hundreds of serious vulnerabilities in open-source programs
    \end{itemize}
}

\nSection{Integration and Component Based Testing}

\nDefinition{Integration testing}{
    Integration testing serves as a process check:
    \begin{itemize}
        \item If module faults are revealed in integration testing, they signal inadequate unit testing 
        \item If integration faults occur in interfaces between correctly implemented modules, the errors can be traced to module breakdown and interface specifications
    \end{itemize}
    \begin{center}
        \nImg[0.9]{ST-38}
    \end{center}
}

\nDefinition{Integration Faults}{
    \begin{itemize}
        \item Inconsistent interpretation of parameters of values (mixed units)
        \item Violation of value domains, capacity, or size limits (buffer overflows)
        \item Side effects on parameters or resources (conflicts on temp files)
        \item Omitted or misunderstood functionality (inconsistent interpretation of web hits)
        \item Nonfunctional properties (Unanticipated performance issues)
        \item Dynamic mismatches
    \end{itemize}
}

\nDefinition{Big bang integration test}{
    Test only after integrating all modules:
    \begin{itemize}
        \item[+] Does not require scaffolding 
        \item[-] Minimum observability, diagnosability, efficacy, feedback 
        \item[-] High cost of repair   
    \end{itemize}
}

\nDefinition{Structural and functional strategies}{
    \textbf{Structural orientation}: modules constructed, integrated and tested based on a hierarchical project structure (top-down, bottom-up, sandwich etc)
    
    \begin{center}
        \nImgs{ST-39}{ST-40}
        \nImgs{ST-41}{ST-41}
    \end{center}

    \textbf{Functional orientation}: modules integrated according to application characteristics or features (threads, critical module)

    \begin{center}
        \nImgs{ST-42}{ST-43}
    \end{center}
    }

\nDefinition{Critical Modules}{
    Strategy: start with riskiest modules:
    \begin{itemize}
        \item Risk assesment necessary first step
        \item May include technical risks (is X feasible), process risks (is schedule for X realistic) or other risks 
        \item May resemble thread or sandwich process in tactics for flexible build order 
        \item Key point is risk-oriented process (integration as risk-reduction activity, deliver bad news early)
    \end{itemize}
}

\nDefinition{Choosing a strategy}{
    \begin{itemize}
        \item Functional strategies require more planning 
        \item but also provide better process visibility, especiall in complex systems 
        \item Possible to combine: top-down, bottom-up or sandwich are reasonable for relatively small components and subsystems 
        \item Combination of threads and critical modules integration testing are often preferred for larger subsystems
    \end{itemize}
}

\nDefinition{Component}{
    Reusable unit of deployment and composition;
    \begin{itemize}
        \item Deployed and integrated multiple times 
        \item Integrated by different teams (usually)
        \item Characterised by an interface or contract 
        \item Often larger grain than objects or packages (a database system, or geometry module)
    \end{itemize}
}

\nDefinition{Component interface contracts}{
    \begin{itemize}
        \item API is distinct from implementation
        \item DOM interface for XML is distinct from possible implementations 
        \item Interface includes everything that must be known to use the component 
        \item More than just method signatures, exceptions etc.
        \item May include non-functional characteristics like performance, capacity and security
        \item May include dependence on other components
    \end{itemize}
}

\nDefinition{Challenges in testing components}{
    \begin{itemize}
        \item The component builder's challenge - impossible to know all the ways a component may be used 
        \item Difficult to recognize and specify all potentially important properties and dependencies 
        \item The component user's challenge - no visibility "inside" the component, often difficult to judge suitability for a particular use and context
    \end{itemize}
}

\nDefinition{Testing a Component: Producer view}{
    \begin{itemize}
        \item First: thorough unit and subsystem testing
        \item Includes thorough functional testing based on API
        \item Reusable component requires at least twice the effort in design, implementation and testing as a subsystem constructed for a single use 
        \item Second: thorough acceptance testing
        \item Based on scenarios of expected use
        \item Includes stress and capacity testing
    \end{itemize}
}
\nDefinition{Testing a component: User view}{
    \begin{itemize}
        \item Not primarily to find faults in the component
        \item Major question: is the component suitable for this application ?
        \item Primary risk is not fitting the applciation context
        \item Unanticipated dependence or interactions with environment
        \item Performance or capcaity limits
        \item Missing functionality, misunderstood API
        \item Risk high when using component for first time
        \item Reducing risk: trial integration early 
        \item Often worthwile to build driver ot test model scenarios, long before actual integration
    \end{itemize}
}
\nSection{System and Acceptance Testing}

\nDefinition{System and acceptance testing}{
    \begin{center}
        \nImg[0.9]{ST-44}
    \end{center}

    \begin{itemize}
        \item Comprehensive
        \item Based on specification of observable behaviour
        \item Independent of design and implementation
        \item Avoid repeating software design errors in system test design
        \item should be performed by a different organisation 
    \end{itemize}
}

\nDefinition{Incremental system testing}{
    \begin{itemize}
        \item System test are often used to measure progress
        \item System test suite covers all features and scenarios of use
        \item As project progresses, the system passes more and more system tests 
        \item Assumes a "threaded" incremental build plan: features exposed at top level as they are developed
    \end{itemize}
}

\nDefinition{Global properties}{
    \begin{itemize}
        \item Performance, latency reliability etc.
        \item Early and incremental testing is still necessary but provide only estimates
        \item A major focus of system testing
        \item The only opportunity to verify global properties against actual system specifications
        \item Especially to find unanticipated effects, e.g. an unexpected performance bottleneck
    \end{itemize}
}

\nDefinition{Context-dependent properties}{
    \begin{itemize}
        \item Beyond system-global: some properties depend on the system context and use 
        \item Example: performance properties depend on environment and configuration 
        \item Example: privacy depends both on system and how it is used 
        \item Example: security depends on threat profiles (and threats change)
        \item Testing is just one part of the approach
    \end{itemize}
}

\nDefinition{Stress testing}{
    \begin{itemize}
        \item Often requires extensive simulation of the execution environment
        \item What happens when parameters are pushed ? (10,100 times more ?)
        \item Often requires more resources (human and machine), than typical test cases 
    \end{itemize}
}

\nDefinition{Capacity testing}{
    \begin{center}
        \nImg[1]{ST-45}
    \end{center}
}
\nDefinition{Security testing}{
    \begin{center}
        \nImg[1]{ST-46}
    \end{center}
}
\nDefinition{Performance testing}{
    \begin{center}
        \nImg[1]{ST-48}
    \end{center}
}

\nDefinition{Compliance testing}{
    \begin{center}
        \nImg[1]{ST-49}
    \end{center}
}

\nDefinition{Documentation testing}{
    \begin{center}
        \nImg[1]{ST-50}
    \end{center}
}

\nDefinition{Estimating dependability}{
    Measuring quality, not searching for faults.

    \begin{itemize}
        \item \textbf{reliability} - survival Probability
        \item \textbf{Availability} - fraction of time a system meets specs 
        \item \textbf{Failsafe} - system fails to a known safe state 
        \item \textbf{Dependability} - system does the right thing at right time
    \end{itemize}
}

\nDefinition{Statistical sampling}{
    \begin{itemize}
        \item Need valid operational profile (model)
        \item sometimes from older version of system 
        \item sometimes from operational environment 
        \item sensitivity testing reveals parameters which are most important 
        \item Clear definition of what is being measured 
        \item Many random samples
    \end{itemize}
}

\nDefinition{System reliability}{
    \begin{center}
        \nImg[1]{ST-51}
        \nImg[1]{ST-52}
    \end{center}   
}

\nDefinition{Mean time to failure (MTTF)}{
    \begin{center}
        \nImg[1]{ST-53}
    \end{center}
}

\nDefinition{Serial system reliability}{
    \begin{center}
        \nImg[1]{ST-54}
    \end{center}
}

\nDefinition{Parallel system reliability}{
    \begin{center}
        \nImg[1]{ST-55}
    \end{center}
}
\nDefinition{Process-based measures}{
    \begin{itemize}
        \item Less rigorous than statistical 
        \item Alpha - real users, controlled enviormnent 
        \item Beta -real users (real environment)
    \end{itemize}    

}

\nDefinition{Usability testing}{
    \begin{center}
        \nImg[1]{ST-56}
    \end{center}
}

\nDefinition{Reliability testing}{
    \begin{center}
        \nImg[1]{ST-57}
    \end{center}
}

\nDefinition{Availability/reparability testing}{
    \begin{center}
        \nImg[1]{ST-58}
    \end{center}
}
\nSection{Concurrency Testing}

\end{document}
