\documentclass{article}

\usepackage{notes}
\usepackage{array}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{cancel}

\usepackage[hidelinks]{hyperref}
\usepackage[a4paper,margin=0.5in]{geometry}
\renewcommand\vec{\mathbf}

\graphicspath{{./Images/}}

\everymath{\displaystyle}
\DeclarePairedDelimiter{\ceil}{\lceil}{\rceil}

\begin{document}

\title{ST Condensed Summary Notes For Quick In-Exam Strategic Fact Deployment }
\author{Maksymilian Mozolewski}
\maketitle
\tableofcontents

\pagebreak

\nChapter{ST}

\nSection{Introduction}

\nDefinition{Validation}{
    Checking that the software specification adheres to the requirements of the customer. Are we building the right product?
}
\nDefinition{Verification}{
    Checking that the software adheres to the software specification. Are we building the product right ?
}

\nDefinition{Software Specification}{
    Defines the product being created, includes:
    \begin{itemize}
        \item \textbf{Functional requirements} - describe features of a product
        \item \textbf{Non-Functional requirements} - constraints on the product (e.g. security, reliability etc.)
    \end{itemize}
}

\nDefinition{Software bug}{
    A software bug occurs when one of the following occurs:
    \begin{itemize}
        \item The software does not do something that the true specification says it should do.
        \item The software does something that the specification says it should not do.
        \item The software does something that the specification does not mention.
        \item The software does not do something that the product specification does not mention but should.
        \item The software is difficult to understand, hard to use, slow, etc..
    \end{itemize}
}

\nDefinition{Phases of Software Testing}{
    \begin{itemize}
        \item Test generation - Writing tests, Automatic test generation etc.
        \item Test Execution - Actually executing tests, build tools etc.
        \item Test Oracle - determining what the output for each test \textbf{should} be
        \item Test Adequacy - determining the effectiveness of the test suite.
    \end{itemize}
}

\nDefinition{Software Fault}{
    A static defect in software 
}

\nDefinition{Software Error}{
    An incorrect internal state that is the manifestation of some \textbf{fault}
}

\nDefinition{Software Failure}{
    External, incorrect behaviour with respect to the requirements, or other description of expected behaviour. (The actual moment the software fails, for some input space)
}

\nSection{Unit Testing}

\nDefinition{Unit Testing}{
    Looking for errors in a subsystem in isolation:

    \begin{itemize}
        \item Generally a class or object
        \item Junit in Java
    \end{itemize}
}

\nDefinition{Junit 4}{
    \begin{center}
        \nImg[1]{ST-1}
    \end{center}
}

\nDefinition{Unit Testing Tips}{
    \begin{itemize}
        \item Tests need to be \textbf{atomic} - the failure of a test should pinpoint the location of the fault 
        \item Each test name should be clear, long and descriptive 
        \item Assertions should always have clear messages to know what failed 
        \item Write many small tests, not one big one ($\approx 1$ assertion per test)
        \item Test for expected errors/exceptions too
        \item Choose descriptive assert method (not always assertTrue, i.e. prefer more specific methods)
        \item Choose representative test cases for equivalent input classes 
        \item Avoid complex logic in test methods if possible 
        \item Use helpers, @Before to reduce redundancy between tests
        \item Never rely on test execution order, or call other test fixtures (apart from helpers)
        \item Dont share state between tests 
    \end{itemize}
}


\nSection{Functional Testing}

\nDefinition{Functional Testing (Black-box testing)}{
    Deriving tets cases from program specifications. Functional refers to the source of information used in test case design, not to what is tested 
    Also known as \textbf{specification-based} testing.

    Functioal specification = description of intended program behaviour. We act as if we didn't know anything about the code (hence black-box)
    
    Example:
    \begin{center}
        \nImg[0.95]{ST-2}
    \end{center}

    \begin{center}
        \nImg[0.8]{ST-3}
    \end{center}


    \begin{itemize}
        \item[+] Often reveals ambiguities and inconsistency in specifications 
        \item[+] Useful for assesing testability
        \item[+] Useful explanation of specification (test cases outline the specification)
        \item[+] does not require any code to write tests (TDD)
        \item[+] Ideal for missing logic (if we only test what's there - white box testing, we won't identify what's missing!)
    \end{itemize}
    }


\nDefinition{Random Testing}{
    Pick possible inputs uniformly. 
    \begin{itemize}
        \item[+] avoids designer bias (test designer can make logical mistakes and bad assumptions)
        \item[-] treats all inputs as equally valuable
        \item[-] Real faults are distributed non-uniformly (think finding roots, 3 important cases, rest is same class)
        \item[-] input space is often extremely large    
    \end{itemize}
}
\nDefinition{Systematic Functional Testing}{
    Try to select inputs that are especially valuable (non-uniform selection)

    Usually by choosing representatives of classes that are apt to fail often or not at all.

    Functional Testing \textbf{IS} systematic testing
    \begin{center}
        \nImg[0.9]{ST-4}
    \end{center}
}

\nDefinition{Partition Testing}{
    Exploit some knowledge to choose input samples more likely to include "special" or trouble-prone regions of input space.
    Failures are sparce in the whole input space but we may find regions in which they are dense

    Ideally want \textbf{Equivalance classes} of inputs - for example, the positive numbers might be equivalent with respect to the program, i.e. 1 and 2 will cause the same general behaviour.

    Then we only really need one value from each partition, since if one condition/value in a partition passes, all others will also pass and vice-versa!

    \begin{center}
        \nImg[0.9]{ST-5}
    \end{center}
    }

\nDefinition{Quasi-Partition testing}{
    Separate the input space into classes, whose union is the entire space, where the classes can overlap
}

\nDefinition{Boundary Value Analysis}{
    Testing of the inputs on the boundaries between equivalence partitions (both \textbf{valid and invalid})
    \begin{center}
        \nImg[0.8]{ST-6}
    \end{center}
    }

\nDefinition{Functional vs Structural (White-box testing)}{
    Functional testing applies at all granuality levels: Unit, Integration, System and Regression testing.

    Structural testing applies to a relatively small part of the system: Unit and Integration testing.
}

\nDefinition{Functional Testing Process}{
    \begin{enumerate}
        \item Decompose the specification into \textbf{Independently testable features} (think functions: Airport connection check etc. )
        \item Select representative values of each input, or behaviours of a model (which part of the input space are interesting ?)
        \item Form test specifications (input/output values, model behaviours etc)
        \item Produce and execute actual tests
    \end{enumerate}
}

\nSection{Combinatorial Testing}

\nDefinition{Combinatorial Testing}{
    Identify distinct attributes that can be varied:
    \begin{itemize}
        \item Environment - characterized by combination of hardware and software (IE, Firefox, XP, OSX, 10GB Ram etc.)
        \item Input Parameters 
        \item State Variables
    \end{itemize}
    \textbf{Equivalence partitioning} and \textbf{boundary value analysis} can be used to identify values of each attribute.

    Systematically generate combinations of values for different attributes to be tested (combinations of attributes)

    It's often impractical to test all of the combinations of distinct attributes, so we can pick certain subsets according to combination strategies.
    }

\nDefinition{Key ideas}{
    \begin{itemize}
        \item Category-partition testing  - identification of values that characterise the input space, done manually, the actual combinations of attributes are generated automatically.
        \item Pairwise testing - systematically test interactions among attributes of the program input space with a relatively small number of test cases 
        \item Catalog-based testing - aggregate and synthesize the experience of test designers in a particular organization or applciation domain, to aid in identifying attribute values (automates the manual parts too) 
    \end{itemize}
}


\nDefinition{Category Partitioning (manual)}{
    \begin{enumerate}
        \item Decompose the specification into ITF's (Independently testable features)
        \item For each feature identify:
            \begin{itemize}
                \item Parameters (i.e. Arriving flight, Departing flight)
                \item Environmental elements (State of airport database) 
            \end{itemize}
        \item For each parameter and environment element identify \textbf{elementary characteristics} or \textbf{categories} (arrival destination matches departure origin, origin and destination airports exist in the base, database is available)
        \item Identify relevant values, For each characteristic (\textbf{category}) identify \textbf{classes of} values, e.g.:
            \begin{itemize}
                \item normal values (1,2,3)
                \item boundary values  (0,-1)
                \item special values (INF)
                \item error values (' ',"","CHEESE")
            \end{itemize}
        
        \item Introduce constraints ("[error]" cases are tested once, [property if-property] run only in combination with if-property, [single] run once)
    \end{enumerate}
}


\nDefinition{Constraints}{

    \begin{itemize}
        \item[-] [error], value class which corresponds to erroneous values, only needs to be tested once, with all the other parameters and environments set to any valid values
        \item[-] [property] [if-property] mark a value class as a property to be referenced by others
        \item[-] [if-property] only include this property in combinations combined with value classes marked with the given property     
        \item[-] [single], same as error, but difference in semantics - rationale
        
    \end{itemize}

    \begin{center}
        \nImg[0.8]{ST-7}
    \end{center}
}

\nDefinition{Pairwise Combinatorial Testing}{
    Generate combinations that efficently cover all pairs of classes. Most failures are triggered by single 
    values or combinations of a few values. Covering pairs, reduces the number of test cases, but reveals most faults.

    \begin{center}
        \nImg[0.9]{ST-9}
    \end{center}

    When designing test cases, we set all irrelevant values (not included in the pair) with valid values.

    In this case we want test cases which cover, all combinations of values between Display Mode and Language, Display Mode and Fonts, Fonts and Color and so on and so on.
    
    This would yield 136 pairs in total for this example ($3 \cdot 4 + 3 \cdot 3 ...$)
    }
 

\nDefinition{Catalog Based Testing}{
    Deriving value classes requires human judgement. Gathering experience in a systematic collection can:
    \begin{itemize}
        \item Speed up the test design process
        \item Routinize many decisions, better focusing human effort 
        \item Accelerate training and reduce human error 
        \item Catalaogs \textbf{capture the experience of test designers} by listing important cases for each possible type of variable
    \end{itemize}

    Process:
    \begin{enumerate}
        \item Analyze the initial specification to identify simple elements:
            \begin{itemize}
                \item Pre-conditions
                \item Post-conditions
                \item Definitions
                \item Variables
                \item Operations
            \end{itemize}
        \item Derive a first set of test case specifications from pre-conditions, post-conditions and definitions 
        \item Complete the set of test case specifications using test catalogs
    \end{enumerate}

    \begin{center}
        \nImg{ST-8}
    \end{center}
    }

\nSection{Finite Models}

    \nDefinition{Properties of models}{
        \begin{itemize}
            \item Compact: representable and manipulable in a reasonably compact form 
            \item Predictive: must represent some salient characteristics of the modelled artifact well enough to distinguish between good and bad outcomes of analysis
            \item Semantically meaningful: it is usually necessary to interpret analysis results in a way that permits diagnosis of the causes of failure
            \item Sufficiently general: models intended for analysis of some important characteristic must be general enough for practical use in the intended domain of application
        \end{itemize}
    }

    \nDefinition{Intraprocedural Control Flow Graph}{
        Nodes represent regions of source code (not necessarily lines), each block of code in a node has a single entry and exit point.
        Edges represent posibility that control flow transitions from end of one block to beginning of another
        
        \begin{center}
            \nImg[0.9]{ST-10}
        \end{center}
        }

    \nDefinition{Call Graph}{
        Similiar to control flow graph, but at higher level, nodes represent functions, and edges the possibility of a call from one call to another.

        \begin{center}
            \nImg[0.9]{ST-11}
        \end{center}

        \textit{Note: the 'calls' relation represented by the edges, is overestimated via call graphs, some calls are simply impossible in real execution due to state}
    }

    \nDefinition{Context Insensitive Call Graphs}{
        Each edge represents any call to a procedure with any context
        \begin{center}
            \nImg[0.9]{ST-12}
        \end{center}
    }

    \nDefinition{Context Sensitive Call Graphs}{
        Each edge represents a unique \textbf{call stack} as well as function call (different calls have different edges)
        \begin{center}
            \nImg[0.9]{ST-13}
        \end{center}
    }


    \nDefinition{Context exponential growth}{
        The call stacks grow exponentially making context sensitive call graphs very large:

        \begin{center}
            \nImg[0.9]{ST-14}
        \end{center}
    }

    \nDefinition{Finite State Machines}{
        Each node is a state, and each edge is a possible transition, can be used to model high-level program behaviour

        \begin{center}
            \nImg[0.6]{ST-15}
        \end{center}
    }

    \nDefinition{Models and System properties}{
        Models might be easier to validate against program specifications, and if the program satisfies some model which satisfies 
        the program specification, then those specifications are met by the program.

        \begin{center}
            \nImg[0.7]{ST-16}
        \end{center}
    }

\nSection{Structural Testing}

    \nDefinition{Structural Testing (White-box testing)}{
        Type of testing based on the structure of the program itself, still testing product functionality against the specification but the throughness measures are changed.

        Ansers the question "what is missing in our test suite?" - If a part of a program is not executed by any test case in the suite, faults in that part cannot be exposed - different types of structural testing define \textbf{part} differently. 
        
        \begin{itemize}
            \item[-] Executing all control flow elements does not guarantee fidning all faults - might depend on state
            \item[+] Increases confidence in thoroughness of testing, removes obvious inadequacies  
            \item[+] Coverage criteria can be automated fully
        \end{itemize}
        }

    \nDefinition{Statement Testing}{
        Adequacy criterion: each statement (or node in CFG) must be executed at least once
        $$\frac{\textit{executed statements}}{\textit{statements}}$$
    

        \begin{center}
            \nImg[1]{ST-17}
        \end{center}
       
        \begin{itemize}
            \item[-] can miss some cases (if branches without an else case for example)
        \end{itemize}
        \textit{Note: no essential differences between statement granuality, 100\% CFG node coverage iff 100\% statement coverage}    
    

    }

    \nDefinition{Branch Testing}{
        Adequacy criterion: each branch (edge in the CFG) must be executed at least once 
        $$\frac{\textit{executed branches}}{\textit{branches}}$$
    
        In above example:
        \begin{center}
            \nImg[0.7]{ST-18}
        \end{center}

        \begin{itemize}
            \item[+] subsumes statement testing
            \item[-] can still miss some states (compound logical expressions such as $A | B$, can lead to both branches with only A being varied)  
        \end{itemize}
        }
    
    \nDefinition{Basic condition testing}{
        Adequacy criterion: each atomic condition must be executed once when it's true and once when it's false. i.e. $A | B$ requires that A,B be False and True in some test (not all combinations though).
    
        $$\frac{\textit{truth values taken by all basic conditions}}{2\cdot \textit{basic conditions}}$$
        
        \begin{itemize}
            \item does not imply branch coverage (not realy comparable)
        \end{itemize}
        }
    
    \nDefinition{Compound condition testing}{
        Adequacy criterion: each compound condition must have all possible combinations of its atomic conditions executed.
        $$\frac{\textit{combinations of compound conditions executed}}{\textit{compound conditions cases}}$$

        \begin{itemize}
            \item[+] subsumes branch testing
            \item[-] $2^{n}$ cases arising from each condition with n atomic conditions  
        \end{itemize}
    }

    \nDefinition{Modified condition/decision (MC/DC)}{
        Adequacy criterion: all \textbf{important} combinations of conditions are tested, i.e. each basic condition shown to independently affect the outcome of each compound condition.

        \begin{itemize}
            \item For each basic condition C, two test cases are required 
            \item In each case, the values of all the other basic conditions must stay the same, and only C must vary
            \item The whole condition must evaluate to true in one case, and false in another 
            \item[+] n+1 test cases for n basic conditions (for minimal test suite)
            \item[+] subsumes statement, basic condition and branch testing
            \item[+] good balance of thoroughness and test size (widely used)
            
        \end{itemize}

        \begin{center}
            \nImg[0.9]{ST-19}
        \end{center}
    }


    \nDefinition{Path adequacy}{
        Adequacy criterion: each path must be executed at least once. I.e. a path is just a walk over the CFG (along the directed edges)
        $$\frac{\textit{executed paths}}{\textit{paths}}$$
        \begin{itemize}
            \item[-] number of paths is infinite if any loops are present in the CFG
            \item[-] usually impossible to satisfy 
            \item[+] very thorough   
        \end{itemize}
        }

    \nDefinition{Boundary interior path testing}{
        Adequacy criterion: Requires the set of paths from the root of the tree to each leaf is the required set of subpaths for boundary/interior coverage.
        $$\frac{\textit{paths}}{\textit{unique paths reaching a leaf}}$$
    
        \begin{center}
            \nImg[0.9]{ST-20}
        \end{center}

        \begin{itemize}
            \item[-] Still number of leaf paths can grow exponentially (parallel if statements N statements = $2^N$ paths that must be traversed)
            \item[-] some paths might not be reachable since some conditions are not independent 
            \item[+] more managable 
        \end{itemize}
        }

    \nDefinition{Loop boundary adequacy}{
        Adequacy criterion: for every loop:
            \begin{itemize}
                \item In at least one test case, the loop body is iterated zero times
                \item In at least one test case, the loop body is iterated once 
                \item In at least one test case, the loop body is iterated more than once
            \end{itemize}
    }

    \nDefinition{LCSAJ Adequacy}{
        Adequacy criterion: A test suite satisfies LCSAJ adequacy, if all the LCSAJ segments are executed in some test.

        $$\frac{\textit{LCSAJ's executed}}{all LCSAJ's}$$
        LCSAJ stands for Linear Code Sequence and Jump, i.e. a node in the CFG followed by a branch.

        Various "thoroughness" levels:
        \begin{itemize}
            \item $TER_{1}$ = statement coverage
            \item $TER_{2}$ = branch coverage
            \item $TER_{n+2}$ = coverage of n consecutive LCSAJs 
        \end{itemize}
    }

    \nDefinition{Cyclomatic adequacy}{
        Adequacy criterion: number of "linearly independent" paths (if you treat each node as an entry in a vector signifying the presence of an edge in the given path) is equal to cyclomatic number of the CFG.

        The cyclomatic number can be calculates as: $e - n + 2$
    }

    \nDefinition{Subsumption relation}{
        \begin{center}
            \nImg[0.9]{ST-21}
        \end{center}
    }
\nSection{Data Flow Models}

    \nDefinition{Def-Use Pairs}{
        Associates a point in a program where a value is produced with a point where it is used. 

        Definition - where a variable gets a value assigned:
        \begin{itemize}
            \item Variable declaration
            \item Variable initialization
            \item Assignment
            \item Values received by a parameter
        \end{itemize}

        Use - extraction of a value from a variable:
        \begin{itemize}
            \item Expression
            \item Conditional statement
            \item Parameter passing 
            \item Return
        \end{itemize}

        \begin{center}
            \nImg[1]{ST-22}
        \end{center}
    }

    \nDefinition{Definition-Clear path}{
        A definition-clear path is a path along the CFG from a definition to a use of the same variable without another definition of the variable between. (If such a definition occurs, it is said to \textbf{kill} the former definition)

        Def-use pairs are formed iff there exist definition-clear paths between the definition and the use of the variable.
    }

    \nDefinition{Data dependence graph}{
        Nodes just like in a CFG, but edges represent def-use relationships:

        \begin{center}
            \nImg[1]{ST-23}
        \end{center}
    }

    \nDefinition{Calculating def-use pairs}{
        There is an association (d,u) between a definition of variable v at d and its use at u iff:
        \begin{itemize}
            \item There is at least one control flow path from d to u
            \item With no intervening definition of v
            \item $v_{d}$ reaches u ($v_{d}$ is a reaching definition at u)
            \item If a control flow path passes through another definition e of the same variable v, $v_{e}$ kills $v_{d}$ at that point
        \end{itemize}
        Even if we consider only loop-free paths, the number of paths in a graph can be exponentially larger than the number of nodes and edges. In practice, don't search all paths, summarize the reaching definitions at a node over all the paths reaching it.
    
        \nDefinition{DF Algorithm}{
            The data flow algorithm is a flexible tool which can be applied over any CFG's to find out some properties of the given graph.
            
            The algorithm is based on the following set of equations, where b is some block of code (usually enough to carry out analysis on granuality of CFG node's or the edges of those nodes):
            \begin{align}
                out_{b} = trans_{b}(in_{b}) \\
                in_{b} = join_{p\in pred_{b}}(out_{p}) 
            \end{align}

            Here $trans_{b}$ is the \textbf{transfer} function, of the block b. It works on the entry state $in_{b}$ yielding the exit state $out_{b}$.
            The $join$ operation combines the exit statuses of the predecessors p, of b, yielding the \textbf{entry} state of b.

            I.e. join works out the state of some "data-based property" at the start of our block, by looking at the predecessor nodes, whereas the transfer function, works out how this property gets altered by the current block.

            Each particular type of data-flow analysis has its own specific transfer function and join operation. Some require backward analysis which is very similar except the transfer function is applied to the exit state, and yields the entry state, while the join operation works 
            on the entry states of the successors to yield the exit state, i.e.:

            \begin{align}
                out_{b} = join_{s\in succ_{b}}(in_{s})\\
                in_{b} = trans_{b}(out_{b}) 
            \end{align}

            The \textbf{entry point} (in forward flow, backward flow this would be the \textbf{exit point}) plays an important role, since it has no predecessors, its entry state is well defined at the start, if there are no cycles (no loops), solving the equations is straight forward, since we can simply work out the out states of nodes in \textbf{topological order} of the CFG.

            If \textbf{cycles are present}, an iterative approach is required
            \begin{center}
                \nImg[0.75]{ST-27}
            \end{center}
            }
    }



    \nDefinition{Reaching definition analysis}{
        Reaching definition analysis is a type of forward analysis which calculates for each program block, a set of definitions which may potentially reach this program point

        The DF equations for this analysis are:
        \begin{align}
            out_{b} = gen(b) \cup (in_{b} - kill(b)) \\
            in_{b} = \bigcup_{p \in pred_{b}}(out_{p}) 
        \end{align}

        in other words:

        \begin{align}
            trans_{b} = gen(b) \cup (in_{b} - kill(b)) \\ 
            join_{b} = \bigcup
        \end{align}

        where:
        $$
            gen(b) = \textit{set of definitions/modifiations locally available in block}         $$

        $$
            kill(b) = \textit{set of definitions, killed by definitions in the block (not locally available, but in the rest of the program)}
        $$
            }
    \nDefinition{Available expression analysis}{
        A forward analysis, which determines the set of available expressions, i.e. those which need to be recomputed (in compiler design) at a program point. An expression is available if the operands of the expression are not modified on any path from the occurence of that expression to the program point.
        This analysis uses the following DF equations:

        \begin{align}
            out_{b} = gen(b) \cup (in_{b} - kill(b)) \\
            in_{b} = \bigcap{p \in pred_{b}}(out_{p}) 
        \end{align}

        in other words:

        \begin{align}
            trans_{b} = gen(b) \cup (in_{b} - kill(b)) \\ 
            join_{b} = \bigcap
        \end{align}

        where: 
        $$
        gen(b) = \textit{set of expressions computed at b} $$
        $$
        kill(b) = \textit{set of expressions which whose variables are modified at b}
        $$
    }

    \nDefinition{Liveness analysis}{
        An example of \textbf{backward} analysis, which finds the set of live variables, i.e. those which hold a value which may be needed in later parts of a program at point p.

        in the example below:

        b = 3;
        c = 5;
        a = f(b * c);

        the set of live variables after line 2 is \{b,c\}, since they are used in the next line, and after line 1, this set is only \{b\}

        the DF equation used in this analysis are as follows:
        \begin{align}
            out(b) = 
                \begin{cases}
                    \bigcap{s \in succ_{b}}(in_{s}) \; \textit{ if b $\neq$ exit}  \\
                    \emptyset \; \textit{ otherwise }
                \end{cases}    \\
            in(b) = gen(b) \cup (out(b) - kill(b))
        \end{align}

        where:
        $$
        gen(b) = \textit{set of variables that are used in b, before any assignment} $$
        $$
        kill(b) = \textit{set of variables that are modified in s}
        $$

    }

    \nDefinition{Iterative solution of dataflow equations}{
        Cycles in a CFG complicate dataflow analyses, and necessitate a more complex approach. 
        We can deal with cycles with the following iterative algorithm:

        \begin{enumerate}
            \item Approximate the $in$ state of \textbf{each} block (for $\bigcap$ transfer functions, first guess is the union of all "gen" sets, for analyses using $\bigcup$ first guess is the empty set, i.e. the most \textbf{conservative} option).
            \item Compute the $out$ states bu applying the transfer function on the $in$ states.
            \item Now update the $in$-states are updated with the join operations
            \item Repeat steps 2-3 untill reaching a \textbf{fixpoint}, i.e. the point of convergence in which the in-states do not change (and so the out-states)
        \end{enumerate}

        The order in which we update the nodes matters, usually apply the most \textbf{natural} order, i.e. in forward analysis, you'd use \textbf{reverse postorder} - visit a node before any of its successors, unless when the successor is reached by a back edge (think loop start nodes).
        Or in the case of backward analysis, you'd want to use \textbf{postorder} i.e. visit a node after all its successor nodes have been visited (DFS) 
    }
    \nDefinition{Classification of analyses}{
        DF algorithm analyses can be classified based on:
        \begin{itemize}
            \item if a node's set depends on that of its predecessors/successors (\textbf{Forward or backward})
            \item if a node's set contains a value iff it is coming from any/all of its inputs (\textbf{any/all path})
        \end{itemize}

        \begin{center}
            \begin{tabular}{|c|c|c|}
                \hline 
                -& Any-path ($\bigcup$)&  All-paths ($\bigcap$)\\ \hline
                Forward (pred) & Reach & Avail \\ \hline 
                Backward (succ) & Live & "inevitable" \\ \hline 
            \end{tabular}   
        \end{center}

    }

    \nDefinition{Scope of DF analysis}{
        \begin{itemize}
            \item Intra-procedural - within a single method or procedure 
            \item Inter-procedural - across severall methods or classes
        \end{itemize}

        Cost/precision trade-offs for inter-procedural analysis are critical, and difficult. 
        Two main trade-offs:
        \begin{itemize}
            \item Context-sensitivity
            \item Flow-sensitivity
        \end{itemize}
    }

    \nDefinition{Context sensitivity}{
        \begin{center}
            \nImg[1]{ST-28}
        \end{center}
    }

    \nDefinition{Flow sensitivity}{
        Reach, Avail etc. were all \textbf{flow-sensitive}:
        \begin{itemize}
            \item They considered ordering and control flow decisions
            \item Within a single procedure or method, this is fairly cheap: $O(n^3)$ for n CFG nodes 
            \item Many inter-procedural flow analyses are \textbf{flow-insensitive} 
            \item $O(n^{3})$ would not be acceptable for all the statements in a program
            \item Often flow-insensitive is good enough (i.e. type checking)
        \end{itemize}
    }

    \nDefinition{Summary}{
        DF algorithms:
        \begin{itemize}
            \item[+] Can be implemented by efficient iterative algorithms 
            \item[+] Widely applicable (not just for "classic data flow" properties)
            \item[-] Unable to distinguish feasible from infeasible paths
            \item[-] Analysies spanning whole programs (e.g. alias analysis) must trade off precision against computational cost    
        \end{itemize}
    }

\nSection{Data Flow Testing}

\nDefinition{Def-Use pair testing}{
    Adequacy criteria: Each DU \textbf{pair} is exercised by at least one test case 
}

\nDefinition{Def-Use path testing}{
    Adequacy criteria: Each DU \textbf{clear path} (simple and non looping) is exercised by at least one test case 
}

\nDefinition{Def testing}{
    Adequacy criteria: Each Definition has at least one test case which exercises a DU pair containing that definition 
}

\nDefinition{DF with complex structures}{
    Problem of aliases - which references are (always or sometimes) the same?
    
    Arrays and pointers are critical for data flow analysis:
    \begin{itemize}
        \item Under-estimation of aliases may fail to include some DU paris
        \item Over-estimation, on the other hand, may introduce unfeasible test obligations
    \end{itemize} 

    For testing, it may be preferable to accept under-estimation
}

\nDefinition{Infeasibility}{
    \begin{center}
        \nImg[0.9]{ST-29}
    \end{center}

    \begin{itemize}
        \item In practice, reasonable coverage, is often but not always achievable
        \item All DU paths is more often impractical
    \end{itemize}
}

\nSection{Test Selection and Adequacy}

\nDefinition{Test suite adequacy}{
    A real way of measuring effective testing is impossible, provably undecidable.

    What we can do is define \textbf{inadequacy} criteria:
    \begin{itemize}
        \item If the specification describes different treatment in two cases, but the test suite does not check that the two cases are in fact treated differently,
         we may conclude that the test suite is inadequate to guard against faults in the program logic 
        \item If no test in the test suite executes a particular program statement, the test suite is inadequate to guard against faults in that statement.
        \item If a test suite fails to satisfy some criterion, the obligation that has not been satisfied may provide some useful information about improving the test suite 
        \item If a test suite satisfies all the obligation by all the criteria, we do not know definitively that it is an effective test suite, but we have some evidecnce of its thorougness
    \end{itemize}
}

\nDefinition{Unsatisfiability}{
    Sometimes no test suite can satisfy a criterion for a given program (defensive programming - can't happen checks for example)
    
    We can either exclude unsatisfiable obligations from the criterion or measure the extent to which a test suite approaches an adequacy criterion (percentage).

    An adequacy criterion is satisfied or not, acoverage measure is the fraction of satisfied obligations
}


\nDefinition{Comparing criteria}{
    We can try to distinguish stronger from weaker adequacy criteria. 

    Best way to do that is by describing conditions under which one adequacy criterion is provably stronger than another - 
    i.e. gives stronger guarantees
}

\nDefinition{Subsumption}{
    Test adequacy criterion A subsumes B iff, for every program P, every test suite satisfying A with respect to P also satisfies B with respect to P.

    Example: branch coverage subsumes statement coverage
}

\nDefinition{Uses of adequacy criteria}{
    We shouldn't blindly rely on coverage criteria as infallible measure of success of a test suite. Ideally use it to guide improvement to a test suite,
    not as the primary motivation behind test design!
}

\nSection{Mutation Testing}

\nDefinition{Mutation testing}{
    Adequacy criteria: A test suite satisfies the criteria, iff every generated mutant is killed by the suite.

    I.e. a mutant is a program with a small random modification. 
    A mutant is killed if the test suite fails on incorrect behaviour introduced by the mutant.
    Mutants might produce equivalent behaviour, which is not useful from a testing perspective. Want mutants which should mess up the program in weird ways

    \begin{itemize}
        \item[+] measures quality of test cases nicely 
        \item[+] provides tester with a clear target (mutants to kill)
        \item[-] computationally intensive, a lot of mutants
        \item[-] equivalent mutants are a practical problem - determining which mutants are equivalent is un-decidable!
        \item[-] really only useful at unit testing level     
    \end{itemize}
    }

\nDefinition{Mutation operators}{
    predefined program modification rules corresponding to a fault model.

    Ideally we would like mutation operators which are representative of all realistic types of faults that could occur in practice (be made by a real programmer).

    In general operator set is large, and can capture all syntactic variations in a program.
}

\nDefinition{Mutation coverage}{
    
    \begin{itemize}
        \item Complete coverage equals the killing of all non-equivalent mutants (or random sample, not necessarily all possible modifications)
        \item The amount of coverage is called the \textbf{mutation score}
        \item The number of mutants depends on definition of mutation operators and structure of software
        \item Random sampling used since this set of mutants can be large.
    \end{itemize}

}

\nDefinition{Assumptions}{
    \begin{itemize}
        \item Competent programmer assumption - they write programs which are nearly correct
        \item Coupling effect assumption - test cases that distinguish all programs differing from a correct one by only simple errors are so sensitive that they also implicitly distinguish more complex errors 
    \end{itemize}

    Some empirical evidence of the above
}

\nSection{Test Driven Developement}

\nDefinition{TDD}{
    Never write a single line of code unless you have a failing automated test.

    \begin{center}
        \nImg[0.5]{ST-30}
    \end{center}
}

\nDefinition{Summary}{
    \begin{itemize}
        \item[+] Confidence in change - can change code without shitting your pants 
        \item[+] Documents requirements via tests 
        \item[+] Discovers usability issues or problems with requirements early 
        \item[+] Regression testing = stable software = quality software
        \item[+] Major quality improvement for minor time investment
        \item[-] Programmers like to code, not test 
        \item[-] Test writing is consuming 
        \item[-] Test completeness is difficult to judge 
        \item[-] May not always be suitable
        \item Only really relevant to Functional testing          
    \end{itemize}
}

\nSection{Regression Testing}
\nSection{Security Testing}
\nSection{Integration and Component Based Testing}
\nSection{System and Acceptance Testing}
\nSection{Concurrency Testing}

\end{document}
